{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "MNIST dataset v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yKUVQAunrZxp",
        "uMEeBKO7CGug",
        "S4D5OgO8CGuo",
        "oS6cvTCuCGvi",
        "Nn-QgyAiznZa",
        "YxOMqf7B_gXS",
        "XdX8IwgLCG09",
        "-QpDgqwrmUfD",
        "-5eyNFM8cemP",
        "MdBCEddPCG1z",
        "u0sa5HSPCG3z",
        "oOfnQIhbCG5W"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0ea92eb36594ddfa3e780775043737a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c9fc3353fa9949a9b32c8bbe159984bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8b838e09d65c4178bc41088b05cfe6a9",
              "IPY_MODEL_b139350bc1ef46ce8bdceb923ea47eda"
            ]
          }
        },
        "c9fc3353fa9949a9b32c8bbe159984bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b838e09d65c4178bc41088b05cfe6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2e8c3303bab4eb3b2709753d2510ac6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_441b2bd37ff64e51b4ce474aaa02ed3c"
          }
        },
        "b139350bc1ef46ce8bdceb923ea47eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c603b7837adc4e76b4ff88423dc73af5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 1065786.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c177b1af242d43568cd89424116b485e"
          }
        },
        "d2e8c3303bab4eb3b2709753d2510ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "441b2bd37ff64e51b4ce474aaa02ed3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c603b7837adc4e76b4ff88423dc73af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c177b1af242d43568cd89424116b485e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "713e62660a164c55b6922ff7da313314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f66f0dde2bb34dd5a1372e3b397a0b87",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0171313e86ef4e0c83f12651d18ae19a",
              "IPY_MODEL_d578d165cb37424e9aad741a86d5e507"
            ]
          }
        },
        "f66f0dde2bb34dd5a1372e3b397a0b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0171313e86ef4e0c83f12651d18ae19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b49b57287d442f98c43f7419e7d9760",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71ebc1d94cb04936a79031592df91aef"
          }
        },
        "d578d165cb37424e9aad741a86d5e507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3dba4feff6224104967367ccb7c829ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 116986.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a7328dfb5bc4288812e3c91d36b9b7d"
          }
        },
        "5b49b57287d442f98c43f7419e7d9760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71ebc1d94cb04936a79031592df91aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3dba4feff6224104967367ccb7c829ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a7328dfb5bc4288812e3c91d36b9b7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85ca9c66068c442d82904b06eaadcfb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_69bb0f65adb745b69e6ca63f826f59d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_62543b9781f44190aa7e05551e84dae9",
              "IPY_MODEL_3dd2015a4dd64ad4aad4b58084557165"
            ]
          }
        },
        "69bb0f65adb745b69e6ca63f826f59d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62543b9781f44190aa7e05551e84dae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7bb3b2c3b36b451b8b0dbae0b4ea1d39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e16318594dab4d3b918b3f09afbd44b2"
          }
        },
        "3dd2015a4dd64ad4aad4b58084557165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01f1facb6dc44f0a9ff502be27dfd27a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:18&lt;00:00, 285503.16it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_feafb10f47b5497f835d715fdf64c8b5"
          }
        },
        "7bb3b2c3b36b451b8b0dbae0b4ea1d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e16318594dab4d3b918b3f09afbd44b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01f1facb6dc44f0a9ff502be27dfd27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "feafb10f47b5497f835d715fdf64c8b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d32ae72420ac4c7d911055721182e59d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a04c0f629c54810ba2898908d3eff67",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d3d66b6583154b4c93e7db49b90d8fb6",
              "IPY_MODEL_af87362d40854e5d86aba19aa1f1a552"
            ]
          }
        },
        "9a04c0f629c54810ba2898908d3eff67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3d66b6583154b4c93e7db49b90d8fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8b8d9d88bf0480b8eeb4327654cd338",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51f9a7f194b1471fbca0069f2c691eab"
          }
        },
        "af87362d40854e5d86aba19aa1f1a552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f7fb88178a54ed79b2599415faadaa6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/4542 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30be669da31d4b8da598e290b607df93"
          }
        },
        "d8b8d9d88bf0480b8eeb4327654cd338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51f9a7f194b1471fbca0069f2c691eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f7fb88178a54ed79b2599415faadaa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30be669da31d4b8da598e290b607df93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzl11ui1XW5u"
      },
      "source": [
        "Special thanks to following author whom I referenced from in creating this notebook:\n",
        "\n",
        "https://jovian.ml/aakashns/03-logistic-regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKUVQAunrZxp"
      },
      "source": [
        "# Primer on Object Oriented Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BKD9PlGs3eE"
      },
      "source": [
        "Object Oriented Programming (OOP) is at the heart of many ML/DL based frameworks; without understanding it, it is easy to get lost amidst the seeming mess of code on public github repositories. OOP primarily revolves around the creation and use of classes and instances of them. In the example below, we create a Person class with its instance attributes, constructor and instance method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sXTHk2orveH"
      },
      "source": [
        "class Person(object):\r\n",
        "  \"\"\"\r\n",
        "  This is a Person class\r\n",
        "  \"\"\"\r\n",
        "  # constructor\r\n",
        "  def __init__(self, name, age):\r\n",
        "    # instance attributes\r\n",
        "    self.name = name\r\n",
        "    self.age = age\r\n",
        "\r\n",
        "  # instance method\r\n",
        "  def introduce(self):\r\n",
        "    print(f\"I am {self.name} and I am {self.age} years old.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maxl3AibvRh-"
      },
      "source": [
        "We then create an **instance** of the Person class called p1 and call the introduce function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIVKk9KLu4Vg",
        "outputId": "8ba1897b-c2ca-4519-b26b-effb059312f8"
      },
      "source": [
        "p1 = Person(\"John Doe\", 25)\r\n",
        "p1.introduce()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am John Doe and I am 25 years old.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DU2q8wLvYKA"
      },
      "source": [
        "But classes such as Person (above) are generic, and have attributes that can be reused for more specific classes of people, e.g. Policeman, Teacher. Hence, we can **inherit** from the Person class when creating these more specific classes. An example is shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4c82waLuLBn"
      },
      "source": [
        "# child class PoliceMan of parent class Person\r\n",
        "class PoliceMan(Person):\r\n",
        "\r\n",
        "  def __init__(self, name, age, action):\r\n",
        "    super(PoliceMan, self).__init__(name, age)\r\n",
        "    self.action = action\r\n",
        "\r\n",
        "  def work(self):\r\n",
        "    print(f\"I {self.action} criminals as my job.\")\r\n",
        "\r\n",
        "# another child class Teacher of parent class Peron\r\n",
        "class Teacher(Person):\r\n",
        "\r\n",
        "  def __init__(self, name, age, action):\r\n",
        "    super(Teacher, self).__init__(name, age)\r\n",
        "    self.action = action\r\n",
        "  \r\n",
        "  def work(self):\r\n",
        "    print(f\"I {self.action} students at a school.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT5I9ZFEwVFS",
        "outputId": "f6070e9e-97a1-402c-d3fe-da1d856ef440"
      },
      "source": [
        "p2 = PoliceMan(\"Bob\", 32, \"catch\")\r\n",
        "p2.introduce()\r\n",
        "p2.work()\r\n",
        "\r\n",
        "p3 = Teacher(\"Jane\", 27, \"teach\")\r\n",
        "p3.introduce()\r\n",
        "p3.work()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am Bob and I am 32 years old.\n",
            "I catch criminals as my job.\n",
            "I am Jane and I am 27 years old.\n",
            "I teach students at a school.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5ToD1vvxY9C"
      },
      "source": [
        "One additional thing to note about classes is the ability to write something called magic methods in python. These methods are also called 'dunder' methods, and are recognized by the double _ before and after the method name. N.B. is __init__ is one of them!\r\n",
        "\r\n",
        "Dunder methods add a lot of unseen functionality to our classes. For example, ever wondered why you could add 2 integers together? How does the plus sign implement its functionality in python?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4D5h4Xcx9ji",
        "outputId": "a258adb0-d949-4630-bc3d-a10b6ea77c6e"
      },
      "source": [
        "c = 3 + 2\r\n",
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW9qZK_1yB4x"
      },
      "source": [
        "But if we add 2 PoliceMan instances as per above together, we will definitely get an error! Of course, in real life, it doesn't make sense to add 2 PoliceMan together as well right..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "R_t_MgIIx_cP",
        "outputId": "b0874b58-5c21-4bfb-ee76-490081965369"
      },
      "source": [
        "p4 = PoliceMan(\"Craig\", 45, \"apprehend\")\r\n",
        "p5 = PoliceMan(\"Jack\", 33, \"catch\")\r\n",
        "combined = p4 + p5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ace1a32e18b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoliceMan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Craig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apprehend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoliceMan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Jack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"catch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'PoliceMan' and 'PoliceMan'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozdMcQXXydnu"
      },
      "source": [
        "But what if I told you that it is actually possible to add them together, with a little bit of modification to our original PoliceMan class of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeVLNoCdyoOu"
      },
      "source": [
        "# modified PoliceMan class\r\n",
        "class PoliceMan(Person):\r\n",
        "\r\n",
        "  def __init__(self, name, age, action):\r\n",
        "    super(PoliceMan, self).__init__(name, age)\r\n",
        "    self.action = action\r\n",
        "\r\n",
        "  def work(self):\r\n",
        "    print(f\"I {self.action} criminals as my job.\")\r\n",
        "\r\n",
        "  def __add__(self, other_police):\r\n",
        "    total_police = self.name + \" and \" + other_police.name\r\n",
        "    print(f\"We are {total_police}. We now have a police station!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANFqgIXyyzMr",
        "outputId": "9970333b-f30f-4d43-f075-c29da4a12302"
      },
      "source": [
        "p4 = PoliceMan(\"Craig\", 45, \"apprehend\")\r\n",
        "p5 = PoliceMan(\"Jack\", 33, \"catch\")\r\n",
        "combined = p4 + p5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are Craig and Jack. We now have a police station!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BBhzSrb0Qay"
      },
      "source": [
        "In fact, we can check the implemented dunder methods for each of the built in class/data types that we know of by using the dir method. \r\n",
        "\r\n",
        "You should know this by now: where is the dir functionality even implemented???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8q8s4Q_2Eri",
        "outputId": "dec03a66-83e8-4411-af5d-951a45202451"
      },
      "source": [
        "dir(PoliceMan)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " 'introduce',\n",
              " 'work']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvjenePf0JWX",
        "outputId": "db03667d-0a77-4df9-f999-1f91d4c9691d"
      },
      "source": [
        "dir(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abs__',\n",
              " '__add__',\n",
              " '__and__',\n",
              " '__bool__',\n",
              " '__ceil__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__divmod__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__float__',\n",
              " '__floor__',\n",
              " '__floordiv__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getnewargs__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__index__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__int__',\n",
              " '__invert__',\n",
              " '__le__',\n",
              " '__lshift__',\n",
              " '__lt__',\n",
              " '__mod__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__neg__',\n",
              " '__new__',\n",
              " '__or__',\n",
              " '__pos__',\n",
              " '__pow__',\n",
              " '__radd__',\n",
              " '__rand__',\n",
              " '__rdivmod__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__rfloordiv__',\n",
              " '__rlshift__',\n",
              " '__rmod__',\n",
              " '__rmul__',\n",
              " '__ror__',\n",
              " '__round__',\n",
              " '__rpow__',\n",
              " '__rrshift__',\n",
              " '__rshift__',\n",
              " '__rsub__',\n",
              " '__rtruediv__',\n",
              " '__rxor__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__sub__',\n",
              " '__subclasshook__',\n",
              " '__truediv__',\n",
              " '__trunc__',\n",
              " '__xor__',\n",
              " 'bit_length',\n",
              " 'conjugate',\n",
              " 'denominator',\n",
              " 'from_bytes',\n",
              " 'imag',\n",
              " 'numerator',\n",
              " 'real',\n",
              " 'to_bytes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMEeBKO7CGug"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7zXEj6oCGuj"
      },
      "source": [
        "This notebook will give you an idea of how to predict numbers from images using Pytorch. We will build a simple Logistic Regression Neural Network to identify the digits, and you'll see at the end that we can get an accuracy of ~86.5%!\n",
        "\n",
        "##The Data\n",
        "\n",
        "We'll use the famous [*MNIST Handwritten Digits Database*] as our training dataset. It consists of 28px by 28px grayscale images of handwritten digits (0 to 9), along with labels for each image indicating which digit it represents. Here are some sample images from the dataset:\n",
        "\n",
        "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4D5OgO8CGuo"
      },
      "source": [
        "## Exploring the Data\n",
        "\n",
        "Import `torch` and `torchvision`. \n",
        "\n",
        "`torchvision` contains some utilities for working with image data. It also contains helper classes to automatically download and import popular datasets like MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNRUH93hCGu3"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Em4bSsbeRv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFhC-eplCGvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "b0ea92eb36594ddfa3e780775043737a",
            "c9fc3353fa9949a9b32c8bbe159984bd",
            "8b838e09d65c4178bc41088b05cfe6a9",
            "b139350bc1ef46ce8bdceb923ea47eda",
            "d2e8c3303bab4eb3b2709753d2510ac6",
            "441b2bd37ff64e51b4ce474aaa02ed3c",
            "c603b7837adc4e76b4ff88423dc73af5",
            "c177b1af242d43568cd89424116b485e",
            "713e62660a164c55b6922ff7da313314",
            "f66f0dde2bb34dd5a1372e3b397a0b87",
            "0171313e86ef4e0c83f12651d18ae19a",
            "d578d165cb37424e9aad741a86d5e507",
            "5b49b57287d442f98c43f7419e7d9760",
            "71ebc1d94cb04936a79031592df91aef",
            "3dba4feff6224104967367ccb7c829ee",
            "2a7328dfb5bc4288812e3c91d36b9b7d",
            "85ca9c66068c442d82904b06eaadcfb7",
            "69bb0f65adb745b69e6ca63f826f59d2",
            "62543b9781f44190aa7e05551e84dae9",
            "3dd2015a4dd64ad4aad4b58084557165",
            "7bb3b2c3b36b451b8b0dbae0b4ea1d39",
            "e16318594dab4d3b918b3f09afbd44b2",
            "01f1facb6dc44f0a9ff502be27dfd27a",
            "feafb10f47b5497f835d715fdf64c8b5",
            "d32ae72420ac4c7d911055721182e59d",
            "9a04c0f629c54810ba2898908d3eff67",
            "d3d66b6583154b4c93e7db49b90d8fb6",
            "af87362d40854e5d86aba19aa1f1a552",
            "d8b8d9d88bf0480b8eeb4327654cd338",
            "51f9a7f194b1471fbca0069f2c691eab",
            "3f7fb88178a54ed79b2599415faadaa6",
            "30be669da31d4b8da598e290b607df93"
          ]
        },
        "outputId": "e1bc984b-c7c0-4571-c02a-cb9911113a7a"
      },
      "source": [
        "# Download training dataset\n",
        "dataset = MNIST(root='data/', download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0ea92eb36594ddfa3e780775043737a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "713e62660a164c55b6922ff7da313314",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ca9c66068c442d82904b06eaadcfb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d32ae72420ac4c7d911055721182e59d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziEyyiPFCGvM"
      },
      "source": [
        "When this statement is executed for the first time, it downloads the data to the `data/` directory next to the notebook and creates a PyTorch `Dataset`. On subsequent executions, the download is skipped as the data is already downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUvJZY5_CGvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752cf121-e4ab-4e2e-d96a-c3c55e49b661"
      },
      "source": [
        "# Check the size of the dataset\n",
        "print(len(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5g5-v3BCGvU"
      },
      "source": [
        "The dataset has 60,000 images which can be used to train the model. There is also an additonal test set of 10,000 images which can be created by passing `train=False` to the `MNIST` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am76cz8rCGvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7182a87-601f-4ddb-d38a-383998af2bac"
      },
      "source": [
        "test_dataset = MNIST(root='data/', train=False)\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS6cvTCuCGvi"
      },
      "source": [
        "## Visualising the data\r\n",
        "Looking at a sample element from the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AF1dFQ7CGvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c96bb5-5b83-4c23-b623-15e19f4d7a17"
      },
      "source": [
        "print(dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<PIL.Image.Image image mode=L size=28x28 at 0x7FF2CD94E470>, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjXvpEoCGvy"
      },
      "source": [
        "It's a tuple, consisting of a 28x28 image and a label. The image is an object of the class `PIL.Image.Image`, which is a part of the Python imaging library [Pillow](https://pillow.readthedocs.io/en/stable/). We can view the image within Jupyter using [`matplotlib`](https://matplotlib.org/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAzbnC_PCGv1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pW5okdsCGwA"
      },
      "source": [
        "Along with importing `matplotlib`, a special statement `%matplotlib inline` is added to indicate to Jupyter that we want to plot the graphs within the notebook. Without this line, Jupyter will show the image in a popup. Statements starting with `%` are called IPython magic commands, and are used to configure the behavior of Jupyter itself. You can find a full list of magic commands here: https://ipython.readthedocs.io/en/stable/interactive/magics.html .\n",
        "\n",
        "Let's look at a couple of images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJlyu9-SCGwD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "966dc28b-c913-4968-d2ca-12e9a133b8bf"
      },
      "source": [
        "image, label = dataset[0]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysT_G7Q4CGwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5610756a-5439-4445-eac7-147a30bac9cf"
      },
      "source": [
        "image, label = dataset[10]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5rn_RGfCGwb"
      },
      "source": [
        "## Reshaping\n",
        "\n",
        "While it may be useful to visualise these images, there is just one problem: Pytorch does not recognise these raw images.\n",
        "\n",
        "We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Nks_IJCGwc"
      },
      "source": [
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn.functional "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vlckumzCGwl"
      },
      "source": [
        "PyTorch datasets allow us to specify one or more transformation functions which are applied to the images as they are loaded. `torchvision.transforms` contains many such predefined functions, and we'll use the `ToTensor` transform to convert images into PyTorch tensors.\n",
        "\n",
        "`torchvision.transforms` also contains many other [data augmentation techniques](https://pytorch.org/docs/stable/torchvision/transforms.html) that could be used to augment the images in the dataset before training on a model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyUbjXXTCGwn"
      },
      "source": [
        "# MNIST dataset (images and labels)\n",
        "dataset = MNIST(root='data/', \n",
        "                train=True,\n",
        "                transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZPTLQiXCGw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b282419-aa0e-4bea-a2b3-9d7ba89e9884"
      },
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yn02RKlCGxA"
      },
      "source": [
        "The image is now converted to a 1x28x28 tensor. The first dimension is used to keep track of the color channels. \n",
        "\n",
        "Since images in the MNIST dataset are grayscale, there's just one channel. Most other datasets have images with color, in which case there are 3 channels: red, green and blue (RGB), so their more common dimensions would be `[3, HEIGHT, WIDTH]`. \n",
        "\n",
        "Our MNIST dataset are greyscale values, hence, the first dimension = 1. Let's look at some sample values inside the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z52bZ_xRCGxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c62cc2-fdd1-4ca3-db51-3f08ec8dff00"
      },
      "source": [
        "# prints only 10th to 15th pixel of image\n",
        "\n",
        "print(img_tensor[:,10:15,10:15])\n",
        "print(torch.max(img_tensor), torch.min(img_tensor))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
            "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
            "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
            "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
            "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
            "tensor(1.) tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeJOGNbGCGxQ"
      },
      "source": [
        "The values range from 0 to 1, with 0 representing black, 1 white and the values in between different shades of grey. \n",
        "\n",
        "We plot the tensor as an image using `plt.imshow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcRDfd2mCGxR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "224cbea4-2bf7-4a5c-8360-1e035b75eb8b"
      },
      "source": [
        "# Plot the image by passing in the 28x28 matrix, prints only pixel value from 10 to 15th position\n",
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDixW3zACGxg"
      },
      "source": [
        "Note that we need to pass just the 28x28 matrix to `plt.imshow`, without a channel dimension. We also pass a colour map (`cmap=gray`) to indicate that we want to see a grayscale image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFtriMFECGxh"
      },
      "source": [
        "## Training and Validation Datasets\n",
        "\n",
        "While building real world machine learning models, it is quite common to split the dataset into 3 parts:\n",
        "\n",
        "1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
        "2. **Validation set** - used to evaluate the model while training, check for potential overfitting/underfitting and adjust hyperparameters (e.g. learning rate) accordingly.\n",
        "3. **Test set** - used to compare different models after training and to see how accurate model is on dataset that it hasn't seen.\n",
        "\n",
        "In the MNIST dataset, there are 60,000 training images, and 10,000 test images. The test set is standardized so that different researchers can report the results of their models against the same set of images. \n",
        "\n",
        "Since there's no predefined validation set, we must manually split the 60,000 images into training and validation datasets. We set aside 10,000 randomly chosen images for validation. We can do this using the `random_spilt` method from PyTorch.\n",
        "\n",
        "**Why random split?**\n",
        "\n",
        "It's important to choose a random sample for creating a validation set, because training data is often ordered by the target labels i.e. images of 0s, followed by images of 1s, followed by images of 2s and so on. If we were to pick a 10% validation set simply by selecting the last 10% of the images, the validation set would only consist of images of 8s and 9s, whereas the training set would contain no images of 8s and 9s. The trained model would not be that good as it has not seen images of 8s and 9s.\n",
        "\n",
        "Below is a code snippet on how to split dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO1BDDkWCGxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc532b76-a14e-41c1-f324-626b1b34a186"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCmd2q_CGx0"
      },
      "source": [
        "We can now created data loaders to help us load the data in batches. We'll use a batch size of 128.\n",
        "\n",
        "We split dataset into batches because training on batches generally requires less memory, and also allows network to train faster with these mini-batches.\n",
        "\n",
        "For more information on batch size, refer to this [forum](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network#:~:text=Advantages%20of%20using%20a%20batch,dataset%20in%20your%20machine's%20memory).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz8m8cNlCGx2"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#batch_size = 128\n",
        "\n",
        "# train_loader and val_loader breaks dataset into batches\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_ds, batch_size=128, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyCWntgkbiW"
      },
      "source": [
        "Visualise what train_loader and val_loader contain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHR51Tu9kbr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd1c68b-3078-44f5-dcb3-6ab553c2155e"
      },
      "source": [
        "# See first batch in train_loader\n",
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break\n",
        "\n",
        "# The same can be done for val_loader, try it out and see for yourself!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 5, 6, 1, 9, 0, 2, 2, 5, 5, 5, 6, 9, 3, 1, 9, 0, 0, 6, 6, 1, 8, 8, 8,\n",
            "        6, 6, 5, 7, 2, 2, 6, 3, 1, 0, 1, 3, 8, 8, 6, 9, 2, 3, 8, 0, 1, 6, 1, 4,\n",
            "        1, 0, 4, 3, 6, 1, 3, 1, 5, 6, 3, 1, 6, 7, 4, 6, 3, 9, 0, 8, 3, 9, 1, 5,\n",
            "        0, 1, 2, 2, 5, 6, 2, 7, 7, 5, 3, 0, 1, 1, 3, 1, 2, 2, 4, 1, 1, 1, 2, 9,\n",
            "        6, 7, 5, 4, 6, 7, 2, 9, 2, 0, 2, 1, 6, 1, 5, 6, 7, 8, 1, 7, 5, 8, 2, 2,\n",
            "        2, 7, 2, 0, 1, 6, 7, 4])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka7GP_JPCGx_"
      },
      "source": [
        "We set `shuffle=True` for the training dataloader, so that the batches generated in each epoch are different, and this randomization helps generalize & speed up the training process. There is no need to shuffle images for validation dataloader since the it is used only for evaluating the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn-QgyAiznZa"
      },
      "source": [
        "# Creating Model Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxOMqf7B_gXS"
      },
      "source": [
        "## Final Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRXZQy2eCG0T"
      },
      "source": [
        "For each of the input images, we get 10 outputs, one for each class. As discussed earlier, we'd like these outputs to represent probabilities, but for that the elements of each output row must lie between 0 to 1 and add up to 1, which is clearly not the case here. (i.e. there are negative values in the sample outputs)\n",
        "\n",
        "To convert the output rows into probabilities, we use the softmax function, which has the following formula:\n",
        "\n",
        "![softmax](https://i.imgur.com/EAh9jLN.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Es-gsBCG0V"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvnBYZ6UCG0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25353d22-58f2-4e18-b18b-c559535139d8"
      },
      "source": [
        "labels = torch.Tensor([3.0, 5.0, 7.0])\r\n",
        "F.softmax(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0159, 0.1173, 0.8668])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdX8IwgLCG09"
      },
      "source": [
        "## Evaluation Metric and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8TfriKLCG1B"
      },
      "source": [
        "# Combine into a single function\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPNlpuEJCG1R"
      },
      "source": [
        "While the accuracy is a great way for evaluating the model, it can't be used as a loss function for optimizing our model using gradient descent, for the following reasons:\n",
        "\n",
        "1. It's not a differentiable function. `torch.max` and `==` are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases. They are simply functions that pick out the index with the highest probabilities.\n",
        "\n",
        "2. It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements. (i.e. gradient descent)\n",
        "\n",
        "Due to these reasons, accuracy is a great **evaluation metric** for classification, but not a good loss function. A commonly used loss function for classification problems is the **cross entropy**, which has the following formula:\n",
        "\n",
        "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
        "\n",
        "While it looks complicated, it's actually quite simple:\n",
        "\n",
        "* For each output row, pick the predicted probability for the correct label. E.g. if the predicted probabilities for an image are `[0.1, 0.3, 0.2, ...]` and the correct label is `1`, we pick the corresponding element `0.3` and ignore the rest.\n",
        "\n",
        "* Then, take the logarithm of the picked probability. If the probability is high i.e. close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions.\n",
        "\n",
        "* Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data.\n",
        "\n",
        "Unlike accuracy, cross-entropy is a continuous and differentiable function that also provides good feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). This makes it a good choice for the loss function. \n",
        "\n",
        "As you might expect, PyTorch provides an efficient and tensor-friendly implementation of cross entropy as part of the `torch.nn.functional` package. Moreover, it also performs softmax internally, so we can directly pass in the outputs of the model without converting them into probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9kk7tkOgO_o"
      },
      "source": [
        "This sets the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU3M6oWUCG1S"
      },
      "source": [
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QpDgqwrmUfD"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "We are going to use the `optim.SGD` optimizer to update the weights and biases during training, with a learning rate which will be defined later within the fit function.\n",
        "\n",
        "Configurations like batch size, learning rate etc. need to be picked in advance while training machine learning models, and are called hyperparameters. Picking the right hyperparameters is critical for training an accurate model within a reasonable amount of time, and is an active area of research and experimentation. You can try out different learning rates and see how it affects the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrKUq9rcALX3"
      },
      "source": [
        "model = Model()\r\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5eyNFM8cemP"
      },
      "source": [
        "## Debugging and Unit Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnvSm7v0MzW"
      },
      "source": [
        "Now we've set up all the basic building blocks to finally put together a training loop. Hooray! Well, not quite.\r\n",
        "\r\n",
        "It's always best to fix problems (bugs as we call them) as you go along rather than accummulate them and try to solve them at the very end. You will begin to realize that this is possible the more you practise as you begin to encounter very repetitive problems which you need to debug.\r\n",
        "\r\n",
        "Bugs come in 2 different flavors: the kinds you can see and the kinds you can't. The first is generally easier to fix, because the console literally prints out everything wrong for you (seriously, the red lines aren't that scary after awhile). The second is much harder to fix due to the myriad of issues from which it can stem, and definitely differentiates the novice from the expert. But don't fret, even the best practitioners spend 90% of their modelling time debugging neural networks. \r\n",
        "\r\n",
        "In the spirit of solving problems as they come, it is good practice to write simple unit tests that help provide some sanity check along the way.\r\n",
        "\r\n",
        "The first unit test tackles a super common bug which will perplex many beginners: shape issues. You are not a true practitioner if you have never faced such an issue before while trying to build a ML model. Thankfully, we almost always know that the problem lies mainly in 3 places: 1) Converting datasets into dataloaders 2) Passing the inputs from the dataloaders through the model 3) Calculating the loss between the labels and the predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ty3gNQdPYA"
      },
      "source": [
        "from torch import Tensor\r\n",
        "from torch import nn\r\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvCJqaNNcj0l"
      },
      "source": [
        "# Code out the model class here\r\n",
        "\r\n",
        "class Model(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, hidden_dims: list, input_size: int, num_classes: int):\r\n",
        "    super(Model, self).__init__()\r\n",
        "    self.hidden_dims = hidden_dims\r\n",
        "    assert len(self.hidden_dims) <= 4, \"Too many layers used.\"\r\n",
        "    self.layers = []\r\n",
        "    prev_dim = input_size\r\n",
        "    for cur_dim in self.hidden_dims:\r\n",
        "      self.layers.append(nn.Linear(prev_dim, cur_dim))\r\n",
        "      self.layers.append(nn.Sigmoid())\r\n",
        "      prev_dim = cur_dim\r\n",
        "    self.layers.append(nn.Linear(prev_dim, num_classes))\r\n",
        "    self.model = nn.ModuleList(self.layers)\r\n",
        "\r\n",
        "  def forward(self, x: Tensor):\r\n",
        "    B = x.shape[0]\r\n",
        "    x = x.reshape(B, -1)\r\n",
        "    for layer in self.model:\r\n",
        "      x = layer(x)\r\n",
        "    return x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4RhwjscuXBv"
      },
      "source": [
        "def test_shapes(train_dataset,\r\n",
        "                valid_dataset,\r\n",
        "                model, \r\n",
        "                loss_fn, num_classes):\r\n",
        "  \r\n",
        "  # check if there are shape issues with the data, e.g. need collating function\r\n",
        "  train_dl = DataLoader(train_dataset, batch_size=32,\r\n",
        "                      shuffle=True, num_workers=4)\r\n",
        "                      \r\n",
        "  valid_dl = DataLoader(valid_dataset, batch_size=8,\r\n",
        "                        shuffle=False, num_workers=4)\r\n",
        "  \r\n",
        "  # check if there are issues with any layer in your model\r\n",
        "  x_batch, y_batch = next(iter(train_loader))\r\n",
        "  _, C, H, W = x_batch.shape\r\n",
        "  model = model(input_size=C*H*W, num_classes=num_classes)\r\n",
        "  print(model) \r\n",
        "  preds = model(x_batch)\r\n",
        "\r\n",
        "  # check if there are any issues with the labels or output\r\n",
        "  loss = loss_fn(preds, y_batch)\r\n",
        "  return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX7bbrusu7Xf",
        "outputId": "d7d38152-53f0-4ddd-f7b5-6f171f9bbdb6"
      },
      "source": [
        "model = partial(Model, hidden_dims=[300])\r\n",
        "# should return a float value if there are no shape issues\r\n",
        "test_shapes(train_ds,\r\n",
        "            val_ds,\r\n",
        "            model,\r\n",
        "            loss_fn, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (model): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=300, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3350913524627686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRWrvHpC0Vbk"
      },
      "source": [
        "The second unit test is more generic and will also catch shape bugs from the previous step along with other possible bugs. We still recommend going ahead with the first unit test first though as it will get one issue out of the way. \r\n",
        "\r\n",
        "Andrej Karpathy, head of AI at OpenAI, once said that if your model can't overfit on a single batch of data, that is a **sure sign that your architecture doesn't work**. This makes sense because your model will begin to memorize the data after a few epochs and so should definitely overfit to that batch of data. \r\n",
        "\r\n",
        "Of course, the previous holds true if everything else is set correctly, e.g. learning rate is as extremely small (1e-6 or less), no regularization, data augmentation, simplest possible architecture without any form of layer normalizations, etc. What should the loss look like? It should start high and eventually (should not be too gradual) drop to 0: no plateauing, no increasing, no exploding, no oscillating. If your loss looks anything like what's been described not to happen, you're probably doing something wrong. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Try overfitting this one batch of your data and see how it works out. We've attached links below for more details on debugging neural networks.\r\n",
        "\r\n",
        "https://karpathy.github.io/2019/04/25/recipe/\r\n",
        "\r\n",
        "https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hkydrby6ZS"
      },
      "source": [
        "from torch.optim import Optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEuJ1x1bwsxn"
      },
      "source": [
        "def overfit_one_batch(train_dl,\r\n",
        "                      model,\r\n",
        "                      loss_func,\r\n",
        "                      opt,\r\n",
        "                      num_classes,\r\n",
        "                      device):\r\n",
        "  \r\n",
        "  first_batch = next(iter(train_dl))\r\n",
        "  _, C, H, W = first_batch[0].shape\r\n",
        "  model = model(input_size=C*H*W, num_classes=num_classes)\r\n",
        "  model = model.to(device)\r\n",
        "  opt = opt(params=model.parameters())\r\n",
        "  model.train()\r\n",
        "  for batch_idx, (data, target) in enumerate([first_batch] * 10000):\r\n",
        "    train_batch, labels = first_batch[0].to(device), first_batch[1].to(device)\r\n",
        "    preds = model(train_batch)\r\n",
        "    loss = loss_func(preds, labels)\r\n",
        "    print(loss.item())\r\n",
        "    opt.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    opt.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc52_p5jwyt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6971543b-054b-4cfa-bc8e-e36e460049c4"
      },
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\r\n",
        "\r\n",
        "# from this we can likely tell that using linear layers isn't a good strategy for images\r\n",
        "# oh bother, let's go ahead with training anyway\r\n",
        "opt = partial(torch.optim.SGD, lr=1e-2) # Import SGD optimizer directly\r\n",
        "model = partial(Model, hidden_dims=[300])\r\n",
        "num_classes = 10\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "overfit_one_batch(train_dl,\r\n",
        "                  model,\r\n",
        "                  loss_fn,\r\n",
        "                  opt,\r\n",
        "                  num_classes,\r\n",
        "                  device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.009115707129240036\n",
            "0.009113172069191933\n",
            "0.009110622107982635\n",
            "0.00910811685025692\n",
            "0.009105551987886429\n",
            "0.009103076532483101\n",
            "0.009100526571273804\n",
            "0.009097990579903126\n",
            "0.009095500223338604\n",
            "0.0090929651632905\n",
            "0.00909045897424221\n",
            "0.009087923914194107\n",
            "0.009085402823984623\n",
            "0.009082897566258907\n",
            "0.009080377407371998\n",
            "0.009077901020646095\n",
            "0.009075351059436798\n",
            "0.009072860702872276\n",
            "0.009070338681340218\n",
            "0.009067803621292114\n",
            "0.009065298363566399\n",
            "0.009062792174518108\n",
            "0.009060315787792206\n",
            "0.009057795628905296\n",
            "0.00905529037117958\n",
            "0.009052769280970097\n",
            "0.009050292894244194\n",
            "0.009047801606357098\n",
            "0.009045295417308807\n",
            "0.009042790159583092\n",
            "0.009040284901857376\n",
            "0.009037807583808899\n",
            "0.009035301394760609\n",
            "0.009032825008034706\n",
            "0.009030318818986416\n",
            "0.009027858264744282\n",
            "0.009025395847856998\n",
            "0.009022904559969902\n",
            "0.00902041420340538\n",
            "0.009017937816679478\n",
            "0.009015415795147419\n",
            "0.009012925438582897\n",
            "0.009010478854179382\n",
            "0.009007957763969898\n",
            "0.009005525149405003\n",
            "0.009003018960356712\n",
            "0.009000557474792004\n",
            "0.008998095989227295\n",
            "0.008995603770017624\n",
            "0.008993172086775303\n",
            "0.0089906957000494\n",
            "0.008988218382000923\n",
            "0.008985697291791439\n",
            "0.008983206935226917\n",
            "0.00898080412298441\n",
            "0.008978386409580708\n",
            "0.008975865319371223\n",
            "0.00897340290248394\n",
            "0.008971001952886581\n",
            "0.008968538604676723\n",
            "0.008966047316789627\n",
            "0.0089636305347085\n",
            "0.008961139246821404\n",
            "0.008958720602095127\n",
            "0.008956287987530231\n",
            "0.008953811600804329\n",
            "0.008951305411756039\n",
            "0.00894884392619133\n",
            "0.008946426212787628\n",
            "0.008943978697061539\n",
            "0.008941516280174255\n",
            "0.008939114399254322\n",
            "0.008936652913689613\n",
            "0.008934219367802143\n",
            "0.008931801654398441\n",
            "0.008929340168833733\n",
            "0.008926891721785069\n",
            "0.008924518711864948\n",
            "0.008922027423977852\n",
            "0.008919638581573963\n",
            "0.008917177096009254\n",
            "0.008914759382605553\n",
            "0.008912310935556889\n",
            "0.008909879252314568\n",
            "0.008907491341233253\n",
            "0.00890502892434597\n",
            "0.008902625180780888\n",
            "0.008900193497538567\n",
            "0.00889777485281229\n",
            "0.008895328268408775\n",
            "0.008892939426004887\n",
            "0.008890537545084953\n",
            "0.00888808909803629\n",
            "0.008885612711310387\n",
            "0.008883252739906311\n",
            "0.008880864828824997\n",
            "0.008878402411937714\n",
            "0.008876044303178787\n",
            "0.00887362565845251\n",
            "0.008871193043887615\n",
            "0.008868790231645107\n",
            "0.008866356685757637\n",
            "0.008863968774676323\n",
            "0.008861579932272434\n",
            "0.008859148249030113\n",
            "0.008856818079948425\n",
            "0.00885438546538353\n",
            "0.008851908147335052\n",
            "0.008849549107253551\n",
            "0.008847176097333431\n",
            "0.008844802156090736\n",
            "0.00884238351136446\n",
            "0.008840024471282959\n",
            "0.008837606757879257\n",
            "0.008835189044475555\n",
            "0.00883281510323286\n",
            "0.008830411359667778\n",
            "0.008828037418425083\n",
            "0.008825649507343769\n",
            "0.008823290467262268\n",
            "0.008820930495858192\n",
            "0.008818482980132103\n",
            "0.008816123940050602\n",
            "0.008813735097646713\n",
            "0.0088113471865654\n",
            "0.008808973245322704\n",
            "0.008806629106402397\n",
            "0.00880421046167612\n",
            "0.00880185142159462\n",
            "0.008799521252512932\n",
            "0.008797133341431618\n",
            "0.008794729597866535\n",
            "0.008792414329946041\n",
            "0.008790011517703533\n",
            "0.008787666447460651\n",
            "0.00878530740737915\n",
            "0.008782919496297836\n",
            "0.008780529722571373\n",
            "0.008778171613812447\n",
            "0.008775796741247177\n",
            "0.008773422800004482\n",
            "0.008771107532083988\n",
            "0.0087687186896801\n",
            "0.008766388520598412\n",
            "0.008764045313000679\n",
            "0.008761655539274216\n",
            "0.008759296499192715\n",
            "0.008756967261433601\n",
            "0.008754637092351913\n",
            "0.008752306923270226\n",
            "0.008749947883188725\n",
            "0.008747618645429611\n",
            "0.008745213970541954\n",
            "0.00874288473278284\n",
            "0.008740540593862534\n",
            "0.008738180622458458\n",
            "0.008735866285860538\n",
            "0.008733506314456463\n",
            "0.008731176145374775\n",
            "0.008728832006454468\n",
            "0.008726516738533974\n",
            "0.008724186569452286\n",
            "0.008721856400370598\n",
            "0.008719542063772678\n",
            "0.008717138320207596\n",
            "0.00871485285460949\n",
            "0.00871255248785019\n",
            "0.008710207417607307\n",
            "0.008707817643880844\n",
            "0.008705517277121544\n",
            "0.00870320200920105\n",
            "0.008700872771441936\n",
            "0.00869858730584383\n",
            "0.00869621243327856\n",
            "0.008693926967680454\n",
            "0.008691610768437386\n",
            "0.008689296431839466\n",
            "0.008686981163918972\n",
            "0.00868471059948206\n",
            "0.008682320825755596\n",
            "0.008680050261318684\n",
            "0.008677690289914608\n",
            "0.008675404824316502\n",
            "0.008673089556396008\n",
            "0.008670774288475513\n",
            "0.008668488822877407\n",
            "0.008666172623634338\n",
            "0.008663858287036419\n",
            "0.008661571890115738\n",
            "0.008659256622195244\n",
            "0.008656926453113556\n",
            "0.008654655888676643\n",
            "0.008652325719594955\n",
            "0.008650083094835281\n",
            "0.008647738955914974\n",
            "0.0086454376578331\n",
            "0.008643152192234993\n",
            "0.008640836924314499\n",
            "0.008638580329716206\n",
            "0.008636309765279293\n",
            "0.008633993566036224\n",
            "0.008631737902760506\n",
            "0.008629392832517624\n",
            "0.00862715020775795\n",
            "0.008624864742159843\n",
            "0.008622579276561737\n",
            "0.008620264008641243\n",
            "0.00861800741404295\n",
            "0.008615721017122269\n",
            "0.008613435551524162\n",
            "0.00861116498708725\n",
            "0.008608848787844181\n",
            "0.0086065623909235\n",
            "0.0086043206974864\n",
            "0.008602005429565907\n",
            "0.008599748834967613\n",
            "0.00859750621020794\n",
            "0.008595235645771027\n",
            "0.008592949248850346\n",
            "0.00859066378325224\n",
            "0.008588407188653946\n",
            "0.00858612172305584\n",
            "0.00858388002961874\n",
            "0.008581608533859253\n",
            "0.008579380810260773\n",
            "0.00857712421566248\n",
            "0.008574808947741985\n",
            "0.00857258215546608\n",
            "0.008570295758545399\n",
            "0.0085680540651083\n",
            "0.008565752767026424\n",
            "0.00856351014226675\n",
            "0.00856128428131342\n",
            "0.008559012785553932\n",
            "0.008556756190955639\n",
            "0.008554513566195965\n",
            "0.008552285842597485\n",
            "0.008549985475838184\n",
            "0.008547743782401085\n",
            "0.008545531891286373\n",
            "0.008543290197849274\n",
            "0.008541017770767212\n",
            "0.0085388058796525\n",
            "0.008536534383893013\n",
            "0.008534306660294533\n",
            "0.008532021194696426\n",
            "0.008529763668775558\n",
            "0.008527551777660847\n",
            "0.008525324054062366\n",
            "0.008523156866431236\n",
            "0.008520870469510555\n",
            "0.008518642745912075\n",
            "0.008516386151313782\n",
            "0.008514218963682652\n",
            "0.008511886931955814\n",
            "0.00850971881300211\n",
            "0.008507506921887398\n",
            "0.008505250327289104\n",
            "0.008503022603690624\n",
            "0.00850077997893095\n",
            "0.008498569019138813\n",
            "0.008496340364217758\n",
            "0.008494128473103046\n",
            "0.008491916581988335\n",
            "0.00848967395722866\n",
            "0.008487431332468987\n",
            "0.008485248312354088\n",
            "0.00848299078643322\n",
            "0.00848082359880209\n",
            "0.008478567004203796\n",
            "0.00847641285508871\n",
            "0.008474155329167843\n",
            "0.008471973240375519\n",
            "0.008469730615615845\n",
            "0.008467547595500946\n",
            "0.008465304970741272\n",
            "0.008463166654109955\n",
            "0.008460910059511662\n",
            "0.008458726108074188\n",
            "0.008456528186798096\n",
            "0.008454331196844578\n",
            "0.008452088572084904\n",
            "0.008449905551970005\n",
            "0.008447648026049137\n",
            "0.008445479907095432\n",
            "0.00844328198581934\n",
            "0.008441098965704441\n",
            "0.008438901044428349\n",
            "0.008436732925474644\n",
            "0.008434505201876163\n",
            "0.008432337082922459\n",
            "0.008430109359323978\n",
            "0.008427941240370274\n",
            "0.0084257572889328\n",
            "0.008423559367656708\n",
            "0.008421375416219234\n",
            "0.008419222198426723\n",
            "0.008416995406150818\n",
            "0.008414781652390957\n",
            "0.008412628434598446\n",
            "0.00841047428548336\n",
            "0.008408261463046074\n",
            "0.008406064473092556\n",
            "0.008403909392654896\n",
            "0.008401771076023579\n",
            "0.008399573154747486\n",
            "0.0083973603323102\n",
            "0.008395235985517502\n",
            "0.008393037132918835\n",
            "0.008390854112803936\n",
            "0.008388715796172619\n",
            "0.008386531844735146\n",
            "0.008384348824620247\n",
            "0.00838220864534378\n",
            "0.008380010724067688\n",
            "0.008377842605113983\n",
            "0.008375673554837704\n",
            "0.008373535238206387\n",
            "0.008371395990252495\n",
            "0.008369168266654015\n",
            "0.008367073722183704\n",
            "0.008364874869585037\n",
            "0.008362720720469952\n",
            "0.00836056750267744\n",
            "0.008358398452401161\n",
            "0.008356200531125069\n",
            "0.008354091085493565\n",
            "0.008351980708539486\n",
            "0.008349782787263393\n",
            "0.008347643539309502\n",
            "0.008345460519194603\n",
            "0.008343365974724293\n",
            "0.008341167122125626\n",
            "0.008339057676494122\n",
            "0.008336903527379036\n",
            "0.008334720507264137\n",
            "0.008332566358149052\n",
            "0.00833042711019516\n",
            "0.008328316733241081\n",
            "0.008326148614287376\n",
            "0.008324024267494678\n",
            "0.008321870118379593\n",
            "0.008319729939103127\n",
            "0.00831759162247181\n",
            "0.008315452374517918\n",
            "0.008313356898725033\n",
            "0.008311217650771141\n",
            "0.008309093303978443\n",
            "0.008306954056024551\n",
            "0.008304828777909279\n",
            "0.00830264575779438\n",
            "0.00830056518316269\n",
            "0.008298397064208984\n",
            "0.0082963015884161\n",
            "0.008294133469462395\n",
            "0.008292023092508316\n",
            "0.008289928548038006\n",
            "0.008287803269922733\n",
            "0.008285664021968842\n",
            "0.008283554576337337\n",
            "0.008281445130705833\n",
            "0.008279334753751755\n",
            "0.00827718060463667\n",
            "0.008275113999843597\n",
            "0.008272945880889893\n",
            "0.008270850405097008\n",
            "0.008268755860626698\n",
            "0.008266571909189224\n",
            "0.00826447643339634\n",
            "0.00826238188892603\n",
            "0.008260287344455719\n",
            "0.008258162066340446\n",
            "0.008256066590547562\n",
            "0.008253972046077251\n",
            "0.008251831866800785\n",
            "0.008249752223491669\n",
            "0.00824764184653759\n",
            "0.00824554730206728\n",
            "0.008243421092629433\n",
            "0.008241341449320316\n",
            "0.008239232003688812\n",
            "0.008237121626734734\n",
            "0.00823507085442543\n",
            "0.008232960477471352\n",
            "0.008230851031839848\n",
            "0.008228740654885769\n",
            "0.008226675912737846\n",
            "0.00822453573346138\n",
            "0.008222455158829689\n",
            "0.008220329880714417\n",
            "0.008218249306082726\n",
            "0.008216184563934803\n",
            "0.008214088156819344\n",
            "0.008211993612349033\n",
            "0.008209927938878536\n",
            "0.008207847364246845\n",
            "0.00820575188845396\n",
            "0.00820367131382227\n",
            "0.008201620541512966\n",
            "0.008199510164558887\n",
            "0.008197414688766003\n",
            "0.008195349015295506\n",
            "0.008193282410502434\n",
            "0.00819117296487093\n",
            "0.008189122192561626\n",
            "0.00818705651909113\n",
            "0.008185005746781826\n",
            "0.008182880468666553\n",
            "0.008180814795196056\n",
            "0.008178763091564178\n",
            "0.008176697418093681\n",
            "0.00817461684346199\n",
            "0.0081725362688303\n",
            "0.008170455694198608\n",
            "0.008168434724211693\n",
            "0.00816630944609642\n",
            "0.00816427357494831\n",
            "0.008162133395671844\n",
            "0.008160126395523548\n",
            "0.008158104494214058\n",
            "0.008155995048582554\n",
            "0.008153988048434258\n",
            "0.008151892572641373\n",
            "0.008149811066687107\n",
            "0.008147790096700191\n",
            "0.008145738393068314\n",
            "0.008143657818436623\n",
            "0.00814160704612732\n",
            "0.008139599114656448\n",
            "0.008137519471347332\n",
            "0.008135497570037842\n",
            "0.00813341699540615\n",
            "0.008131350390613079\n",
            "0.00812931451946497\n",
            "0.008127248845994473\n",
            "0.008125212043523788\n",
            "0.008123190142214298\n",
            "0.008121123537421227\n",
            "0.008119132369756699\n",
            "0.00811708066612482\n",
            "0.008114971220493317\n",
            "0.008112993091344833\n",
            "0.008110897615551949\n",
            "0.00810884591192007\n",
            "0.008106883615255356\n",
            "0.008104803040623665\n",
            "0.0081027802079916\n",
            "0.008100759238004684\n",
            "0.008098722435534\n",
            "0.008096671663224697\n",
            "0.008094576187431812\n",
            "0.008092598058283329\n",
            "0.008090561255812645\n",
            "0.008088539354503155\n",
            "0.008086488582193851\n",
            "0.008084481582045555\n",
            "0.008082429878413677\n",
            "0.008080393075942993\n",
            "0.008078460581600666\n",
            "0.008076335303485394\n",
            "0.008074358105659485\n",
            "0.008072365075349808\n",
            "0.00807029940187931\n",
            "0.008068322204053402\n",
            "0.008066314272582531\n",
            "0.00806423369795084\n",
            "0.00806221179664135\n",
            "0.008060249499976635\n",
            "0.008058182895183563\n",
            "0.008056219667196274\n",
            "0.008054139092564583\n",
            "0.008052205666899681\n",
            "0.008050183765590191\n",
            "0.008048132061958313\n",
            "0.008046125061810017\n",
            "0.008044102229177952\n",
            "0.00804209616035223\n",
            "0.008040103130042553\n",
            "0.008038096129894257\n",
            "0.008036118932068348\n",
            "0.008034097030758858\n",
            "0.00803210400044918\n",
            "0.00803008209913969\n",
            "0.008028075098991394\n",
            "0.008026111871004105\n",
            "0.008024105802178383\n",
            "0.008022097870707512\n",
            "0.008020075969398022\n",
            "0.00801808387041092\n",
            "0.008016090840101242\n",
            "0.008014082908630371\n",
            "0.008012091740965843\n",
            "0.008010128512978554\n",
            "0.008008105680346489\n",
            "0.008006158284842968\n",
            "0.008004151284694672\n",
            "0.008002173155546188\n",
            "0.008000195026397705\n",
            "0.007998173125088215\n",
            "0.007996181026101112\n",
            "0.007994217798113823\n",
            "0.00799222569912672\n",
            "0.007990232668817043\n",
            "0.00798824056982994\n",
            "0.007986277341842651\n",
            "0.00798426941037178\n",
            "0.007982292212545872\n",
            "0.007980328984558582\n",
            "0.007978307083249092\n",
            "0.00797637365758419\n",
            "0.007974395528435707\n",
            "0.007972417399287224\n",
            "0.007970484904944897\n",
            "0.00796849187463522\n",
            "0.007966543547809124\n",
            "0.007964550517499447\n",
            "0.00796254351735115\n",
            "0.00796053558588028\n",
            "0.007958631962537766\n",
            "0.007956639863550663\n",
            "0.007954691536724567\n",
            "0.007952728308737278\n",
            "0.0079507352784276\n",
            "0.007948786951601505\n",
            "0.007946808822453022\n",
            "0.007944831624627113\n",
            "0.007942883297801018\n",
            "0.007940949872136116\n",
            "0.007938985712826252\n",
            "0.00793699361383915\n",
            "0.007935031317174435\n",
            "0.007933111861348152\n",
            "0.00793117843568325\n",
            "0.00792917050421238\n",
            "0.007927222177386284\n",
            "0.007925273850560188\n",
            "0.007923310622572899\n",
            "0.007921363227069378\n",
            "0.007919428870081902\n",
            "0.007917480543255806\n",
            "0.007915502414107323\n",
            "0.007913539186120033\n",
            "0.007911604829132557\n",
            "0.007909701205790043\n",
            "0.007907751947641373\n",
            "0.007905744947493076\n",
            "0.007903856225311756\n",
            "0.007901892066001892\n",
            "0.00789995864033699\n",
            "0.007897995412349701\n",
            "0.007896047085523605\n",
            "0.007894128561019897\n",
            "0.007892180234193802\n",
            "0.007890230976045132\n",
            "0.007888312451541424\n",
            "0.00788633432239294\n",
            "0.007884474471211433\n",
            "0.007882481440901756\n",
            "0.007880548015236855\n",
            "0.007878613658249378\n",
            "0.00787670910358429\n",
            "0.007874775677919388\n",
            "0.00787285715341568\n",
            "0.00787089392542839\n",
            "0.007869004271924496\n",
            "0.007867070846259594\n",
            "0.00786509271711111\n",
            "0.00786321796476841\n",
            "0.00786126870661974\n",
            "0.00785937998443842\n",
            "0.007857400923967361\n",
            "0.007855527102947235\n",
            "0.007853592745959759\n",
            "0.00785168819129467\n",
            "0.007849769666790962\n",
            "0.007847835309803486\n",
            "0.00784590095281601\n",
            "0.007843982428312302\n",
            "0.007842063903808594\n",
            "0.007840189151465893\n",
            "0.007838298566639423\n",
            "0.007836336269974709\n",
            "0.007834416814148426\n",
            "0.007832497358322144\n",
            "0.007830578833818436\n",
            "0.007828718982636929\n",
            "0.007826799526810646\n",
            "0.007824866101145744\n",
            "0.007822916842997074\n",
            "0.007821042090654373\n",
            "0.007819167338311672\n",
            "0.007817262783646584\n",
            "0.007815373130142689\n",
            "0.007813424803316593\n",
            "0.007811535149812698\n",
            "0.007809600792825222\n",
            "0.007807726040482521\n",
            "0.0078058065846562386\n",
            "0.007803947199136019\n",
            "0.0078019979409873486\n",
            "0.007800137624144554\n",
            "0.007798247970640659\n",
            "0.007796388119459152\n",
            "0.007794469594955444\n",
            "0.007792550604790449\n",
            "0.007790645584464073\n",
            "0.007788785267621279\n",
            "0.007786881178617477\n",
            "0.007784976623952389\n",
            "0.007783116307109594\n",
            "0.007781197316944599\n",
            "0.007779307663440704\n",
            "0.007777388207614422\n",
            "0.007775557227432728\n",
            "0.007773652672767639\n",
            "0.007771777454763651\n",
            "0.007769887801259756\n",
            "0.007768013048917055\n",
            "0.00776609405875206\n",
            "0.007764292880892754\n",
            "0.007762374356389046\n",
            "0.00776049867272377\n",
            "0.007758593652397394\n",
            "0.007756733801215887\n",
            "0.0077548883855342865\n",
            "0.007752998732030392\n",
            "0.007751079276204109\n",
            "0.007749219890683889\n",
            "0.0077473437413573265\n",
            "0.007745498791337013\n",
            "0.007743608672171831\n",
            "0.00774173391982913\n",
            "0.007739888969808817\n",
            "0.007738027721643448\n",
            "0.007736182305961847\n",
            "0.007734278682619333\n",
            "0.007732432335615158\n",
            "0.007730528246611357\n",
            "0.0077286818996071815\n",
            "0.0077267782762646675\n",
            "0.007724962197244167\n",
            "0.007723086979240179\n",
            "0.007721181958913803\n",
            "0.007719351910054684\n",
            "0.007717476226389408\n",
            "0.007715645711869001\n",
            "0.007713770493865013\n",
            "0.007711910177022219\n",
            "0.007710064295679331\n",
            "0.007708233781158924\n",
            "0.0077063883654773235\n",
            "0.007704528048634529\n",
            "0.007702637929469347\n",
            "0.007700806949287653\n",
            "0.007698931731283665\n",
            "0.007697116117924452\n",
            "0.007695241365581751\n",
            "0.007693424355238676\n",
            "0.007691505830734968\n",
            "0.007689689286053181\n",
            "0.007687828969210386\n",
            "0.007686012890189886\n",
            "0.007684167008846998\n",
            "0.007682336028665304\n",
            "0.007680461276322603\n",
            "0.007678614929318428\n",
            "0.007676755078136921\n",
            "0.007674953900277615\n",
            "0.007673078216612339\n",
            "0.007671262137591839\n",
            "0.007669432088732719\n",
            "0.007667615544050932\n",
            "0.00766575476154685\n",
            "0.00766390934586525\n",
            "0.0076620932668447495\n",
            "0.007660232950001955\n",
            "0.0076584466733038425\n",
            "0.007656556088477373\n",
            "0.007654695305973291\n",
            "0.007652909029275179\n",
            "0.007651078049093485\n",
            "0.0076492177322506905\n",
            "0.007647386752068996\n",
            "0.007645614445209503\n",
            "0.0076437839306890965\n",
            "0.007641967851668596\n",
            "0.007640136871486902\n",
            "0.007638276554644108\n",
            "0.007636474911123514\n",
            "0.007634628564119339\n",
            "0.007632813416421413\n",
            "0.007630996871739626\n",
            "0.0076291803270578384\n",
            "0.007627363782376051\n",
            "0.00762559287250042\n",
            "0.007623746525496244\n",
            "0.007621959783136845\n",
            "0.007620114367455244\n",
            "0.007618297822773457\n",
            "0.007616422604769468\n",
            "0.007614620961248875\n",
            "0.007612834684550762\n",
            "0.007610988803207874\n",
            "0.0076092020608484745\n",
            "0.0076073999516665936\n",
            "0.0076055992394685745\n",
            "0.0076037822291255\n",
            "0.007601951248943806\n",
            "0.007600150071084499\n",
            "0.007598333526402712\n",
            "0.007596516516059637\n",
            "0.007594715338200331\n",
            "0.007592929061502218\n",
            "0.007591097615659237\n",
            "0.007589282002300024\n",
            "0.007587568834424019\n",
            "0.007585752755403519\n",
            "0.007583936210721731\n",
            "0.007582090795040131\n",
            "0.007580273784697056\n",
            "0.007578516378998756\n",
            "0.007576744072139263\n",
            "0.007574899587780237\n",
            "0.007573141250759363\n",
            "0.00757132563740015\n",
            "0.007569494191557169\n",
            "0.0075677367858588696\n",
            "0.007565935142338276\n",
            "0.00756416330114007\n",
            "0.007562420796602964\n",
            "0.0075605749152600765\n",
            "0.00755881704390049\n",
            "0.007557014934718609\n",
            "0.007555229123681784\n",
            "0.007553442381322384\n",
            "0.0075516109354794025\n",
            "0.0075498828664422035\n",
            "0.007548096589744091\n",
            "0.007546250708401203\n",
            "0.007544463034719229\n",
            "0.007542735897004604\n",
            "0.00754090491682291\n",
            "0.007539191748946905\n",
            "0.007537405006587505\n",
            "0.007535603363066912\n",
            "0.007533860392868519\n",
            "0.007532014511525631\n",
            "0.007530242204666138\n",
            "0.007528469432145357\n",
            "0.007526712492108345\n",
            "0.007524955086410046\n",
            "0.007523197680711746\n",
            "0.007521411404013634\n",
            "0.0075196390971541405\n",
            "0.007517881691455841\n",
            "0.007516080047935247\n",
            "0.007514293305575848\n",
            "0.007512535434216261\n",
            "0.0075107780285179615\n",
            "0.007508991286158562\n",
            "0.0075072189792990685\n",
            "0.007505490910261869\n",
            "0.007503704633563757\n",
            "0.007501976564526558\n",
            "0.007500159554183483\n",
            "0.007498417515307665\n",
            "0.007496689446270466\n",
            "0.007494887337088585\n",
            "0.0074931299313902855\n",
            "0.00749138742685318\n",
            "0.007489585317671299\n",
            "0.007487842813134193\n",
            "0.007486115209758282\n",
            "0.007484327536076307\n",
            "0.0074825710617005825\n",
            "0.0074808429926633835\n",
            "0.00747907068580389\n",
            "0.007477342616766691\n",
            "0.007475554943084717\n",
            "0.007473768666386604\n",
            "0.007472040131688118\n",
            "0.007470312528312206\n",
            "0.007468570023775101\n",
            "0.0074667977169156075\n",
            "0.007465054746717215\n",
            "0.007463326212018728\n",
            "0.007461554370820522\n",
            "0.007459811866283417\n",
            "0.007458054460585117\n",
            "0.0074563403613865376\n",
            "0.00745462765917182\n",
            "0.007452869787812233\n",
            "0.007451112847775221\n",
            "0.00744934007525444\n",
            "0.007447612006217241\n",
            "0.007445869036018848\n",
            "0.007444112095981836\n",
            "0.007442413363605738\n",
            "0.007440656423568726\n",
            "0.007438942790031433\n",
            "0.0074372002854943275\n",
            "0.007435456849634647\n",
            "0.007433713413774967\n",
            "0.007431956939399242\n",
            "0.007430228870362043\n",
            "0.007428515702486038\n",
            "0.007426758296787739\n",
            "0.007425059098750353\n",
            "0.0074233608320355415\n",
            "0.007421588525176048\n",
            "0.00741988979279995\n",
            "0.007418161723762751\n",
            "0.007416418753564358\n",
            "0.007414676249027252\n",
            "0.007412933278828859\n",
            "0.0074111903086304665\n",
            "0.007409492041915655\n",
            "0.007407748606055975\n",
            "0.00740606477484107\n",
            "0.007404366508126259\n",
            "0.007402653805911541\n",
            "0.007400910835713148\n",
            "0.007399197202175856\n",
            "0.007397438865154982\n",
            "0.00739574059844017\n",
            "0.007393998093903065\n",
            "0.007392314728349447\n",
            "0.0073906308971345425\n",
            "0.007388828322291374\n",
            "0.007387159392237663\n",
            "0.007385445758700371\n",
            "0.007383703254163265\n",
            "0.007381974719464779\n",
            "0.00738024665042758\n",
            "0.0073785921558737755\n",
            "0.007376879453659058\n",
            "0.007375165820121765\n",
            "0.00737345265224576\n",
            "0.007371753919869661\n",
            "0.00737005565315485\n",
            "0.00736834155395627\n",
            "0.007366657722741365\n",
            "0.007364943623542786\n",
            "0.0073632169514894485\n",
            "0.007361473049968481\n",
            "0.00735980411991477\n",
            "0.0073581356555223465\n",
            "0.007356421556323767\n",
            "0.007354722823947668\n",
            "0.0073530240915715694\n",
            "0.007351340726017952\n",
            "0.0073496270924806595\n",
            "0.007347928825765848\n",
            "0.007346229627728462\n",
            "0.00734451599419117\n",
            "0.007342802360653877\n",
            "0.0073411185294389725\n",
            "0.00733940489590168\n",
            "0.007337736431509256\n",
            "0.00733603723347187\n",
            "0.007334338966757059\n",
            "0.007332625798881054\n",
            "0.007330926600843668\n",
            "0.007329257670789957\n",
            "0.007327588740736246\n",
            "0.0073259491473436356\n",
            "0.0073242648504674435\n",
            "0.007322551216930151\n",
            "0.0073208375833928585\n",
            "0.007319197990000248\n",
            "0.007317499723285437\n",
            "0.007315770722925663\n",
            "0.007314101792871952\n",
            "0.007312403060495853\n",
            "0.007310704328119755\n",
            "0.007309035398066044\n",
            "0.0073073809035122395\n",
            "0.007305696606636047\n",
            "0.007304027210921049\n",
            "0.007302343379706144\n",
            "0.0073006595484912395\n",
            "0.0072990055195987225\n",
            "0.007297291420400143\n",
            "0.007295652292668819\n",
            "0.007293953560292721\n",
            "0.007292269729077816\n",
            "0.007290614303201437\n",
            "0.00728896027430892\n",
            "0.007287290878593922\n",
            "0.007285636849701405\n",
            "0.007283937651664019\n",
            "0.007282268721610308\n",
            "0.007280613761395216\n",
            "0.0072789001278579235\n",
            "0.007277230732142925\n",
            "0.007275576237589121\n",
            "0.007273892872035503\n",
            "0.007272238377481699\n",
            "0.007270568050444126\n",
            "0.007268883753567934\n",
            "0.007267289329320192\n",
            "0.0072656492702662945\n",
            "0.007263906300067902\n",
            "0.0072622657753527164\n",
            "0.007260611746460199\n",
            "0.007258927449584007\n",
            "0.007257287856191397\n",
            "0.007255618926137686\n",
            "0.0072539933025836945\n",
            "0.007252353709191084\n",
            "0.007250684779137373\n",
            "0.007249029818922281\n",
            "0.0072473459877073765\n",
            "0.007245691027492285\n",
            "0.007244036998599768\n",
            "0.0072423964738845825\n",
            "0.007240742444992065\n",
            "0.007239073049277067\n",
            "0.007237418554723263\n",
            "0.0072357929311692715\n",
            "0.007234138436615467\n",
            "0.007232469040900469\n",
            "0.007230814546346664\n",
            "0.007229174952954054\n",
            "0.007227580063045025\n",
            "0.007225940469652414\n",
            "0.0072242701426148415\n",
            "0.007222615648061037\n",
            "0.0072209457866847515\n",
            "0.007219291757792234\n",
            "0.007217696402221918\n",
            "0.007216041442006826\n",
            "0.007214401848614216\n",
            "0.007212746422737837\n",
            "0.007211106829345226\n",
            "0.00720948213711381\n",
            "0.007207871880382299\n",
            "0.007206202484667301\n",
            "0.007204532623291016\n",
            "0.007202922832220793\n",
            "0.0072013274766504765\n",
            "0.0071996282786130905\n",
            "0.007198032457381487\n",
            "0.007196377497166395\n",
            "0.007194782141596079\n",
            "0.007193142548203468\n",
            "0.007191502954810858\n",
            "0.0071898773312568665\n",
            "0.007188266608864069\n",
            "0.007186656817793846\n",
            "0.007185017224401236\n",
            "0.007183332461863756\n",
            "0.007181722205132246\n",
            "0.0071800826117396355\n",
            "0.007178472355008125\n",
            "0.00717683183029294\n",
            "0.007175206206738949\n",
            "0.0071736108511686325\n",
            "0.007172000594437122\n",
            "0.007170375436544418\n",
            "0.007168765179812908\n",
            "0.00716709578409791\n",
            "0.007165455725044012\n",
            "0.007163845002651215\n",
            "0.007162235211580992\n",
            "0.007160654291510582\n",
            "0.007159058004617691\n",
            "0.007157432846724987\n",
            "0.007155807688832283\n",
            "0.007154152728617191\n",
            "0.0071525718085467815\n",
            "0.007150932215154171\n",
            "0.0071493214927613735\n",
            "0.007147741504013538\n",
            "0.007146071642637253\n",
            "0.007144461385905743\n",
            "0.007142850663512945\n",
            "0.007141284644603729\n",
            "0.007139688823372126\n",
            "0.0071380785666406155\n",
            "0.007136468309909105\n",
            "0.0071348282508552074\n",
            "0.007133172824978828\n",
            "0.007131591904908419\n",
            "0.007129996083676815\n",
            "0.007128430064767599\n",
            "0.007126804906874895\n",
            "0.007125194650143385\n",
            "0.007123613730072975\n",
            "0.007122003473341465\n",
            "0.007120436988770962\n",
            "0.007118796929717064\n",
            "0.007117187138646841\n",
            "0.007115590386092663\n",
            "0.0071139950305223465\n",
            "0.007112384308129549\n",
            "0.007110788952559233\n",
            "0.007109192665666342\n",
            "0.007107597775757313\n",
            "0.007105971686542034\n",
            "0.007104420568794012\n",
            "0.007102854549884796\n",
            "0.007101214956492186\n",
            "0.007099648006260395\n",
            "0.007098022848367691\n",
            "0.007096471264958382\n",
            "0.007094860542565584\n",
            "0.0070932647213339806\n",
            "0.007091714069247246\n",
            "0.007090087980031967\n",
            "0.007088507525622845\n",
            "0.00708689633756876\n",
            "0.0070853447541594505\n",
            "0.007083734963089228\n",
            "0.007082168012857437\n",
            "0.007080587092787027\n",
            "0.007078991737216711\n",
            "0.00707739545032382\n",
            "0.007075830362737179\n",
            "0.007074248511344194\n",
            "0.007072667125612497\n",
            "0.007071086671203375\n",
            "0.007069520186632872\n",
            "0.007067939266562462\n",
            "0.007066328544169664\n",
            "0.007064777426421642\n",
            "0.007063195575028658\n",
            "0.007061600219458342\n",
            "0.0070600491017103195\n",
            "0.007058482617139816\n",
            "0.007056871894747019\n",
            "0.007055319845676422\n",
            "0.0070537687279284\n",
            "0.007052202709019184\n",
            "0.007050562184303999\n",
            "0.00704903993755579\n",
            "0.00704745901748538\n",
            "0.00704587809741497\n",
            "0.007044326514005661\n",
            "0.00704273022711277\n",
            "0.007041178643703461\n",
            "0.0070395683869719505\n",
            "0.0070380461402237415\n",
            "0.007036465220153332\n",
            "0.007034913636744022\n",
            "0.0070333331823349\n",
            "0.007031751796603203\n",
            "0.007030200213193893\n",
            "0.0070286039263010025\n",
            "0.007027111481875181\n",
            "0.007025574799627066\n",
            "0.007023979444056749\n",
            "0.007022397592663765\n",
            "0.007020831573754549\n",
            "0.007019339129328728\n",
            "0.007017757277935743\n",
            "0.007016176823526621\n",
            "0.007014566101133823\n",
            "0.007013043854385614\n",
            "0.007011536508798599\n",
            "0.0070099555887281895\n",
            "0.00700837466865778\n",
            "0.007006852421909571\n",
            "0.007005300838500261\n",
            "0.007003719452768564\n",
            "0.007002138067036867\n",
            "0.007000601850450039\n",
            "0.006999064702540636\n",
            "0.006997483782470226\n",
            "0.00699596107006073\n",
            "0.006994395051151514\n",
            "0.006992857903242111\n",
            "0.0069913361221551895\n",
            "0.006989725399762392\n",
            "0.006988218054175377\n",
            "0.006986710708588362\n",
            "0.006985129788517952\n",
            "0.006983607541769743\n",
            "0.006982026156038046\n",
            "0.006980534177273512\n",
            "0.006978997029364109\n",
            "0.006977444980293512\n",
            "0.006975923199206591\n",
            "0.006974327377974987\n",
            "0.006972790230065584\n",
            "0.006971282884478569\n",
            "0.006969716399908066\n",
            "0.006968164816498756\n",
            "0.00696661276742816\n",
            "0.006965090986341238\n",
            "0.006963553372770548\n",
            "0.006962016690522432\n",
            "0.006960494909435511\n",
            "0.0069589875638484955\n",
            "0.00695745088160038\n",
            "0.006955913733690977\n",
            "0.006954376585781574\n",
            "0.0069528548046946526\n",
            "0.006951303221285343\n",
            "0.00694976607337594\n",
            "0.006948214024305344\n",
            "0.00694672204554081\n",
            "0.006945185363292694\n",
            "0.00694363284856081\n",
            "0.006942140404134989\n",
            "0.006940588820725679\n",
            "0.006939081009477377\n",
            "0.006937544327229261\n",
            "0.006936036981642246\n",
            "0.0069344849325716496\n",
            "0.006932932883501053\n",
            "0.006931396666914225\n",
            "0.006929888855665922\n",
            "0.006928351707756519\n",
            "0.006926903501152992\n",
            "0.0069253514520823956\n",
            "0.006923829670995474\n",
            "0.006922336295247078\n",
            "0.00692082941532135\n",
            "0.0069192927330732346\n",
            "0.006917756050825119\n",
            "0.0069162482395768166\n",
            "0.006914740893989801\n",
            "0.006913218181580305\n",
            "0.006911726202815771\n",
            "0.006910232827067375\n",
            "0.006908666342496872\n",
            "0.006907174363732338\n",
            "0.006905607413500547\n",
            "0.00690415920689702\n",
            "0.00690259225666523\n",
            "0.006901114247739315\n",
            "0.006899621803313494\n",
            "0.006898099090903997\n",
            "0.006896577309817076\n",
            "0.006895025726407766\n",
            "0.0068935914896428585\n",
            "0.00689209857955575\n",
            "0.006890532560646534\n",
            "0.006889040116220713\n",
            "0.006887577474117279\n",
            "0.0068860542960464954\n",
            "0.0068845320492982864\n",
            "0.0068830098025500774\n",
            "0.006881546229124069\n",
            "0.006880009081214666\n",
            "0.0068785459734499454\n",
            "0.006876994390040636\n",
            "0.0068755606189370155\n",
            "0.006874053739011288\n",
            "0.006872531957924366\n",
            "0.006870993413031101\n",
            "0.006869515869766474\n",
            "0.0068680234253406525\n",
            "0.006866516079753637\n",
            "0.006865008268505335\n",
            "0.0068635595962405205\n",
            "0.006862052250653505\n",
            "0.006860530003905296\n",
            "0.006859066430479288\n",
            "0.006857588887214661\n",
            "0.006856051739305258\n",
            "0.006854573264718056\n",
            "0.006853080820292234\n",
            "0.006851632613688707\n",
            "0.00685009453445673\n",
            "0.006848602090030909\n",
            "0.006847168318927288\n",
            "0.006845676340162754\n",
            "0.006844182498753071\n",
            "0.006842660717666149\n",
            "0.006841168273240328\n",
            "0.006839675363153219\n",
            "0.006838182453066111\n",
            "0.0068367342464625835\n",
            "0.006835256237536669\n",
            "0.006833762861788273\n",
            "0.006832226179540157\n",
            "0.006830763071775436\n",
            "0.006829343736171722\n",
            "0.0068278065882623196\n",
            "0.006826328579336405\n",
            "0.006824880838394165\n",
            "0.006823401432484388\n",
            "0.006821909919381142\n",
            "0.0068204160779714584\n",
            "0.006818923633545637\n",
            "0.006817474961280823\n",
            "0.0068159825168550014\n",
            "0.006814518012106419\n",
            "0.006813055370002985\n",
            "0.0068115778267383575\n",
            "0.006810070015490055\n",
            "0.0068086665123701096\n",
            "0.006807143334299326\n",
            "0.006805650424212217\n",
            "0.006804231554269791\n",
            "0.006802723743021488\n",
            "0.006801275536417961\n",
            "0.00679978309199214\n",
            "0.006798348855227232\n",
            "0.006796885281801224\n",
            "0.0067953928373754025\n",
            "0.006793929263949394\n",
            "0.006792495492845774\n",
            "0.006790987215936184\n",
            "0.006789554376155138\n",
            "0.00678809080272913\n",
            "0.006786597892642021\n",
            "0.0067851347848773\n",
            "0.006783685646951199\n",
            "0.00678219273686409\n",
            "0.006780744530260563\n",
            "0.006779281422495842\n",
            "0.006777788046747446\n",
            "0.006776398979127407\n",
            "0.006774890702217817\n",
            "0.006773456931114197\n",
            "0.006771978922188282\n",
            "0.006770515348762274\n",
            "0.006769096478819847\n",
            "0.006767588667571545\n",
            "0.006766184698790312\n",
            "0.006764721125364304\n",
            "0.006763258017599583\n",
            "0.006761779077351093\n",
            "0.0067603313364088535\n",
            "0.006758882664144039\n",
            "0.006757419090718031\n",
            "0.006756014656275511\n",
            "0.006754581350833178\n",
            "0.006753058172762394\n",
            "0.006751609966158867\n",
            "0.00675019109621644\n",
            "0.006748741492629051\n",
            "0.006747264415025711\n",
            "0.006745859514921904\n",
            "0.006744396407157183\n",
            "0.006742962170392275\n",
            "0.006741528399288654\n",
            "0.006740049924701452\n",
            "0.006738616619259119\n",
            "0.006737153045833111\n",
            "0.006735675502568483\n",
            "0.006734255701303482\n",
            "0.006732807494699955\n",
            "0.006731388624757528\n",
            "0.006729939486831427\n",
            "0.006728446576744318\n",
            "0.006727027241140604\n",
            "0.006725622806698084\n",
            "0.006724144797772169\n",
            "0.006722725462168455\n",
            "0.006721321493387222\n",
            "0.0067198872566223145\n",
            "0.0067184385843575\n",
            "0.006716975010931492\n",
            "0.006715526804327965\n",
            "0.006714136805385351\n",
            "0.006712643895298243\n",
            "0.006711224559694529\n",
            "0.006709820590913296\n",
            "0.006708371452987194\n",
            "0.0067069376818835735\n",
            "0.0067055197432637215\n",
            "0.0067040701396763325\n",
            "0.006702621933072805\n",
            "0.006701217498630285\n",
            "0.006699812598526478\n",
            "0.006698334589600563\n",
            "0.0066969008184969425\n",
            "0.006695525720715523\n",
            "0.0066940621472895145\n",
            "0.006692687049508095\n",
            "0.006691238842904568\n",
            "0.006689819972962141\n",
            "0.006688370835036039\n",
            "0.006686937063932419\n",
            "0.006685532629489899\n",
            "0.006684083957225084\n",
            "0.006682694423943758\n",
            "0.006681274622678757\n",
            "0.00667982641607523\n",
            "0.00667845131829381\n",
            "0.006677017547190189\n",
            "0.006675598677247763\n",
            "0.006674149073660374\n",
            "0.0066727157682180405\n",
            "0.006671295501291752\n",
            "0.006669847294688225\n",
            "0.006668472196906805\n",
            "0.006667069159448147\n",
            "0.006665619555860758\n",
            "0.0066641997545957565\n",
            "0.006662796251475811\n",
            "0.0066613913513720036\n",
            "0.0066599720157682896\n",
            "0.006658552680164576\n",
            "0.0066571482457220554\n",
            "0.006655744276940823\n",
            "0.006654324941337109\n",
            "0.006652905605733395\n",
            "0.0066515011712908745\n",
            "0.006650111638009548\n",
            "0.006648633163422346\n",
            "0.00664724363014102\n",
            "0.006645912770181894\n",
            "0.006644493900239468\n",
            "0.0066430894657969475\n",
            "0.006641669664531946\n",
            "0.006640265695750713\n",
            "0.006638817023485899\n",
            "0.0066374121233820915\n",
            "0.006636022124439478\n",
            "0.0066346037201583385\n",
            "0.006633228622376919\n",
            "0.006631794385612011\n",
            "0.006630404852330685\n",
            "0.006629014853388071\n",
            "0.006627551279962063\n",
            "0.006626265123486519\n",
            "0.006624830886721611\n",
            "0.006623441353440285\n",
            "0.00662199268117547\n",
            "0.006620618049055338\n",
            "0.006619198247790337\n",
            "0.006617838516831398\n",
            "0.00661640428006649\n",
            "0.006615014746785164\n",
            "0.00661362474784255\n",
            "0.006612250115722418\n",
            "0.006610830780118704\n",
            "0.00660944078117609\n",
            "0.006608080584555864\n",
            "0.006606631446629763\n",
            "0.006605286616832018\n",
            "0.006603837013244629\n",
            "0.006602507084608078\n",
            "0.006601116620004177\n",
            "0.006599667947739363\n",
            "0.006598322652280331\n",
            "0.006596903782337904\n",
            "0.0065954988822340965\n",
            "0.0065941535867750645\n",
            "0.0065927631221711636\n",
            "0.006591418758034706\n",
            "0.006589999422430992\n",
            "0.006588638760149479\n",
            "0.006587189622223377\n",
            "0.006585800088942051\n",
            "0.006584454793483019\n",
            "0.006583094596862793\n",
            "0.006581675261259079\n",
            "0.006580285727977753\n",
            "0.006578954868018627\n",
            "0.006577550899237394\n",
            "0.0065761455334723\n",
            "0.006574756465852261\n",
            "0.006573395803570747\n",
            "0.006571976467967033\n",
            "0.00657061580568552\n",
            "0.006569225341081619\n",
            "0.0065678805112838745\n",
            "0.006566476542502642\n",
            "0.006565145682543516\n",
            "0.0065637556836009026\n",
            "0.006562365684658289\n",
            "0.006560931447893381\n",
            "0.006559615954756737\n",
            "0.006558270659297705\n",
            "0.006556865759193897\n",
            "0.006555520463734865\n",
            "0.006554130464792252\n",
            "0.006552740465849638\n",
            "0.006551395170390606\n",
            "0.0065499902702867985\n",
            "0.006548600737005472\n",
            "0.006547270342707634\n",
            "0.006545910146087408\n",
            "0.006544564384967089\n",
            "0.006543188821524382\n",
            "0.006541769951581955\n",
            "0.006540409289300442\n",
            "0.006539093796163797\n",
            "0.0065377335995435715\n",
            "0.0065363142639398575\n",
            "0.00653490936383605\n",
            "0.006533623207360506\n",
            "0.006532218307256699\n",
            "0.00653088791295886\n",
            "0.006529483012855053\n",
            "0.006528167054057121\n",
            "0.006526806391775608\n",
            "0.006525416858494282\n",
            "0.006524085998535156\n",
            "0.006522740703076124\n",
            "0.006521365139633417\n",
            "0.006519990507513285\n",
            "0.006518614944070578\n",
            "0.006517284549772739\n",
            "0.006515908986330032\n",
            "0.006514549721032381\n",
            "0.006513218395411968\n",
            "0.006511858198791742\n",
            "0.006510482635349035\n",
            "0.006509137339890003\n",
            "0.006507821846753359\n",
            "0.006506431847810745\n",
            "0.006505040917545557\n",
            "0.006503725424408913\n",
            "0.006502395495772362\n",
            "0.006501019466668367\n",
            "0.006499644368886948\n",
            "0.006498313508927822\n",
            "0.0064969537779688835\n",
            "0.006495608016848564\n",
            "0.006494277156889439\n",
            "0.006492931395769119\n",
            "0.006491586100310087\n",
            "0.006490225903689861\n",
            "0.006488835904747248\n",
            "0.006487504579126835\n",
            "0.006486159283667803\n",
            "0.006484857760369778\n",
            "0.006483498029410839\n",
            "0.00648218160495162\n",
            "0.006480806972831488\n",
            "0.006479490548372269\n",
            "0.006478144787251949\n",
            "0.006476769223809242\n",
            "0.006475453730672598\n",
            "0.006474093534052372\n",
            "0.006472747772932053\n",
            "0.0064714024774730206\n",
            "0.006470056250691414\n",
            "0.006468725390732288\n",
            "0.006467350758612156\n",
            "0.006466049235314131\n",
            "0.006464718375355005\n",
            "0.006463402416557074\n",
            "0.006462042219936848\n",
            "0.006460696458816528\n",
            "0.00645936606451869\n",
            "0.006458019372075796\n",
            "0.006456718780100346\n",
            "0.006455358117818832\n",
            "0.006454086862504482\n",
            "0.006452741101384163\n",
            "0.006451365537941456\n",
            "0.006450035143643618\n",
            "0.006448688916862011\n",
            "0.00644740229472518\n",
            "0.006446057464927435\n",
            "0.006444726604968309\n",
            "0.00644341018050909\n",
            "0.006442035082727671\n",
            "0.006440718658268452\n",
            "0.006439358461648226\n",
            "0.006438072305172682\n",
            "0.006436711642891169\n",
            "0.006435410585254431\n",
            "0.006434079259634018\n",
            "0.006432777736335993\n",
            "0.00643141707405448\n",
            "0.006430101580917835\n",
            "0.006428800523281097\n",
            "0.006427454296499491\n",
            "0.00642615370452404\n",
            "0.006424822378903627\n",
            "0.00642352132126689\n",
            "0.006422145292162895\n",
            "0.00642081443220377\n",
            "0.0064195431768894196\n",
            "0.006418257020413876\n",
            "0.006416911259293556\n",
            "0.006415609270334244\n",
            "0.00641421927139163\n",
            "0.00641297735273838\n",
            "0.006411646027117968\n",
            "0.0064103007316589355\n",
            "0.0064090583473443985\n",
            "0.006407698150724173\n",
            "0.006406367290765047\n",
            "0.006405065767467022\n",
            "0.006403734441846609\n",
            "0.006402433384209871\n",
            "0.006401088088750839\n",
            "0.006399786099791527\n",
            "0.006398470140993595\n",
            "0.0063971541821956635\n",
            "0.006395852658897638\n",
            "0.006394551135599613\n",
            "0.0063932351768016815\n",
            "0.006391948089003563\n",
            "0.00639058742672205\n",
            "0.006389286834746599\n",
            "0.006387999746948481\n",
            "0.006386698689311743\n",
            "0.006385397166013718\n",
            "0.006384051404893398\n",
            "0.006382794585078955\n",
            "0.006381463725119829\n",
            "0.006380161736160517\n",
            "0.006378860678523779\n",
            "0.006377559155225754\n",
            "0.006376213394105434\n",
            "0.006374942138791084\n",
            "0.006373625714331865\n",
            "0.006372339557856321\n",
            "0.006371052470058203\n",
            "0.00636972114443779\n",
            "0.00636844988912344\n",
            "0.0063671329990029335\n",
            "0.006365817040205002\n",
            "0.006364515516906977\n",
            "0.006363243795931339\n",
            "0.0063619427382946014\n",
            "0.006360671017318964\n",
            "0.006359295919537544\n",
            "0.006358038634061813\n",
            "0.006356721743941307\n",
            "0.006355465389788151\n",
            "0.006354148965328932\n",
            "0.006352891679853201\n",
            "0.006351560354232788\n",
            "0.006350243929773569\n",
            "0.0063489871099591255\n",
            "0.006347686052322388\n",
            "0.006346384063363075\n",
            "0.006345097906887531\n",
            "0.0063438112847507\n",
            "0.006342554464936256\n",
            "0.006341267842799425\n",
            "0.006339936517179012\n",
            "0.006338649429380894\n",
            "0.006337348371744156\n",
            "0.006336076185107231\n",
            "0.006334790028631687\n",
            "0.006333488039672375\n",
            "0.006332186982035637\n",
            "0.006330856122076511\n",
            "0.006329642608761787\n",
            "0.00632834155112505\n",
            "0.006327040493488312\n",
            "0.0063257538713514805\n",
            "0.006324526388198137\n",
            "0.0063231950625777245\n",
            "0.006321938242763281\n",
            "0.006320636253803968\n",
            "0.006319349631667137\n",
            "0.0063180779106915\n",
            "0.006316835526376963\n",
            "0.00631553353741765\n",
            "0.006314277183264494\n",
            "0.006312945391982794\n",
            "0.0063117328099906445\n",
            "0.006310431286692619\n",
            "0.006309159565716982\n",
            "0.0063078138045966625\n",
            "0.006306586787104607\n",
            "0.0063052698969841\n",
            "0.006304013077169657\n",
            "0.006302711088210344\n",
            "0.006301483605057001\n",
            "0.006300211884081364\n",
            "0.006298909895122051\n",
            "0.006297622807323933\n",
            "0.006296366453170776\n",
            "0.006295123603194952\n",
            "0.0062937927432358265\n",
            "0.006292580161243677\n",
            "0.006291307974606752\n",
            "0.006289991550147533\n",
            "0.0062887342646718025\n",
            "0.006287433207035065\n",
            "0.006286175921559334\n",
            "0.006284888833761215\n",
            "0.006283676717430353\n",
            "0.006282360292971134\n",
            "0.0062811472453176975\n",
            "0.006279860623180866\n",
            "0.006278603337705135\n",
            "0.0062773460522294044\n",
            "0.006276059430092573\n",
            "0.006274801678955555\n",
            "0.006273544859141111\n",
            "0.0062722135335206985\n",
            "0.006271030753850937\n",
            "0.006269699893891811\n",
            "0.006268457043915987\n",
            "0.006267170421779156\n",
            "0.006265913136303425\n",
            "0.006264685653150082\n",
            "0.006263428367674351\n",
            "0.006262141279876232\n",
            "0.0062608844600617886\n",
            "0.006259642541408539\n",
            "0.006258340552449226\n",
            "0.0062571424059569836\n",
            "0.006255870219320059\n",
            "0.006254613399505615\n",
            "0.006253326311707497\n",
            "0.00625208392739296\n",
            "0.006250826641917229\n",
            "0.006249583326280117\n",
            "0.006248341873288155\n",
            "0.006247084587812424\n",
            "0.006245827302336693\n",
            "0.006244570016860962\n",
            "0.006243297830224037\n",
            "0.0062421150505542755\n",
            "0.006240798160433769\n",
            "0.0062395259737968445\n",
            "0.006238298490643501\n",
            "0.006237071007490158\n",
            "0.006235784385353327\n",
            "0.00623457133769989\n",
            "0.006233299151062965\n",
            "0.006232071667909622\n",
            "0.0062307994812726974\n",
            "0.006229556631296873\n",
            "0.00622832914814353\n",
            "0.006227013189345598\n",
            "0.006225814577192068\n",
            "0.006224542856216431\n",
            "0.0062232855707407\n",
            "0.006222028285264969\n",
            "0.006220800336450338\n",
            "0.006219572853296995\n",
            "0.006218270864337683\n",
            "0.006217042915523052\n",
            "0.006215860601514578\n",
            "0.00621460285037756\n",
            "0.006213330663740635\n",
            "0.006212103180587292\n",
            "0.006210846360772848\n",
            "0.006209677085280418\n",
            "0.006208346225321293\n",
            "0.0062071774154901505\n",
            "0.006205861456692219\n",
            "0.006204633042216301\n",
            "0.006203449796885252\n",
            "0.006202192511409521\n",
            "0.006200920790433884\n",
            "0.006199707742780447\n",
            "0.00619846535846591\n",
            "0.006197252310812473\n",
            "0.006196024361997843\n",
            "0.0061947968788445\n",
            "0.006193525157868862\n",
            "0.006192282307893038\n",
            "0.006191099062561989\n",
            "0.0061898124404251575\n",
            "0.006188598927110434\n",
            "0.006187327206134796\n",
            "0.006186128593981266\n",
            "0.006184945348650217\n",
            "0.0061836582608520985\n",
            "0.006182489451020956\n",
            "0.0061812326312065125\n",
            "0.006180005148053169\n",
            "0.006178747396916151\n",
            "0.006177519913762808\n",
            "0.006176292430609465\n",
            "0.006175079382956028\n",
            "0.006173836998641491\n",
            "0.006172623950988054\n",
            "0.006171366665512323\n",
            "0.006170153617858887\n",
            "0.006168910767883062\n",
            "0.006167668383568525\n",
            "0.006166484672576189\n",
            "0.006165257189422846\n",
            "0.006164029240608215\n",
            "0.006162845995277166\n",
            "0.006161558907479048\n",
            "0.0061603751964867115\n",
            "0.0061591630801558495\n",
            "0.006157890893518925\n",
            "0.006156736984848976\n",
            "0.006155464332550764\n",
            "0.006154251750558615\n",
            "0.006153038237243891\n",
            "0.00615178095176816\n",
            "0.006150582805275917\n",
            "0.006149414461106062\n",
            "0.006148127373307943\n",
            "0.006146944127976894\n",
            "0.006145745515823364\n",
            "0.006144532933831215\n",
            "0.006143319886177778\n",
            "0.00614206213504076\n",
            "0.006140849553048611\n",
            "0.00613962160423398\n",
            "0.006138452794402838\n",
            "0.006137224845588207\n",
            "0.006136012263596058\n",
            "0.006134813651442528\n",
            "0.006133570801466703\n",
            "0.006132402457296848\n",
            "0.006131130736321211\n",
            "0.0061299619264900684\n",
            "0.006128748878836632\n",
            "0.006127520930022001\n",
            "0.00612633815035224\n",
            "0.006125094834715128\n",
            "0.0061239260248839855\n",
            "0.006122653838247061\n",
            "0.006121455691754818\n",
            "0.0061202868819236755\n",
            "0.006119014695286751\n",
            "0.006117876153439283\n",
            "0.0061166477389633656\n",
            "0.006115449592471123\n",
            "0.0061142221093177795\n",
            "0.006113008130341768\n",
            "0.006111854687333107\n",
            "0.006110596936196089\n",
            "0.00610941369086504\n",
            "0.006108200643211603\n",
            "0.00610700249671936\n",
            "0.006105819251388311\n",
            "0.006104606203734875\n",
            "0.006103437393903732\n",
            "0.006102223880589008\n",
            "0.0061009665951132774\n",
            "0.0060997833497822285\n",
            "0.006098585203289986\n",
            "0.006097416393458843\n",
            "0.006096188444644213\n",
            "0.006094990763813257\n",
            "0.006093792151659727\n",
            "0.006092564202845097\n",
            "0.0060913958586752415\n",
            "0.006090197712182999\n",
            "0.006089043337851763\n",
            "0.006087815389037132\n",
            "0.006086631678044796\n",
            "0.00608544796705246\n",
            "0.006084220949560404\n",
            "0.006083096377551556\n",
            "0.006081853061914444\n",
            "0.006080684717744589\n",
            "0.006079457234591246\n",
            "0.006078273523598909\n",
            "0.006077060010284185\n",
            "0.006075832061469555\n",
            "0.0060746930539608\n",
            "0.006073509808629751\n",
            "0.006072281859815121\n",
            "0.006071142852306366\n",
            "0.006069944705814123\n",
            "0.006068790331482887\n",
            "0.006067532580345869\n",
            "0.006066319532692432\n",
            "0.006065135821700096\n",
            "0.006064011715352535\n",
            "0.006062799133360386\n",
            "0.006061600986868143\n",
            "0.0060604168102145195\n",
            "0.006059233099222183\n",
            "0.006058020051568747\n",
            "0.006056851241737604\n",
            "0.006055727601051331\n",
            "0.006054513622075319\n",
            "0.006053344812244177\n",
            "0.00605210242792964\n",
            "0.0060509489849209785\n",
            "0.006049779709428549\n",
            "0.006048640236258507\n",
            "0.00604739785194397\n",
            "0.006046258844435215\n",
            "0.006045075133442879\n",
            "0.006043876986950636\n",
            "0.006042707711458206\n",
            "0.006041524466127157\n",
            "0.006040326319634914\n",
            "0.006039157044142485\n",
            "0.006037944462150335\n",
            "0.006036790087819099\n",
            "0.00603565014898777\n",
            "0.006034452468156815\n",
            "0.006033299025148153\n",
            "0.006032085511833429\n",
            "0.006030916236341\n",
            "0.006029747426509857\n",
            "0.006028564181178808\n",
            "0.006027395371347666\n",
            "0.006026227027177811\n",
            "0.006025027949362993\n",
            "0.006023874040693045\n",
            "0.006022720597684383\n",
            "0.006021507084369659\n",
            "0.006020353175699711\n",
            "0.006019183434545994\n",
            "0.0060180299915373325\n",
            "0.006016832310706377\n",
            "0.0060156784020364285\n",
            "0.006014538928866386\n",
            "0.006013369653373957\n",
            "0.006012185476720333\n",
            "0.00601095799356699\n",
            "0.006009848788380623\n",
            "0.006008665077388287\n",
            "0.006007526069879532\n",
            "0.006006341893225908\n",
            "0.006005159113556147\n",
            "0.006004004273563623\n",
            "0.006002850830554962\n",
            "0.006001666653901339\n",
            "0.006000527646392584\n",
            "0.005999373737722635\n",
            "0.005998174659907818\n",
            "0.005997006315737963\n",
            "0.0059958817437291145\n",
            "0.005994683597236872\n",
            "0.005993484985083342\n",
            "0.005992359947413206\n",
            "0.005991206504404545\n",
            "0.005990023259073496\n",
            "0.005988883785903454\n",
            "0.005987758748233318\n",
            "0.005986575502902269\n",
            "0.005985436029732227\n",
            "0.005984282586723566\n",
            "0.005983113311231136\n",
            "0.0059819296002388\n",
            "0.005980805028229952\n",
            "0.005979636218398809\n",
            "0.00597846694290638\n",
            "0.005977313499897718\n",
            "0.0059761591255664825\n",
            "0.005975020118057728\n",
            "0.005973850842565298\n",
            "0.0059726969338953495\n",
            "0.005971542559564114\n",
            "0.00597038958221674\n",
            "0.005969278980046511\n",
            "0.005968140438199043\n",
            "0.005966986063867807\n",
            "0.005965772084891796\n",
            "0.0059646181762218475\n",
            "0.005963464267551899\n",
            "0.005962325260043144\n",
            "0.005961185786873102\n",
            "0.0059600165113806725\n",
            "0.005958863068372011\n",
            "0.005957738496363163\n",
            "0.005956584122031927\n",
            "0.005955400876700878\n",
            "0.005954291205853224\n",
            "0.005953136831521988\n",
            "0.005951982457190752\n",
            "0.005950828082859516\n",
            "0.0059496741741895676\n",
            "0.005948550533503294\n",
            "0.005947411060333252\n",
            "0.005946345627307892\n",
            "0.005945132113993168\n",
            "0.005943977739661932\n",
            "0.005942793563008308\n",
            "0.0059416694566607475\n",
            "0.005940544884651899\n",
            "0.005939391441643238\n",
            "0.005938236601650715\n",
            "0.0059371269308030605\n",
            "0.0059360028244555\n",
            "0.0059348782524466515\n",
            "0.005933693610131741\n",
            "0.005932584870606661\n",
            "0.005931385792791843\n",
            "0.005930305924266577\n",
            "0.005929152015596628\n",
            "0.005928042344748974\n",
            "0.005926873069256544\n",
            "0.005925734061747789\n",
            "0.005924579221755266\n",
            "0.005923440679907799\n",
            "0.005922301206737757\n",
            "0.005921205971390009\n",
            "0.005920066498219967\n",
            "0.0059188976883888245\n",
            "0.005917758215218782\n",
            "0.005916663445532322\n",
            "0.00591552397236228\n",
            "0.005914384964853525\n",
            "0.005913200788199902\n",
            "0.005912150256335735\n",
            "0.005910951644182205\n",
            "0.005909841973334551\n",
            "0.0059087323024868965\n",
            "0.005907622631639242\n",
            "0.0059064538218081\n",
            "0.005905344150960445\n",
            "0.00590421911329031\n",
            "0.005903094541281462\n",
            "0.005901940632611513\n",
            "0.005900816060602665\n",
            "0.005899676121771336\n",
            "0.005898537114262581\n",
            "0.005897441878914833\n",
            "0.005896347109228373\n",
            "0.005895223002880812\n",
            "0.005894023925065994\n",
            "0.005892884451895952\n",
            "0.005891774315387011\n",
            "0.005890649743378162\n",
            "0.005889540072530508\n",
            "0.005888429936021566\n",
            "0.005887276493012905\n",
            "0.00588615145534277\n",
            "0.005885026883333921\n",
            "0.005883917212486267\n",
            "0.005882836878299713\n",
            "0.005881712771952152\n",
            "0.005880558397620916\n",
            "0.005879433825612068\n",
            "0.005878324154764414\n",
            "0.005877184681594372\n",
            "0.005876089446246624\n",
            "0.005874964874237776\n",
            "0.005873854737728834\n",
            "0.005872730165719986\n",
            "0.0058716353960335255\n",
            "0.005870510824024677\n",
            "0.005869356915354729\n",
            "0.0058682463131845\n",
            "0.0058671520091593266\n",
            "0.005866086110472679\n",
            "0.005864917300641537\n",
            "0.005863822530955076\n",
            "0.005862652789801359\n",
            "0.005861543118953705\n",
            "0.005860448349267244\n",
            "0.005859323777258396\n",
            "0.005858198739588261\n",
            "0.005857118871062994\n",
            "0.0058559938333928585\n",
            "0.005854928866028786\n",
            "0.00585380382835865\n",
            "0.005852679722011089\n",
            "0.005851584021002054\n",
            "0.005850429646670818\n",
            "0.005849290173500776\n",
            "0.005848254542797804\n",
            "0.005847129970788956\n",
            "0.005845990963280201\n",
            "0.0058449250645935535\n",
            "0.0058437855914235115\n",
            "0.005842735059559345\n",
            "0.005841581150889397\n",
            "0.0058404854498803616\n",
            "0.005839390680193901\n",
            "0.005838266108185053\n",
            "0.005837155971676111\n",
            "0.005836075637489557\n",
            "0.005834921728819609\n",
            "0.005833826493471861\n",
            "0.0058327317237854\n",
            "0.005831622052937746\n",
            "0.005830511916428804\n",
            "0.005829401779919863\n",
            "0.005828292109072208\n",
            "0.005827196873724461\n",
            "0.005826101638376713\n",
            "0.0058250208385288715\n",
            "0.005823881831020117\n",
            "0.0058227721601724625\n",
            "0.005821750964969397\n",
            "0.005820597056299448\n",
            "0.005819560959935188\n",
            "0.00581843638792634\n",
            "0.005817386321723461\n",
            "0.005816201213747263\n",
            "0.005815135780721903\n",
            "0.005813996307551861\n",
            "0.005812901072204113\n",
            "0.005811820738017559\n",
            "0.005810740869492292\n",
            "0.005809616297483444\n",
            "0.0058085061609745026\n",
            "0.005807455629110336\n",
            "0.005806375294923782\n",
            "0.005805265624076128\n",
            "0.005804155021905899\n",
            "0.00580310495570302\n",
            "0.005801979918032885\n",
            "0.005800899583846331\n",
            "0.005799804348498583\n",
            "0.005798709113150835\n",
            "0.0057976143434643745\n",
            "0.005796519108116627\n",
            "0.005795423872768879\n",
            "0.005794314201921225\n",
            "0.005793233867734671\n",
            "0.00579213909804821\n",
            "0.0057910289615392685\n",
            "0.005789933260530233\n",
            "0.005788883194327354\n",
            "0.00578781682997942\n",
            "0.005786677822470665\n",
            "0.005785612389445305\n",
            "0.0057845464907586575\n",
            "0.005783436819911003\n",
            "0.005782326217740774\n",
            "0.005781261250376701\n",
            "0.00578018045052886\n",
            "0.0057790856808424\n",
            "0.005778035148978233\n",
            "0.005776939447969198\n",
            "0.005775844678282738\n",
            "0.005774764344096184\n",
            "0.005773669108748436\n",
            "0.005772559437900782\n",
            "0.005771508906036615\n",
            "0.005770443007349968\n",
            "0.005769332870841026\n",
            "0.005768267437815666\n",
            "0.005767215974628925\n",
            "0.005766106303781271\n",
            "0.005765011068433523\n",
            "0.005763916298747063\n",
            "0.005762865301221609\n",
            "0.0057617854326963425\n",
            "0.005760689731687307\n",
            "0.005759638734161854\n",
            "0.005758558865636587\n",
            "0.005757507868111134\n",
            "0.005756397731602192\n",
            "0.005755302496254444\n",
            "0.00575422216206789\n",
            "0.0057531860657036304\n",
            "0.00575209129601717\n",
            "0.005751010961830616\n",
            "0.005749930627644062\n",
            "0.0057488796301186085\n",
            "0.005747784394770861\n",
            "0.005746704526245594\n",
            "0.005745638627558947\n",
            "0.005744557827711105\n",
            "0.005743477493524551\n",
            "0.005742426961660385\n",
            "0.005741346161812544\n",
            "0.005740266293287277\n",
            "0.005739185959100723\n",
            "0.00573816429823637\n",
            "0.005737069062888622\n",
            "0.005736018531024456\n",
            "0.005734923295676708\n",
            "0.005733857396990061\n",
            "0.005732791963964701\n",
            "0.005731726530939341\n",
            "0.005730646662414074\n",
            "0.005729595199227333\n",
            "0.005728514865040779\n",
            "0.005727419629693031\n",
            "0.005726397968828678\n",
            "0.005725303199142218\n",
            "0.00572420796379447\n",
            "0.005723202135413885\n",
            "0.00572213577106595\n",
            "0.00572104100137949\n",
            "0.005720019340515137\n",
            "0.005718924105167389\n",
            "0.005717844236642122\n",
            "0.005716792773455381\n",
            "0.005715727806091309\n",
            "0.00571464654058218\n",
            "0.005713581573218107\n",
            "0.005712530110031366\n",
            "0.005711494479328394\n",
            "0.005710413679480553\n",
            "0.00570937804877758\n",
            "0.005708297714591026\n",
            "0.005707261152565479\n",
            "0.0057061659172177315\n",
            "0.005705114919692278\n",
            "0.005704064853489399\n",
            "0.00570305809378624\n",
            "0.005701948422938585\n",
            "0.005700911860913038\n",
            "0.005699846427887678\n",
            "0.005698780529201031\n",
            "0.005697700195014477\n",
            "0.005696649197489023\n",
            "0.005695583764463663\n",
            "0.005694502964615822\n",
            "0.005693497136235237\n",
            "0.005692430771887302\n",
            "0.00569136580452323\n",
            "0.005690343677997589\n",
            "0.005689249373972416\n",
            "0.005688242148607969\n",
            "0.005687162280082703\n",
            "0.005686155520379543\n",
            "0.0056850602850317955\n",
            "0.005683965049684048\n",
            "0.005682958755642176\n",
            "0.005681892856955528\n",
            "0.005680871661752462\n",
            "0.005679775960743427\n",
            "0.005678754765540361\n",
            "0.005677719134837389\n",
            "0.005676653236150742\n",
            "0.005675602704286575\n",
            "0.005674566142261028\n",
            "0.005673485808074474\n",
            "0.005672465078532696\n",
            "0.005671369377523661\n",
            "0.005670362617820501\n",
            "0.005669312085956335\n",
            "0.005668275523930788\n",
            "0.0056672245264053345\n",
            "0.005666188895702362\n",
            "0.005665152799338102\n",
            "0.005664086900651455\n",
            "0.005663006566464901\n",
            "0.005661985371261835\n",
            "0.0056609343737363815\n",
            "0.005659868940711021\n",
            "0.0056588477455079556\n",
            "0.00565782655030489\n",
            "0.005656775087118149\n",
            "0.00565572502091527\n",
            "0.005654703360050917\n",
            "0.005653667263686657\n",
            "0.005652600899338722\n",
            "0.005651609972119331\n",
            "0.0056505585089325905\n",
            "0.005649463273584843\n",
            "0.005648427177220583\n",
            "0.0056473915465176105\n",
            "0.005646369885653257\n",
            "0.005645348224788904\n",
            "0.005644297692924738\n",
            "0.0056432317942380905\n",
            "0.005642239935696125\n",
            "0.005641218274831772\n",
            "0.005640138406306505\n",
            "0.005639116745442152\n",
            "0.00563811045140028\n",
            "0.00563707435503602\n",
            "0.00563600892201066\n",
            "0.005634987261146307\n",
            "0.005633936263620853\n",
            "0.005632900167256594\n",
            "0.005631893407553434\n",
            "0.005630782805383205\n",
            "0.005629762075841427\n",
            "0.005628711078315973\n",
            "0.00562768941745162\n",
            "0.005626668222248554\n",
            "0.005625647027045488\n",
            "0.005624596029520035\n",
            "0.005623604170978069\n",
            "0.00562256807461381\n",
            "0.005621517077088356\n",
            "0.005620495416224003\n",
            "0.00561945978552103\n",
            "0.005618378985673189\n",
            "0.005617372691631317\n",
            "0.005616365931928158\n",
            "0.005615344271063805\n",
            "0.005614293273538351\n",
            "0.005613301414996386\n",
            "0.005612250883132219\n",
            "0.00561121478676796\n",
            "0.005610238295048475\n",
            "0.005609216168522835\n",
            "0.0056081656366586685\n",
            "0.005607158876955509\n",
            "0.0056061530485749245\n",
            "0.00560508668422699\n",
            "0.005604035221040249\n",
            "0.005603088531643152\n",
            "0.005602007731795311\n",
            "0.005600986536592245\n",
            "0.005599950440227985\n",
            "0.005598943680524826\n",
            "0.005597907584160566\n",
            "0.0055968863889575005\n",
            "0.005595850292593241\n",
            "0.0055948286317288876\n",
            "0.005593806970864534\n",
            "0.005592830944806337\n",
            "0.005591779015958309\n",
            "0.005590802524238825\n",
            "0.005589765962213278\n",
            "0.005588744767010212\n",
            "0.005587738007307053\n",
            "0.005586716346442699\n",
            "0.0055856951512396336\n",
            "0.005584614351391792\n",
            "0.005583637859672308\n",
            "0.005582616198807955\n",
            "0.005581623874604702\n",
            "0.005580543540418148\n",
            "0.005579537712037563\n",
            "0.005578590091317892\n",
            "0.005577568430453539\n",
            "0.0055765327997505665\n",
            "0.005575510673224926\n",
            "0.00557446014136076\n",
            "0.005573542322963476\n",
            "0.00557249179109931\n",
            "0.005571470130234957\n",
            "0.0055704484693706036\n",
            "0.005569412373006344\n",
            "0.005568435415625572\n",
            "0.005567414220422506\n",
            "0.00556640699505806\n",
            "0.0055653415620327\n",
            "0.005564364138990641\n",
            "0.005563357844948769\n",
            "0.005562306381762028\n",
            "0.00556134432554245\n",
            "0.005560263525694609\n",
            "0.0055593461729586124\n",
            "0.005558310076594353\n",
            "0.005557273514568806\n",
            "0.005556296557188034\n",
            "0.005555304232984781\n",
            "0.0055542681366205215\n",
            "0.005553276743739843\n",
            "0.0055522555485367775\n",
            "0.005551248788833618\n",
            "0.005550227127969265\n",
            "0.005549250170588493\n",
            "0.00554822850972414\n",
            "0.005547265987843275\n",
            "0.00554626015946269\n",
            "0.005545193329453468\n",
            "0.00554420193657279\n",
            "0.0055432249791920185\n",
            "0.00554223358631134\n",
            "0.0055412114597857\n",
            "0.005540189798921347\n",
            "0.005539154168218374\n",
            "0.005538176745176315\n",
            "0.005537199787795544\n",
            "0.005536207929253578\n",
            "0.005535201169550419\n",
            "0.005534194875508547\n",
            "0.005533232819288969\n",
            "0.005532166920602322\n",
            "0.005531160160899162\n",
            "0.005530197639018297\n",
            "0.005529147107154131\n",
            "0.005528213921934366\n",
            "0.005527222529053688\n",
            "0.005526156630367041\n",
            "0.005525208543986082\n",
            "0.005524232052266598\n",
            "0.005523225292563438\n",
            "0.005522203631699085\n",
            "0.005521227139979601\n",
            "0.005520205479115248\n",
            "0.005519198719412088\n",
            "0.005518191959708929\n",
            "0.005517215467989445\n",
            "0.005516252014786005\n",
            "0.005515172146260738\n",
            "0.005514239426702261\n",
            "0.005513232201337814\n",
            "0.005512195639312267\n",
            "0.005511308088898659\n",
            "0.005510286428034306\n",
            "0.0055092498660087585\n",
            "0.005508317146450281\n",
            "0.005507310852408409\n",
            "0.005506274290382862\n",
            "0.005505282897502184\n",
            "0.005504320375621319\n",
            "0.0055033136159181595\n",
            "0.005502322223037481\n",
            "0.00550134526565671\n",
            "0.005500397179275751\n",
            "0.005499375984072685\n",
            "0.005498338956385851\n",
            "0.005497392266988754\n",
            "0.005496355704963207\n",
            "0.005495378281921148\n",
            "0.005494401324540377\n",
            "0.005493439268320799\n",
            "0.005492432042956352\n",
            "0.005491440650075674\n",
            "0.005490448791533709\n",
            "0.005489486735314131\n",
            "0.005488494411110878\n",
            "0.005487502086907625\n",
            "0.005486510694026947\n",
            "0.0054855188354849815\n",
            "0.005484556779265404\n",
            "0.005483564455062151\n",
            "0.0054825725965201855\n",
            "0.005481640342622995\n",
            "0.005480618681758642\n",
            "0.005479641258716583\n",
            "0.0054786792024970055\n",
            "0.005477672442793846\n",
            "0.005476709455251694\n",
            "0.005475747864693403\n",
            "0.0054747555404901505\n",
            "0.005473793484270573\n",
            "0.005472786724567413\n",
            "0.005471809301525354\n",
            "0.005470891948789358\n",
            "0.005469870287925005\n",
            "0.005468849092721939\n",
            "0.0054678721353411674\n",
            "0.005466879345476627\n",
            "0.005465932190418243\n",
            "0.005464955233037472\n",
            "0.005463977809995413\n",
            "0.005462956614792347\n",
            "0.005462008994072676\n",
            "0.0054610311053693295\n",
            "0.005460054613649845\n",
            "0.005459092557430267\n",
            "0.005458115134388208\n",
            "0.005457152612507343\n",
            "0.005456161219626665\n",
            "0.005455228500068188\n",
            "0.005454221740365028\n",
            "0.005453229416161776\n",
            "0.005452281795442104\n",
            "0.0054512606002390385\n",
            "0.005450327415019274\n",
            "0.005449336022138596\n",
            "0.005448358599096537\n",
            "0.0054473672062158585\n",
            "0.0054464638233184814\n",
            "0.005445456597954035\n",
            "0.005444479174911976\n",
            "0.005443517118692398\n",
            "0.005442598834633827\n",
            "0.005441577639430761\n",
            "0.005440630484372377\n",
            "0.005439667962491512\n",
            "0.00543866166844964\n",
            "0.0054377284832298756\n",
            "0.005436751060187817\n",
            "0.005435773637145758\n",
            "0.005434826482087374\n",
            "0.005433849059045315\n",
            "0.005432872101664543\n",
            "0.005431910045444965\n",
            "0.0054309917613863945\n",
            "0.005429984536021948\n",
            "0.005428992677479982\n",
            "0.005428016185760498\n",
            "0.0054270680993795395\n",
            "0.005426091141998768\n",
            "0.005425128620117903\n",
            "0.0054241809993982315\n",
            "0.005423218943178654\n",
            "0.005422286223620176\n",
            "0.005421338602900505\n",
            "0.005420331377536058\n",
            "0.005419324617832899\n",
            "0.0054184067994356155\n",
            "0.005417459644377232\n",
            "0.005416511557996273\n",
            "0.0054155196994543076\n",
            "0.00541455764323473\n",
            "0.005413595121353865\n",
            "0.005412618163973093\n",
            "0.005411699879914522\n",
            "0.005410707555711269\n",
            "0.005409745499491692\n",
            "0.0054087829776108265\n",
            "0.005407820921391249\n",
            "0.005406903102993965\n",
            "0.0054059261456131935\n",
            "0.005405007861554623\n",
            "0.005404001101851463\n",
            "0.005403038579970598\n",
            "0.005402076058089733\n",
            "0.005401098635047674\n",
            "0.005400165915489197\n",
            "0.005399233195930719\n",
            "0.005398240871727467\n",
            "0.005397323984652758\n",
            "0.005396345630288124\n",
            "0.005395412910729647\n",
            "0.005394435953348875\n",
            "0.005393473897129297\n",
            "0.0053925407119095325\n",
            "0.0053915344178676605\n",
            "0.005390645936131477\n",
            "0.005389654077589512\n",
            "0.0053887213580310345\n",
            "0.005387729033827782\n",
            "0.005386795848608017\n",
            "0.0053858631290495396\n",
            "0.005384886171668768\n",
            "0.005383952986449003\n",
            "0.005383020266890526\n",
            "0.005382073111832142\n",
            "0.005381125491112471\n",
            "0.005380192305892706\n",
            "0.005379244685173035\n",
            "0.005378252826631069\n",
            "0.005377349443733692\n",
            "0.005376387387514114\n",
            "0.005375469569116831\n",
            "0.005374506115913391\n",
            "0.005373558960855007\n",
            "0.005372596904635429\n",
            "0.005371559876948595\n",
            "0.0053707011975348\n",
            "0.0053696949034929276\n",
            "0.005368836224079132\n",
            "0.00536787323653698\n",
            "0.005366940516978502\n",
            "0.005365977995097637\n",
            "0.005365015473216772\n",
            "0.005364112090319395\n",
            "0.005363135598599911\n",
            "0.0053621879778802395\n",
            "0.005361239891499281\n",
            "0.005360306706279516\n",
            "0.005359344650059938\n",
            "0.005358426366001368\n",
            "0.005357448942959309\n",
            "0.0053565166890621185\n",
            "0.00535553926602006\n",
            "0.00535466568544507\n",
            "0.005353703163564205\n",
            "0.005352741107344627\n",
            "0.005351807922124863\n",
            "0.005350875202566385\n",
            "0.00534991268068552\n",
            "0.005348979961127043\n",
            "0.005348047241568565\n",
            "0.005347143393009901\n",
            "0.005346255376935005\n",
            "0.005345263052731752\n",
            "0.005344330798834562\n",
            "0.005343383178114891\n",
            "0.005342420656234026\n",
            "0.005341487005352974\n",
            "0.005340584088116884\n",
            "0.005339621566236019\n",
            "0.005338673945516348\n",
            "0.005337756127119064\n",
            "0.005336823407560587\n",
            "0.005335875786840916\n",
            "0.0053349570371210575\n",
            "0.005334009882062674\n",
            "0.005333091598004103\n",
            "0.005332144442945719\n",
            "0.005331211723387241\n",
            "0.0053302934393286705\n",
            "0.005329345352947712\n",
            "0.005328367929905653\n",
            "0.005327479913830757\n",
            "0.005326532293111086\n",
            "0.005325628910213709\n",
            "0.00532465148717165\n",
            "0.00532374856993556\n",
            "0.005322830285876989\n",
            "0.005321897566318512\n",
            "0.005320964381098747\n",
            "0.005320046097040176\n",
            "0.005319113843142986\n",
            "0.005318209994584322\n",
            "0.005317262373864651\n",
            "0.005316329188644886\n",
            "0.005315396934747696\n",
            "0.005314449314028025\n",
            "0.005313545931130648\n",
            "0.0053125834092497826\n",
            "0.005311694927513599\n",
            "0.005310761742293835\n",
            "0.00530979922041297\n",
            "0.00530889630317688\n",
            "0.005307992920279503\n",
            "0.0053070602007210255\n",
            "0.005306141916662455\n",
            "0.0053051793947815895\n",
            "0.0053043058142066\n",
            "0.005303357727825642\n",
            "0.0053024557419121265\n",
            "0.00530147785320878\n",
            "0.005300559103488922\n",
            "0.005299670621752739\n",
            "0.005298723001033068\n",
            "0.005297790747135878\n",
            "0.005296872463077307\n",
            "0.00529596908017993\n",
            "0.0052950214594602585\n",
            "0.005294147413223982\n",
            "0.005293200258165598\n",
            "0.005292281974107027\n",
            "0.00529134925454855\n",
            "0.005290460307151079\n",
            "0.005289468448609114\n",
            "0.005288564600050449\n",
            "0.00528769101947546\n",
            "0.00528675876557827\n",
            "0.005285855382680893\n",
            "0.005284921731799841\n",
            "0.0052840630523860455\n",
            "0.005283056292682886\n",
            "0.005282182712107897\n",
            "0.005281205289065838\n",
            "0.00528037641197443\n",
            "0.005279413424432278\n",
            "0.005278525408357382\n",
            "0.005277592223137617\n",
            "0.0052767181769013405\n",
            "0.005275756120681763\n",
            "0.0052748676389455795\n",
            "0.005273845512419939\n",
            "0.005273016169667244\n",
            "0.005272068548947573\n",
            "0.005271165631711483\n",
            "0.005270262248814106\n",
            "0.005269388202577829\n",
            "0.005268425215035677\n",
            "0.005267537664622068\n",
            "0.005266648717224598\n",
            "0.0052657159976661205\n",
            "0.005264827515929937\n",
            "0.005263893865048885\n",
            "0.0052629318088293076\n",
            "0.005262072663754225\n",
            "0.005261184647679329\n",
            "0.005260265897959471\n",
            "0.005259318742901087\n",
            "0.0052584148943424225\n",
            "0.005257526412606239\n",
            "0.005256593693047762\n",
            "0.005255705676972866\n",
            "0.005254772026091814\n",
            "0.005253883544355631\n",
            "0.005252980161458254\n",
            "0.005252032540738583\n",
            "0.005251114722341299\n",
            "0.005250255111604929\n",
            "0.005249336827546358\n",
            "0.0052484190091490746\n",
            "0.005247500725090504\n",
            "0.005246627144515514\n",
            "0.005245694890618324\n",
            "0.005244835279881954\n",
            "0.00524388812482357\n",
            "0.0052429991774261\n",
            "0.005242111161351204\n",
            "0.005241177976131439\n",
            "0.005240303929895163\n",
            "0.005239400081336498\n",
            "0.005238467827439308\n",
            "0.005237564444541931\n",
            "0.005236690863966942\n",
            "0.005235742777585983\n",
            "0.005234869197010994\n",
            "0.005233995616436005\n",
            "0.005233032628893852\n",
            "0.005232188850641251\n",
            "0.005231255199760199\n",
            "0.005230367183685303\n",
            "0.005229478236287832\n",
            "0.0052285753190517426\n",
            "0.005227657034993172\n",
            "0.0052267382852733135\n",
            "0.005225894972681999\n",
            "0.0052249617874622345\n",
            "0.005224029067903757\n",
            "0.005223199725151062\n",
            "0.005222266539931297\n",
            "0.0052213482558727264\n",
            "0.005220444872975349\n",
            "0.005219571758061647\n",
            "0.0052186972461640835\n",
            "0.005217794328927994\n",
            "0.005216876044869423\n",
            "0.00521598756313324\n",
            "0.005215099081397057\n",
            "0.005214165896177292\n",
            "0.005213291849941015\n",
            "0.005212374031543732\n",
            "0.005211544688791037\n",
            "0.005210626404732466\n",
            "0.005209722556173801\n",
            "0.005208819173276424\n",
            "0.005207945592701435\n",
            "0.0052070277743041515\n",
            "0.005206154193729162\n",
            "0.005205206107348204\n",
            "0.005204346496611834\n",
            "0.005203487817198038\n",
            "0.005202584899961948\n",
            "0.005201711319386959\n",
            "0.005200807470828295\n",
            "0.005199904087930918\n",
            "0.005199001170694828\n",
            "0.005198142025619745\n",
            "0.005197253543883562\n",
            "0.00519631989300251\n",
            "0.005195416510105133\n",
            "0.005194557830691338\n",
            "0.005193684250116348\n",
            "0.005192751530557871\n",
            "0.005191877018660307\n",
            "0.005190989002585411\n",
            "0.005190100055187941\n",
            "0.005189241375774145\n",
            "0.0051883673295378685\n",
            "0.005187449045479298\n",
            "0.005186575464904308\n",
            "0.005185672082006931\n",
            "0.005184798501431942\n",
            "0.00518396869301796\n",
            "0.005183021072298288\n",
            "0.005182161927223206\n",
            "0.005181229207664728\n",
            "0.005180354695767164\n",
            "0.005179480649530888\n",
            "0.005178578197956085\n",
            "0.005177733954042196\n",
            "0.005176875274628401\n",
            "0.0051759411580860615\n",
            "0.005175082478672266\n",
            "0.005174163728952408\n",
            "0.0051733055151999\n",
            "0.00517243193462491\n",
            "0.00517154298722744\n",
            "0.005170698743313551\n",
            "0.005169810261577368\n",
            "0.005168892443180084\n",
            "0.005168062634766102\n",
            "0.005167159251868725\n",
            "0.0051662856712937355\n",
            "0.005165397189557552\n",
            "0.005164508242160082\n",
            "0.005163619294762611\n",
            "0.005162760615348816\n",
            "0.005161857232451439\n",
            "0.00516098365187645\n",
            "0.005160079803317785\n",
            "0.005159191321581602\n",
            "0.005158347077667713\n",
            "0.005157473497092724\n",
            "0.0051565999165177345\n",
            "0.005155755672603846\n",
            "0.005154822487384081\n",
            "0.005153962876647711\n",
            "0.00515307392925024\n",
            "0.005152259953320026\n",
            "0.005151386372745037\n",
            "0.005150468088686466\n",
            "0.005149579141288996\n",
            "0.005148705095052719\n",
            "0.0051478613168001175\n",
            "0.005147002171725035\n",
            "0.005146114155650139\n",
            "0.005145195405930281\n",
            "0.005144366063177586\n",
            "0.00514350738376379\n",
            "0.005142589099705219\n",
            "0.005141729488968849\n",
            "0.0051407963037490845\n",
            "0.005139967426657677\n",
            "0.0051390933804214\n",
            "0.005138234701007605\n",
            "0.0051373750902712345\n",
            "0.005136486608535051\n",
            "0.005135642364621162\n",
            "0.005134753882884979\n",
            "0.0051338947378098965\n",
            "0.005133006256073713\n",
            "0.005132132209837437\n",
            "0.005131287965923548\n",
            "0.005130399018526077\n",
            "0.0051295398734509945\n",
            "0.005128637421876192\n",
            "0.00512780761346221\n",
            "0.005126933567225933\n",
            "0.00512607442215085\n",
            "0.005125171039253473\n",
            "0.005124326795339584\n",
            "0.005123467650264502\n",
            "0.005122608505189419\n",
            "0.005121735390275717\n",
            "0.005120905581861734\n",
            "0.00512000173330307\n",
            "0.005119128152728081\n",
            "0.005118254106491804\n",
            "0.00511744013056159\n",
            "0.005116566084325314\n",
            "0.00511569157242775\n",
            "0.005114818457514048\n",
            "0.005113929510116577\n",
            "0.005113099701702595\n",
            "0.005112270824611187\n",
            "0.005111396312713623\n",
            "0.005110523197799921\n",
            "0.005109649151563644\n",
            "0.005108775570988655\n",
            "0.005107887089252472\n",
            "0.0051070572808384895\n",
            "0.0051062279380857944\n",
            "0.005105309188365936\n",
            "0.005104464944452047\n",
            "0.005103591363877058\n",
            "0.005102761555463076\n",
            "0.00510190287604928\n",
            "0.0051009696908295155\n",
            "0.0051001994870603085\n",
            "0.005099310539662838\n",
            "0.005098481196910143\n",
            "0.005097592249512672\n",
            "0.005096807144582272\n",
            "0.005095888860523701\n",
            "0.005095000844448805\n",
            "0.00509417150169611\n",
            "0.00509334122762084\n",
            "0.005092467647045851\n",
            "0.005091608967632055\n",
            "0.005090735387057066\n",
            "0.005089905578643084\n",
            "0.005088987294584513\n",
            "0.00508815748617053\n",
            "0.005087313707917929\n",
            "0.005086439661681652\n",
            "0.005085594952106476\n",
            "0.005084780510514975\n",
            "0.00508387666195631\n",
            "0.005083032418042421\n",
            "0.005082202609628439\n",
            "0.005081388633698225\n",
            "0.00508045544847846\n",
            "0.005079596769064665\n",
            "0.005078707356005907\n",
            "0.0050779227167367935\n",
            "0.00507704820483923\n",
            "0.005076233763247728\n",
            "0.005075359717011452\n",
            "0.005074501037597656\n",
            "0.005073671694844961\n",
            "0.005072842352092266\n",
            "0.0050719683058559895\n",
            "0.0050711240619421005\n",
            "0.005070250015705824\n",
            "0.005069464910775423\n",
            "0.005068546626716852\n",
            "0.0050677466206252575\n",
            "0.005066872574388981\n",
            "0.005066013429313898\n",
            "0.005065184086561203\n",
            "0.005064340308308601\n",
            "0.005063480697572231\n",
            "0.00506266625598073\n",
            "0.005061762873083353\n",
            "0.0050609479658305645\n",
            "0.005060059018433094\n",
            "0.0050592743791639805\n",
            "0.005058400332927704\n",
            "0.0050575705245137215\n",
            "0.005056770984083414\n",
            "0.005055852700024843\n",
            "0.005055053159594536\n",
            "0.005054178647696972\n",
            "0.005053319502621889\n",
            "0.005052460823208094\n",
            "0.005051660817116499\n",
            "0.005050801672041416\n",
            "0.005049972329288721\n",
            "0.005049113184213638\n",
            "0.0050482978112995625\n",
            "0.005047409329563379\n",
            "0.005046624690294266\n",
            "0.005045750644057989\n",
            "0.0050449064001441\n",
            "0.005044061690568924\n",
            "0.0050432030111551285\n",
            "0.0050423587672412395\n",
            "0.005041528958827257\n",
            "0.005040699616074562\n",
            "0.00503986980766058\n",
            "0.005039039999246597\n",
            "0.005038196220993996\n",
            "0.005037322174757719\n",
            "0.005036508198827505\n",
            "0.005035633686929941\n",
            "0.005034804344177246\n",
            "0.005033974535763264\n",
            "0.005033115390688181\n",
            "0.005032286047935486\n",
            "0.00503144133836031\n",
            "0.005030626896768808\n",
            "0.005029782187193632\n",
            "0.005028967745602131\n",
            "0.005028109066188335\n",
            "0.005027294624596834\n",
            "0.005026434548199177\n",
            "0.005025575868785381\n",
            "0.005024746060371399\n",
            "0.00502390181645751\n",
            "0.005023072473704815\n",
            "0.0050222426652908325\n",
            "0.0050214133225381374\n",
            "0.005020598415285349\n",
            "0.005019709933549166\n",
            "0.005018880125135183\n",
            "0.0050180950202047825\n",
            "0.005017251241952181\n",
            "0.00501643680036068\n",
            "0.005015636328607798\n",
            "0.005014762748032808\n",
            "0.005013962741941214\n",
            "0.005013088695704937\n",
            "0.0050122737884521484\n",
            "0.005011414643377066\n",
            "0.005010555498301983\n",
            "0.005009755492210388\n",
            "0.005008925683796406\n",
            "0.005008066538721323\n",
            "0.005007281433790922\n",
            "0.0050064376555383205\n",
            "0.005005592480301857\n",
            "0.005004778504371643\n",
            "0.005003978963941336\n",
            "0.005003149155527353\n",
            "0.005002319812774658\n",
            "0.00500150490552187\n",
            "0.005000675097107887\n",
            "0.004999830853193998\n",
            "0.004999001044780016\n",
            "0.00499815633520484\n",
            "0.004997342359274626\n",
            "0.0049964976496994495\n",
            "0.004995653405785561\n",
            "0.004994838498532772\n",
            "0.004994009155780077\n",
            "0.0049931942485272884\n",
            "0.004992394708096981\n",
            "0.004991564899682999\n",
            "0.004990735091269016\n",
            "0.004989905748516321\n",
            "0.004989076405763626\n",
            "0.004988261964172125\n",
            "0.0049874321557581425\n",
            "0.004986632149666548\n",
            "0.004985773004591465\n",
            "0.0049849580973386765\n",
            "0.0049841138534247875\n",
            "0.004983328282833099\n",
            "0.004982529208064079\n",
            "0.004981669131666422\n",
            "0.004980854690074921\n",
            "0.004980054683983326\n",
            "0.004979180172085762\n",
            "0.004978380631655455\n",
            "0.004977521952241659\n",
            "0.004976736381649971\n",
            "0.004975966177880764\n",
            "0.004975107032805681\n",
            "0.004974262788891792\n",
            "0.0049734776839613914\n",
            "0.004972648341208696\n",
            "0.004971818532794714\n",
            "0.004971018526703119\n",
            "0.004970189183950424\n",
            "0.004969388712197542\n",
            "0.004968544468283653\n",
            "0.004967715125530958\n",
            "0.004966885782778263\n",
            "0.004966100212186575\n",
            "0.004965255968272686\n",
            "0.004964455962181091\n",
            "0.004963626153767109\n",
            "0.004962796811014414\n",
            "0.004961981903761625\n",
            "0.004961152095347643\n",
            "0.004960352089256048\n",
            "0.004959567449986935\n",
            "0.004958723206073046\n",
            "0.004957922734320164\n",
            "0.004957093857228756\n",
            "0.004956308752298355\n",
            "0.004955478478223085\n",
            "0.0049546342343091965\n",
            "0.004953819792717695\n",
            "0.004953019320964813\n",
            "0.004952219780534506\n",
            "0.004951404873281717\n",
            "0.004950575064867735\n",
            "0.004949804861098528\n",
            "0.004948989953845739\n",
            "0.004948160611093044\n",
            "0.004947315901517868\n",
            "0.004946530796587467\n",
            "0.004945701453834772\n",
            "0.004944915883243084\n",
            "0.004944101441651583\n",
            "0.004943301435559988\n",
            "0.004942500963807106\n",
            "0.0049417163245379925\n",
            "0.004940901417285204\n",
            "0.004940056707710028\n",
            "0.004939286969602108\n",
            "0.004938486497849226\n",
            "0.004937686957418919\n",
            "0.004936842247843742\n",
            "0.004936027806252241\n",
            "0.004935242235660553\n",
            "0.004934442229568958\n",
            "0.004933612886816263\n",
            "0.004932798445224762\n",
            "0.004931982606649399\n",
            "0.004931153729557991\n",
            "0.004930353257805109\n",
            "0.004929538816213608\n",
            "0.004928753711283207\n",
            "0.004927924834191799\n",
            "0.0049271536991000175\n",
            "0.004926339257508516\n",
            "0.004925524350255728\n",
            "0.004924695007503033\n",
            "0.0049239094369113445\n",
            "0.004923094529658556\n",
            "0.004922309424728155\n",
            "0.004921464715152979\n",
            "0.004920679610222578\n",
            "0.004919879604130983\n",
            "0.004919109400361776\n",
            "0.004918339196592569\n",
            "0.004917509388178587\n",
            "0.004916694480925798\n",
            "0.004915924277156591\n",
            "0.004915094468742609\n",
            "0.004914264660328627\n",
            "0.004913450218737125\n",
            "0.004912664648145437\n",
            "0.004911880008876324\n",
            "0.004911079537123442\n",
            "0.004910309333354235\n",
            "0.0049094948917627335\n",
            "0.00490864971652627\n",
            "0.004907879512757063\n",
            "0.0049070799723267555\n",
            "0.004906250163912773\n",
            "0.004905479494482279\n",
            "0.004904665052890778\n",
            "0.004903879947960377\n",
            "0.004903094377368689\n",
            "0.004902324173599482\n",
            "0.004901524167507887\n",
            "0.004900724161416292\n",
            "0.004899909254163504\n",
            "0.004899079445749521\n",
            "0.004898324143141508\n",
            "0.004897524137049913\n",
            "0.004896679427474737\n",
            "0.004895924124866724\n",
            "0.004895124118775129\n",
            "0.0048943087458610535\n",
            "0.00489352410659194\n",
            "0.004892724100500345\n",
            "0.004891909193247557\n",
            "0.00489116832613945\n",
            "0.004890324082225561\n",
            "0.004889583680778742\n",
            "0.004888738505542278\n",
            "0.004887998104095459\n",
            "0.004887198098003864\n",
            "0.004886368289589882\n",
            "0.004885627422481775\n",
            "0.004884827882051468\n",
            "0.004884012509137392\n",
            "0.004883212503045797\n",
            "0.0048824273981153965\n",
            "0.0048816269263625145\n",
            "0.004880841821432114\n",
            "0.004880056716501713\n",
            "0.004879256710410118\n",
            "0.004878471605479717\n",
            "0.004877686034888029\n",
            "0.0048769013956189156\n",
            "0.004876071587204933\n",
            "0.004875300917774439\n",
            "0.004874500911682844\n",
            "0.004873730707913637\n",
            "0.004872915800660849\n",
            "0.004872175399214029\n",
            "0.004871345590800047\n",
            "0.004870574921369553\n",
            "0.004869774915277958\n",
            "0.0048690191470086575\n",
            "0.004868204239755869\n",
            "0.004867419600486755\n",
            "0.004866648931056261\n",
            "0.004865819588303566\n",
            "0.004865034017711878\n",
            "0.00486424844712019\n",
            "0.004863478243350983\n",
            "0.004862663336098194\n",
            "0.004861863330006599\n",
            "0.004861093126237392\n",
            "0.004860352259129286\n",
            "0.004859537817537785\n",
            "0.004858752246946096\n",
            "0.004857996478676796\n",
            "0.004857226274907589\n",
            "0.004856441169977188\n",
            "0.004855640698224306\n",
            "0.004854841157793999\n",
            "0.004854040686041117\n",
            "0.004853284917771816\n",
            "0.004852499812841415\n",
            "0.004851743578910828\n",
            "0.004850929137319326\n",
            "0.004850128665566444\n",
            "0.004849388264119625\n",
            "0.004848543554544449\n",
            "0.0048478031530976295\n",
            "0.004847032483667135\n",
            "0.004846276715397835\n",
            "0.004845432471483946\n",
            "0.004844676703214645\n",
            "0.004843906499445438\n",
            "0.00484312092885375\n",
            "0.004842380527406931\n",
            "0.004841594956815243\n",
            "0.004840809851884842\n",
            "0.004839994944632053\n",
            "0.004839224740862846\n",
            "0.004838454071432352\n",
            "0.004837683867663145\n",
            "0.004836898297071457\n",
            "0.004836098290979862\n",
            "0.004835342988371849\n",
            "0.004834557417780161\n",
            "0.004833787214010954\n",
            "0.004833017010241747\n",
            "0.004832216538488865\n",
            "0.004831461235880852\n",
            "0.004830675665289164\n",
            "0.004829875193536282\n",
            "0.004829134792089462\n",
            "0.004828334785997868\n",
            "0.004827564116567373\n",
            "0.004826779011636972\n",
            "0.004826008342206478\n",
            "0.004825253039598465\n",
            "0.004824467469006777\n",
            "0.0048236967995762825\n",
            "0.004822941496968269\n",
            "0.004822141490876675\n",
            "0.00482137082144618\n",
            "0.004820659756660461\n",
            "0.004819829948246479\n",
            "0.004819059744477272\n",
            "0.004818304441869259\n",
            "0.004817518871277571\n",
            "0.00481676310300827\n",
            "0.004815918393433094\n",
            "0.0048151626251637936\n",
            "0.004814377520233393\n",
            "0.004813607316464186\n",
            "0.004812881350517273\n",
            "0.004812081344425678\n",
            "0.004811354912817478\n",
            "0.00481054000556469\n",
            "0.00480979960411787\n",
            "0.004809013567864895\n",
            "0.004808258730918169\n",
            "0.004807488061487675\n",
            "0.004806716926395893\n",
            "0.004806005861610174\n",
            "0.004805190954357386\n",
            "0.004804450087249279\n",
            "0.004803680349141359\n",
            "0.004802924580872059\n",
            "0.004802138544619083\n",
            "0.004801338538527489\n",
            "0.004800582770258188\n",
            "0.0047998130321502686\n",
            "0.004799056798219681\n",
            "0.004798286594450474\n",
            "0.004797560628503561\n",
            "0.004796731285750866\n",
            "0.004795975051820278\n",
            "0.004795204848051071\n",
            "0.004794449079781771\n",
            "0.004793678410351276\n",
            "0.004792922176420689\n",
            "0.004792182240635157\n",
            "0.0047913966700434685\n",
            "0.00479061109945178\n",
            "0.004789840895682573\n",
            "0.004789144266396761\n",
            "0.004788374062627554\n",
            "0.004787573590874672\n",
            "0.0047868480905890465\n",
            "0.004786033183336258\n",
            "0.004785248078405857\n",
            "0.004784506745636463\n",
            "0.00478378077968955\n",
            "0.004782965872436762\n",
            "0.004782254807651043\n",
            "0.004781483672559261\n",
            "0.0047806985676288605\n",
            "0.0047798980958759785\n",
            "0.004779201932251453\n",
            "0.004778431728482246\n",
            "0.004777661059051752\n",
            "0.004776890855282545\n",
            "0.004776179790496826\n",
            "0.004775409121066332\n",
            "0.004774609114974737\n",
            "0.0047738682478666306\n",
            "0.00477311247959733\n",
            "0.004772386513650417\n",
            "0.004771615378558636\n",
            "0.004770815838128328\n",
            "0.004770074971020222\n",
            "0.004769348539412022\n",
            "0.0047685629688203335\n",
            "0.004767807200551033\n",
            "0.004767066799104214\n",
            "0.004766325931996107\n",
            "0.004765540361404419\n",
            "0.0047647999599576\n",
            "0.004764044191688299\n",
            "0.0047633033245801926\n",
            "0.004762532655149698\n",
            "0.00476171774789691\n",
            "0.004760947078466415\n",
            "0.004760191775858402\n",
            "0.004759450908750296\n",
            "0.004758710041642189\n",
            "0.004757968708872795\n",
            "0.004757242277264595\n",
            "0.004756487440317869\n",
            "0.004755672533065081\n",
            "0.004754946101456881\n",
            "0.004754220135509968\n",
            "0.00475343456491828\n",
            "0.004752649459987879\n",
            "0.00475193839520216\n",
            "0.00475121196359396\n",
            "0.004750397056341171\n",
            "0.004749700427055359\n",
            "0.004748945124447346\n",
            "0.004748203791677952\n",
            "0.004747403785586357\n",
            "0.0047466629184782505\n",
            "0.004745936952531338\n",
            "0.004745196085423231\n",
            "0.004744425415992737\n",
            "0.00474368454888463\n",
            "0.004742959048599005\n",
            "0.0047422475181519985\n",
            "0.0047414470463991165\n",
            "0.004740691743791103\n",
            "0.004739964846521616\n",
            "0.004739179741591215\n",
            "0.0047384388744831085\n",
            "0.004737698007375002\n",
            "0.0047369422391057014\n",
            "0.004736186936497688\n",
            "0.004735475406050682\n",
            "0.004734690301120281\n",
            "0.004733979236334562\n",
            "0.004733193665742874\n",
            "0.004732482135295868\n",
            "0.004731697030365467\n",
            "0.004730985499918461\n",
            "0.0047302148304879665\n",
            "0.004729459062218666\n",
            "0.004728673957288265\n",
            "0.004727992694824934\n",
            "0.00472725136205554\n",
            "0.004726480692625046\n",
            "0.004725784529000521\n",
            "0.00472499942407012\n",
            "0.004724258556962013\n",
            "0.004723532125353813\n",
            "0.0047227609902620316\n",
            "0.004722020123153925\n",
            "0.004721279721707106\n",
            "0.0047205532900989056\n",
            "0.0047197830863296986\n",
            "0.004719027318060398\n",
            "0.004718301352113485\n",
            "0.004717531148344278\n",
            "0.004716834053397179\n",
            "0.004716048948466778\n",
            "0.004715322516858578\n",
            "0.004714581649750471\n",
            "0.0047138407826423645\n",
            "0.004713085480034351\n",
            "0.004712388850748539\n",
            "0.0047116330824792385\n",
            "0.004710877314209938\n",
            "0.004710150882601738\n",
            "0.004709410481154919\n",
            "0.0047086994163692\n",
            "0.0047079576179385185\n",
            "0.0047072311863303185\n",
            "0.004706490784883499\n",
            "0.004705705214291811\n",
            "0.004704979248344898\n",
            "0.0047042532823979855\n",
            "0.004703468177467585\n",
            "0.004702727310359478\n",
            "0.00470194173976779\n",
            "0.0047012753784656525\n",
            "0.0047005037777125835\n",
            "0.004699763376265764\n",
            "0.004699052311480045\n",
            "0.004698340781033039\n",
            "0.004697585012763739\n",
            "0.004696829244494438\n",
            "0.004696118179708719\n",
            "0.004695362411439419\n",
            "0.004694636445492506\n",
            "0.0046939244493842125\n",
            "0.004693183582276106\n",
            "0.004692442715167999\n",
            "0.0046917167492210865\n",
            "0.004690975416451693\n",
            "0.00469024945050478\n",
            "0.004689538851380348\n",
            "0.004688797984272242\n",
            "0.004688042216002941\n",
            "0.00468730041757226\n",
            "0.004686559550464153\n",
            "0.004685878288000822\n",
            "0.004685107618570328\n",
            "0.004684396553784609\n",
            "0.004683640785515308\n",
            "0.004682914353907108\n",
            "0.0046821883879601955\n",
            "0.004681447520852089\n",
            "0.004680735990405083\n",
            "0.004679965786635876\n",
            "0.0046792542561888695\n",
            "0.004678542725741863\n",
            "0.004677772056311369\n",
            "0.004677031189203262\n",
            "0.004676320124417543\n",
            "0.004675637930631638\n",
            "0.004674911964684725\n",
            "0.004674141760915518\n",
            "0.004673415329307318\n",
            "0.004672674462199211\n",
            "0.004671977832913399\n",
            "0.00467126676812768\n",
            "0.004670510999858379\n",
            "0.004669784568250179\n",
            "0.004669058136641979\n",
            "0.00466831773519516\n",
            "0.00466759130358696\n",
            "0.004666895139962435\n",
            "0.004666094668209553\n",
            "0.00466539803892374\n",
            "0.004664701409637928\n",
            "0.004663961008191109\n",
            "0.004663234576582909\n",
            "0.004662478808313608\n",
            "0.004661752842366695\n",
            "0.004661026410758495\n",
            "0.004660299979150295\n",
            "0.004659559577703476\n",
            "0.004658862948417664\n",
            "0.004658151417970657\n",
            "0.004657395649701357\n",
            "0.0046566990204155445\n",
            "0.004655973054468632\n",
            "0.004655187483876944\n",
            "0.004654536023736\n",
            "0.0046537951566278934\n",
            "0.004653068725019693\n",
            "0.004652312956750393\n",
            "0.004651615861803293\n",
            "0.0046508898958563805\n",
            "0.004650149494409561\n",
            "0.004649467300623655\n",
            "0.004648711532354355\n",
            "0.00464801536872983\n",
            "0.00464728893712163\n",
            "0.00464656250551343\n",
            "0.004645821172744036\n",
            "0.004645125009119511\n",
            "0.0046443697065114975\n",
            "0.004643658176064491\n",
            "0.004642947111278772\n",
            "0.00464222114533186\n",
            "0.004641479346901178\n",
            "0.00464079761877656\n",
            "0.004640056751668453\n",
            "0.004639315884560347\n",
            "0.00463860435411334\n",
            "0.004637878853827715\n",
            "0.004637167323380709\n",
            "0.004636470228433609\n",
            "0.004635803867131472\n",
            "0.0046350182965397835\n",
            "0.004634321667253971\n",
            "0.004633550997823477\n",
            "0.004632854368537664\n",
            "0.004632158204913139\n",
            "0.004631387535482645\n",
            "0.004630690440535545\n",
            "0.004629979375749826\n",
            "0.004629297647625208\n",
            "0.004628601484000683\n",
            "0.004627875052392483\n",
            "0.004627134185284376\n",
            "0.004626392852514982\n",
            "0.004625711124390364\n",
            "0.004625014495104551\n",
            "0.004624274093657732\n",
            "0.004623547662049532\n",
            "0.0046227918937802315\n",
            "0.004622139967978001\n",
            "0.004621428437530994\n",
            "0.004620716907083988\n",
            "0.004619990475475788\n",
            "0.0046192496083676815\n",
            "0.00461856834590435\n",
            "0.0046178423799574375\n",
            "0.0046171750873327255\n",
            "0.004616463556885719\n",
            "0.004615707788616419\n",
            "0.0046149967238307\n",
            "0.0046143000945448875\n",
            "0.0046135736629366875\n",
            "0.004612921737134457\n",
            "0.00461218087002635\n",
            "0.004611454904079437\n",
            "0.004610713105648756\n",
            "0.004610046278685331\n",
            "0.004609350580722094\n",
            "0.004608608782291412\n",
            "0.004607927519828081\n",
            "0.0046071857213974\n",
            "0.004606459755450487\n",
            "0.0046057188883423805\n",
            "0.0046050227247178555\n",
            "0.004604355897754431\n",
            "0.004603614564985037\n",
            "0.004602962639182806\n",
            "0.004602191969752312\n",
            "0.004601494874805212\n",
            "0.004600768908858299\n",
            "0.004600028041750193\n",
            "0.004599361214786768\n",
            "0.004598650150001049\n",
            "0.004597893916070461\n",
            "0.004597182385623455\n",
            "0.00459651555866003\n",
            "0.004595863167196512\n",
            "0.004595092963427305\n",
            "0.004594455938786268\n",
            "0.004593714140355587\n",
            "0.004592988174408674\n",
            "0.0045923213474452496\n",
            "0.004591565579175949\n",
            "0.004590883851051331\n",
            "0.004590142518281937\n",
            "0.004589475691318512\n",
            "0.004588823299854994\n",
            "0.004588067531585693\n",
            "0.004587371367961168\n",
            "0.004586630500853062\n",
            "0.004585963673889637\n",
            "0.004585207439959049\n",
            "0.0045845406129956245\n",
            "0.004583829082548618\n",
            "0.004583117552101612\n",
            "0.004582421388477087\n",
            "0.004581724759191275\n",
            "0.004581057466566563\n",
            "0.004580316599458456\n",
            "0.004579649306833744\n",
            "0.004578908905386925\n",
            "0.004578211810439825\n",
            "0.0045775156468153\n",
            "0.004576819017529488\n",
            "0.0045760925859212875\n",
            "0.004575395956635475\n",
            "0.004574684426188469\n",
            "0.0045740180648863316\n",
            "0.004573335871100426\n",
            "0.0045726243406534195\n",
            "0.004571927711367607\n",
            "0.004571216646581888\n",
            "0.004570520017296076\n",
            "0.004569838289171457\n",
            "0.004569097422063351\n",
            "0.004568400792777538\n",
            "0.004567733500152826\n",
            "0.004567051772028208\n",
            "0.0045662811025977135\n",
            "0.0045656440779566765\n",
            "0.00456493254750967\n",
            "0.004564235918223858\n",
            "0.004563509486615658\n",
            "0.004562842659652233\n",
            "0.00456211669370532\n",
            "0.004561449866741896\n",
            "0.004560752771794796\n",
            "0.00456004124134779\n",
            "0.004559359513223171\n",
            "0.0045586335472762585\n",
            "0.004557936917990446\n",
            "0.004557240288704634\n",
            "0.004556543659418821\n",
            "0.004555861931294203\n",
            "0.004555210005491972\n",
            "0.0045544542372226715\n",
            "0.004553801845759153\n",
            "0.004553105682134628\n",
            "0.004552363883703947\n",
            "0.004551711957901716\n",
            "0.004551030229777098\n",
            "0.00455027399584651\n",
            "0.004549622535705566\n",
            "0.004548939876258373\n",
            "0.004548214375972748\n",
            "0.004547547083348036\n",
            "0.004546836018562317\n",
            "0.004546109586954117\n",
            "0.004545442759990692\n",
            "0.0045447456650435925\n",
            "0.004544064402580261\n",
            "0.004543367773294449\n",
            "0.0045426711440086365\n",
            "0.004541974514722824\n",
            "0.004541262984275818\n",
            "0.004540625959634781\n",
            "0.004539899528026581\n",
            "0.004539232701063156\n",
            "0.004538506269454956\n",
            "0.004537853877991438\n",
            "0.004537127446383238\n",
            "0.004536490421742201\n",
            "0.0045357937924563885\n",
            "0.004535097163170576\n",
            "0.0045344154350459576\n",
            "0.004533703438937664\n",
            "0.004533022176474333\n",
            "0.004532355349510908\n",
            "0.00453159911558032\n",
            "0.004530946724116802\n",
            "0.004530250560492277\n",
            "0.004529509227722883\n",
            "0.004528902005404234\n",
            "0.0045282053761184216\n",
            "0.004527493380010128\n",
            "0.004526841454207897\n",
            "0.004526100587099791\n",
            "0.004525433294475079\n",
            "0.004524692427366972\n",
            "0.004524084739387035\n",
            "0.004523373208940029\n",
            "0.004522676579654217\n",
            "0.004522024653851986\n",
            "0.004521357826888561\n",
            "0.004520646296441555\n",
            "0.004519979003816843\n",
            "0.004519252572208643\n",
            "0.004518570844084024\n",
            "0.004517904482781887\n",
            "0.0045172227546572685\n",
            "0.0045165554620325565\n",
            "0.004515888635069132\n",
            "0.004515162203460932\n",
            "0.004514465574175119\n",
            "0.004513828083872795\n",
            "0.004513131454586983\n",
            "0.004512419458478689\n",
            "0.004511753097176552\n",
            "0.004511100705713034\n",
            "0.004510404542088509\n",
            "0.004509706981480122\n",
            "0.00450902571901679\n",
            "0.0045083146542310715\n",
            "0.004507602658122778\n",
            "0.004506994970142841\n",
            "0.004506283439695835\n",
            "0.004505676217377186\n",
            "0.004504949785768986\n",
            "0.004504253156483173\n",
            "0.004503600765019655\n",
            "0.004502904135733843\n",
            "0.004502192605286837\n",
            "0.004501510877162218\n",
            "0.004500844050198793\n",
            "0.0045002358965575695\n",
            "0.004499509930610657\n",
            "0.0044987984001636505\n",
            "0.004498116672039032\n",
            "0.0044974349439144135\n",
            "0.004496738314628601\n",
            "0.004496100824326277\n",
            "0.004495418630540371\n",
            "0.004494751803576946\n",
            "0.004494070075452328\n",
            "0.004493343643844128\n",
            "0.004492706619203091\n",
            "0.004491950385272503\n",
            "0.004491313360631466\n",
            "0.00449060183018446\n",
            "0.004489993676543236\n",
            "0.0044893124140799046\n",
            "0.004488630685955286\n",
            "0.004487934056669474\n",
            "0.0044872816652059555\n",
            "0.004486569669097662\n",
            "0.004485932644456625\n",
            "0.004485236015170813\n",
            "0.004484613426029682\n",
            "0.00448391679674387\n",
            "0.004483220167458057\n",
            "0.004482553340494633\n",
            "0.004481871146708727\n",
            "0.004481203854084015\n",
            "0.00448053702712059\n",
            "0.0044798702001571655\n",
            "0.004479218274354935\n",
            "0.004478550981730223\n",
            "0.0044778394512832165\n",
            "0.00447712792083621\n",
            "0.004476475995033979\n",
            "0.004475853405892849\n",
            "0.0044751716777682304\n",
            "0.004474475048482418\n",
            "0.004473808221518993\n",
            "0.0044731260277330875\n",
            "0.004472429398447275\n",
            "0.004471762105822563\n",
            "0.004471109714359045\n",
            "0.004470488056540489\n",
            "0.004469776526093483\n",
            "0.004469078965485096\n",
            "0.004468412604182959\n",
            "0.004467790015041828\n",
            "0.004467078018933535\n",
            "0.004466426558792591\n",
            "0.004465699661523104\n",
            "0.004465091973543167\n",
            "0.004464425146579742\n",
            "0.00446372851729393\n",
            "0.004463061224669218\n",
            "0.004462409298866987\n",
            "0.004461756907403469\n",
            "0.004461060278117657\n",
            "0.004460363648831844\n",
            "0.00445972615852952\n",
            "0.004459073767066002\n",
            "0.004458392038941383\n",
            "0.004457710310816765\n",
            "0.004457058385014534\n",
            "0.004456405993551016\n",
            "0.0044557093642652035\n",
            "0.004455071873962879\n",
            "0.004454359877854586\n",
            "0.0044537377543747425\n",
            "0.00445304112508893\n",
            "0.0044523742981255054\n",
            "0.0044516921043396\n",
            "0.004451069515198469\n",
            "0.004450357984751463\n",
            "0.004449750296771526\n",
            "0.004449083469808102\n",
            "0.004448416642844677\n",
            "0.004447734449058771\n",
            "0.004447022452950478\n",
            "0.004446430131793022\n",
            "0.004445747938007116\n",
            "0.004445050843060017\n",
            "0.004444443620741367\n",
            "0.004443717189133167\n",
            "0.004443109501153231\n",
            "0.004442427773028612\n",
            "0.004441745579242706\n",
            "0.004441063851118088\n",
            "0.00444041145965457\n",
            "0.004439774435013533\n",
            "0.004439107142388821\n",
            "0.004438439849764109\n",
            "0.004437787923961878\n",
            "0.004437105730175972\n",
            "0.0044364542700350285\n",
            "0.004435801412910223\n",
            "0.004435104783624411\n",
            "0.00443448219448328\n",
            "0.0044338153675198555\n",
            "0.004433118738234043\n",
            "0.004432466346770525\n",
            "0.004431799054145813\n",
            "0.004431147128343582\n",
            "0.004430524539202452\n",
            "0.0044298130087554455\n",
            "0.004429160617291927\n",
            "0.004428493790328503\n",
            "0.004427841864526272\n",
            "0.00442720390856266\n",
            "0.0044265370815992355\n",
            "0.004425884690135717\n",
            "0.004425202962011099\n",
            "0.004424550570547581\n",
            "0.004423898179084063\n",
            "0.004423231352120638\n",
            "0.0044226087629795074\n",
            "0.004421927034854889\n",
            "0.004421259742230177\n",
            "0.004420607350766659\n",
            "0.004419940523803234\n",
            "0.0044193328358232975\n",
            "0.004418680444359779\n",
            "0.004418013617396355\n",
            "0.004417316522449255\n",
            "0.004416664596647024\n",
            "0.004416042007505894\n",
            "0.004415374714881182\n",
            "0.004414722789078951\n",
            "0.004414040595293045\n",
            "0.004413403104990721\n",
            "0.004412706475704908\n",
            "0.004412083886563778\n",
            "0.0044114612974226475\n",
            "0.004410779103636742\n",
            "0.004410112276673317\n",
            "0.004409445449709892\n",
            "0.0044088223949074745\n",
            "0.00440815556794405\n",
            "0.0044074589386582375\n",
            "0.004406865686178207\n",
            "0.004406198859214783\n",
            "0.004405531566590071\n",
            "0.004404909443110228\n",
            "0.004404212348163128\n",
            "0.004403619561344385\n",
            "0.004402996972203255\n",
            "0.004402270540595055\n",
            "0.004401647951453924\n",
            "0.0044010104611516\n",
            "0.004400328733026981\n",
            "0.004399691242724657\n",
            "0.004399024415761232\n",
            "0.004398372024297714\n",
            "0.004397719167172909\n",
            "0.004397097043693066\n",
            "0.004396445117890835\n",
            "0.004395747557282448\n",
            "0.004395065829157829\n",
            "0.004394458141177893\n",
            "0.004393821116536856\n",
            "0.0043931687250733376\n",
            "0.004392486996948719\n",
            "0.004391849040985107\n",
            "0.004391182214021683\n",
            "0.004390574526041746\n",
            "0.004389907233417034\n",
            "0.0043892404064536095\n",
            "0.004388602916151285\n",
            "0.004387965425848961\n",
            "0.004387357272207737\n",
            "0.0043866755440831184\n",
            "0.004386038053780794\n",
            "0.004385356325656176\n",
            "0.004384733270853758\n",
            "0.004384155850857496\n",
            "0.004383458755910397\n",
            "0.004382791463285685\n",
            "0.004382139537483454\n",
            "0.0043815020471811295\n",
            "0.004380879458039999\n",
            "0.004380182363092899\n",
            "0.004379560239613056\n",
            "0.00437887804582715\n",
            "0.004378285259008408\n",
            "0.0043776328675448895\n",
            "0.00437693577259779\n",
            "0.004376313649117947\n",
            "0.004375690594315529\n",
            "0.004375053569674492\n",
            "0.004374401178210974\n",
            "0.0043737636879086494\n",
            "0.0043730963952839375\n",
            "0.004372503608465195\n",
            "0.004371821414679289\n",
            "0.004371183924376965\n",
            "0.004370516631752253\n",
            "0.004369879141449928\n",
            "0.004369271919131279\n",
            "0.004368619527667761\n",
            "0.004367967136204243\n",
            "0.004367329645901918\n",
            "0.004366692155599594\n",
            "0.004366055130958557\n",
            "0.004365387838333845\n",
            "0.004364735446870327\n",
            "0.004364112392067909\n",
            "0.004363475367426872\n",
            "0.004362822975963354\n",
            "0.00436218548566103\n",
            "0.004361518193036318\n",
            "0.004360925406217575\n",
            "0.004360273480415344\n",
            "0.004359591286629438\n",
            "0.004359027836471796\n",
            "0.00435831630602479\n",
            "0.004357694182544947\n",
            "0.004357100930064917\n",
            "0.004356388933956623\n",
            "0.0043557812459766865\n",
            "0.004355158656835556\n",
            "0.004354491364210844\n",
            "0.00435385387390852\n",
            "0.004353231750428677\n",
            "0.004352578893303871\n",
            "0.004351956304162741\n",
            "0.004351318813860416\n",
            "0.004350681789219379\n",
            "0.004350088536739349\n",
            "0.004349406808614731\n",
            "0.0043487693183124065\n",
            "0.004348161164671183\n",
            "0.00434746453538537\n",
            "0.004346856847405434\n",
            "0.0043462347239255905\n",
            "0.004345566965639591\n",
            "0.004344944376498461\n",
            "0.004344262648373842\n",
            "0.0043436698615550995\n",
            "0.004343046806752682\n",
            "0.00434237951412797\n",
            "0.004341771826148033\n",
            "0.004341119900345802\n",
            "0.0043404968455433846\n",
            "0.004339904058724642\n",
            "0.00433923676609993\n",
            "0.004338569473475218\n",
            "0.004337961785495281\n",
            "0.004337324295192957\n",
            "0.004336686804890633\n",
            "0.00433609401807189\n",
            "0.004335411824285984\n",
            "0.004334774799644947\n",
            "0.004334151744842529\n",
            "0.004333470016717911\n",
            "0.00433284742757678\n",
            "0.004332209471613169\n",
            "0.004331571981310844\n",
            "0.004330964293330908\n",
            "0.004330297466367483\n",
            "0.004329704213887453\n",
            "0.004329051822423935\n",
            "0.004328444134443998\n",
            "0.004327806178480387\n",
            "0.004327154252678156\n",
            "0.0043265316635370255\n",
            "0.004325953312218189\n",
            "0.0043252864852547646\n",
            "0.0043246932327747345\n",
            "0.0043240259401500225\n",
            "0.004323403351008892\n",
            "0.004322795197367668\n",
            "0.004322158172726631\n",
            "0.004321505781263113\n",
            "0.004320897627621889\n",
            "0.004320245236158371\n",
            "0.004319608211517334\n",
            "0.004319000523537397\n",
            "0.004318333230912685\n",
            "0.0043177250772714615\n",
            "0.0043170880526304245\n",
            "0.004316479898989201\n",
            "0.004315798170864582\n",
            "0.004315204918384552\n",
            "0.0043145823292434216\n",
            "0.004313959274441004\n",
            "0.004313307348638773\n",
            "0.004312684293836355\n",
            "0.004312076605856419\n",
            "0.004311454016715288\n",
            "0.0043108463287353516\n",
            "0.004310208838433027\n",
            "0.004309600684791803\n",
            "0.004308948293328285\n",
            "0.004308341071009636\n",
            "0.0043076882138848305\n",
            "0.004307080525904894\n",
            "0.004306413233280182\n",
            "0.004305820446461439\n",
            "0.0043052127584815025\n",
            "0.004304575268179178\n",
            "0.004303952679038048\n",
            "0.004303314723074436\n",
            "0.004302692133933306\n",
            "0.004302098881453276\n",
            "0.004301461391150951\n",
            "0.0043008094653487206\n",
            "0.00430018687620759\n",
            "0.004299549385905266\n",
            "0.0042989118956029415\n",
            "0.004298288840800524\n",
            "0.0042976513504981995\n",
            "0.004297059029340744\n",
            "0.0042964802123606205\n",
            "0.004295827820897102\n",
            "0.004295190330594778\n",
            "0.004294552840292454\n",
            "0.004293959587812424\n",
            "0.004293322097510099\n",
            "0.004292684607207775\n",
            "0.004292121157050133\n",
            "0.004291439428925514\n",
            "0.004290831740945578\n",
            "0.004290223587304354\n",
            "0.004289571195840836\n",
            "0.004288992844521999\n",
            "0.0042883409187197685\n",
            "0.00428779236972332\n",
            "0.004287110175937414\n",
            "0.004286531824618578\n",
            "0.004285864997655153\n",
            "0.0042852419428527355\n",
            "0.004284664057195187\n",
            "0.004284026566892862\n",
            "0.0042834035120904446\n",
            "0.004282810725271702\n",
            "0.0042821429669857025\n",
            "0.004281550645828247\n",
            "0.0042809126898646355\n",
            "0.004280275199562311\n",
            "0.004279622808098793\n",
            "0.004279074724763632\n",
            "0.004278451669961214\n",
            "0.004277829080820084\n",
            "0.004277206026017666\n",
            "0.004276539199054241\n",
            "0.004275990650057793\n",
            "0.004275353159755468\n",
            "0.004274745471775532\n",
            "0.00427410751581192\n",
            "0.004273544065654278\n",
            "0.004272861871868372\n",
            "0.004272269085049629\n",
            "0.004271646495908499\n",
            "0.004271053709089756\n",
            "0.004270401317626238\n",
            "0.004269793163985014\n",
            "0.004269185476005077\n",
            "0.00426856242120266\n",
            "0.004267939832061529\n",
            "0.004267317242920399\n",
            "0.004266709089279175\n",
            "0.004266101401299238\n",
            "0.004265478812158108\n",
            "0.004264886025339365\n",
            "0.004264218732714653\n",
            "0.004263655282557011\n",
            "0.004263017326593399\n",
            "0.0042624096386134624\n",
            "0.0042617423459887505\n",
            "0.004261194262653589\n",
            "0.004260601010173559\n",
            "0.004259918816387653\n",
            "0.004259355366230011\n",
            "0.0042587327770888805\n",
            "0.0042581395246088505\n",
            "0.004257487133145332\n",
            "0.004256908781826496\n",
            "0.004256286658346653\n",
            "0.004255678504705429\n",
            "0.004255041014403105\n",
            "0.004254462663084269\n",
            "0.004253854975104332\n",
            "0.004253276623785496\n",
            "0.004252594895660877\n",
            "0.004251986742019653\n",
            "0.004251408390700817\n",
            "0.004250770900398493\n",
            "0.004250193014740944\n",
            "0.0042495401576161385\n",
            "0.004248961806297302\n",
            "0.004248383920639753\n",
            "0.004247686825692654\n",
            "0.004247138276696205\n",
            "0.004246545489877462\n",
            "0.004245922435075045\n",
            "0.004245299845933914\n",
            "0.004244692157953978\n",
            "0.004244084004312754\n",
            "0.004243491217494011\n",
            "0.004242883063852787\n",
            "0.004242274910211563\n",
            "0.00424165278673172\n",
            "0.004241029731929302\n",
            "0.004240421578288078\n",
            "0.004239873494952917\n",
            "0.004239221103489399\n",
            "0.0042386422865092754\n",
            "0.004238034598529339\n",
            "0.004237426910549402\n",
            "0.004236788954585791\n",
            "0.004236181266605854\n",
            "0.004235558211803436\n",
            "0.0042349956929683685\n",
            "0.004234327934682369\n",
            "0.0042337351478636265\n",
            "0.004233097657561302\n",
            "0.00423253420740366\n",
            "0.004231926053762436\n",
            "0.004231332801282406\n",
            "0.0042306953109800816\n",
            "0.004230087157338858\n",
            "0.004229509271681309\n",
            "0.004228886682540178\n",
            "0.004228278528898954\n",
            "0.004227685742080212\n",
            "0.004227092489600182\n",
            "0.004226469434797764\n",
            "0.00422586128115654\n",
            "0.004225268494337797\n",
            "0.0042246608063578606\n",
            "0.0042240675538778305\n",
            "0.004223459865897894\n",
            "0.004222881514579058\n",
            "0.004222244024276733\n",
            "0.004221650771796703\n",
            "0.004221043083816767\n",
            "0.004220464266836643\n",
            "0.0042198421433568\n",
            "0.00421924889087677\n",
            "0.004218640737235546\n",
            "0.004217988811433315\n",
            "0.004217410460114479\n",
            "0.004216847009956837\n",
            "0.004216238856315613\n",
            "0.0042156013660132885\n",
            "0.00421502348035574\n",
            "0.0042144302278757095\n",
            "0.0042138369753956795\n",
            "0.004213243722915649\n",
            "0.004212620668113232\n",
            "0.004212013445794582\n",
            "0.004211419727653265\n",
            "0.004210826940834522\n",
            "0.004210174549371004\n",
            "0.0042096409015357494\n",
            "0.0042090327478945255\n",
            "0.004208439961075783\n",
            "0.004207831807434559\n",
            "0.004207238554954529\n",
            "0.004206615965813398\n",
            "0.004206037614494562\n",
            "0.004205415025353432\n",
            "0.004204821772873402\n",
            "0.004204228986054659\n",
            "0.004203605931252241\n",
            "0.004203027579933405\n",
            "0.004202434327453375\n",
            "0.004201826639473438\n",
            "0.004201188683509827\n",
            "0.004200610797852278\n",
            "0.004200003109872341\n",
            "0.004199409857392311\n",
            "0.004198831506073475\n",
            "0.004198238719254732\n",
            "0.004197630099952221\n",
            "0.004197051748633385\n",
            "0.004196414723992348\n",
            "0.004195881076157093\n",
            "0.004195258021354675\n",
            "0.004194664768874645\n",
            "0.004194086883217096\n",
            "0.00419350853189826\n",
            "0.0041928705759346485\n",
            "0.0041923667304217815\n",
            "0.004191729240119457\n",
            "0.004191135987639427\n",
            "0.004190513398498297\n",
            "0.004189964849501848\n",
            "0.004189326893538237\n",
            "0.004188734106719494\n",
            "0.004188185557723045\n",
            "0.004187577869743109\n",
            "0.0041869692504405975\n",
            "0.00418634619563818\n",
            "0.004185753408819437\n",
            "0.0041852048598229885\n",
            "0.004184567369520664\n",
            "0.00418395921587944\n",
            "0.0041833664290606976\n",
            "0.004182802513241768\n",
            "0.004182209726423025\n",
            "0.004181631375104189\n",
            "0.004181023221462965\n",
            "0.004180459771305323\n",
            "0.004179852083325386\n",
            "0.004179243929684162\n",
            "0.004178665578365326\n",
            "0.004178057890385389\n",
            "0.004177449736744165\n",
            "0.004176916088908911\n",
            "0.004176263231784105\n",
            "0.004175730049610138\n",
            "0.00417510699480772\n",
            "0.004174514207988977\n",
            "0.004173965193331242\n",
            "0.004173342604190111\n",
            "0.004172794055193663\n",
            "0.004172156564891338\n",
            "0.0041715484112501144\n",
            "0.004170999862253666\n",
            "0.004170376807451248\n",
            "0.004169768653810024\n",
            "0.004169205669313669\n",
            "0.004168597515672445\n",
            "0.004168019630014896\n",
            "0.00416744127869606\n",
            "0.00416684802621603\n",
            "0.004166284576058388\n",
            "0.00416566152125597\n",
            "0.004165053833276033\n",
            "0.004164490383118391\n",
            "0.0041639115661382675\n",
            "0.004163288976997137\n",
            "0.004162754863500595\n",
            "0.004162102937698364\n",
            "0.004161554388701916\n",
            "0.0041609760373830795\n",
            "0.004160353448241949\n",
            "0.004159849137067795\n",
            "0.004159196745604277\n",
            "0.004158603493124247\n",
            "0.004158039577305317\n",
            "0.004157446324825287\n",
            "0.004156898241490126\n",
            "0.0041563198901712894\n",
            "0.004155726637691259\n",
            "0.004155133385211229\n",
            "0.004154480993747711\n",
            "0.004153961781412363\n",
            "0.00415336899459362\n",
            "0.004152790643274784\n",
            "0.004152212291955948\n",
            "0.004151559434831142\n",
            "0.0041509815491735935\n",
            "0.004150447901338339\n",
            "0.004149840213358402\n",
            "0.004149291198700666\n",
            "0.004148727748543024\n",
            "0.004148134496062994\n",
            "0.0041475119069218636\n",
            "0.004146947991102934\n",
            "0.004146340768784285\n",
            "0.004145791754126549\n",
            "0.004145169164985418\n",
            "0.004144605249166489\n",
            "0.0041440268978476524\n",
            "0.0041434490121901035\n",
            "0.004142870660871267\n",
            "0.0041422476060688496\n",
            "0.004141713958233595\n",
            "0.004141105804592371\n",
            "0.0041405572555959225\n",
            "0.004139964003115892\n",
            "0.00413940055295825\n",
            "0.004138777498155832\n",
            "0.0041381847113370895\n",
            "0.004137605894356966\n",
            "0.004137057811021805\n",
            "0.004136464558541775\n",
            "0.004135871306061745\n",
            "0.004135322757065296\n",
            "0.004134729504585266\n",
            "0.004134106915444136\n",
            "0.004133572801947594\n",
            "0.004132980015128851\n",
            "0.004132386762648821\n",
            "0.004131867550313473\n",
            "0.0041312300600111485\n",
            "0.004130696412175894\n",
            "0.004130118060857058\n",
            "0.004129465669393539\n",
            "0.004128961358219385\n",
            "0.004128338303416967\n",
            "0.004127745050936937\n",
            "0.004127196501940489\n",
            "0.0041266330517828465\n",
            "0.004126039799302816\n",
            "0.004125476349145174\n",
            "0.004124897997826338\n",
            "0.00412427494302392\n",
            "0.0041237263940274715\n",
            "0.004123148508369923\n",
            "0.004122525453567505\n",
            "0.004121991340070963\n",
            "0.004121412988752127\n",
            "0.004120849538594484\n",
            "0.004120271652936935\n",
            "0.004119693301618099\n",
            "0.0041191293857991695\n",
            "0.0041185361333191395\n",
            "0.004117973148822784\n",
            "0.004117379430681467\n",
            "0.004116816446185112\n",
            "0.004116222728043795\n",
            "0.00411565974354744\n",
            "0.004115096293389797\n",
            "0.004114503040909767\n",
            "0.004113983828574419\n",
            "0.004113361239433289\n",
            "0.004112797323614359\n",
            "0.004112234339118004\n",
            "0.004111611284315586\n",
            "0.004111077636480331\n",
            "0.004110454581677914\n",
            "0.004109906032681465\n",
            "0.004109342582523823\n",
            "0.004108808469027281\n",
            "0.004108215216547251\n",
            "0.00410769646987319\n",
            "0.004107058513909578\n",
            "0.004106465261429548\n",
            "0.0041059465147554874\n",
            "0.004105338826775551\n",
            "0.004104760475456715\n",
            "0.004104196559637785\n",
            "0.004103677812963724\n",
            "0.0041030398570001125\n",
            "0.00410247640684247\n",
            "0.004101898055523634\n",
            "0.004101334605365992\n",
            "0.004100771155208349\n",
            "0.004100162535905838\n",
            "0.004099629353731871\n",
            "0.0040990510024130344\n",
            "0.004098457749933004\n",
            "0.004097938537597656\n",
            "0.0040973457507789135\n",
            "0.004096767399460077\n",
            "0.004096189048141241\n",
            "0.004095625132322311\n",
            "0.004095091484487057\n",
            "0.004094542935490608\n",
            "0.004093934781849384\n",
            "0.00409340113401413\n",
            "0.004092822317034006\n",
            "0.004092199727892876\n",
            "0.004091636277735233\n",
            "0.0040910872630774975\n",
            "0.004090509377419949\n",
            "0.004089931026101112\n",
            "0.004089352674782276\n",
            "0.004088833462446928\n",
            "0.004088240675628185\n",
            "0.004087632521986961\n",
            "0.004087113309651613\n",
            "0.0040865493938326836\n",
            "0.004085956607013941\n",
            "0.004085422493517399\n",
            "0.00408487394452095\n",
            "0.004084265790879726\n",
            "0.004083702340722084\n",
            "0.004083138890564442\n",
            "0.004082590341567993\n",
            "0.004082011990249157\n",
            "0.004081418737769127\n",
            "0.004080899525433779\n",
            "0.0040803211741149426\n",
            "0.004079742822796106\n",
            "0.004079209174960852\n",
            "0.004078630357980728\n",
            "0.004078052006661892\n",
            "0.004077548161149025\n",
            "0.004076925106346607\n",
            "0.004076346755027771\n",
            "0.004075827542692423\n",
            "0.00407523475587368\n",
            "0.004074671305716038\n",
            "0.0040740929543972015\n",
            "0.0040735588409006596\n",
            "0.0040729655884206295\n",
            "0.004072417039424181\n",
            "0.004071882925927639\n",
            "0.004071319475769997\n",
            "0.00407071178779006\n",
            "0.004070162773132324\n",
            "0.004069599322974682\n",
            "0.004069006070494652\n",
            "0.004068457521498203\n",
            "0.004067863803356886\n",
            "0.004067315254360437\n",
            "0.004066781606525183\n",
            "0.004066173452883959\n",
            "0.004065654706209898\n",
            "0.004065046552568674\n",
            "0.004064527340233326\n",
            "0.00406394898891449\n",
            "0.004063415341079235\n",
            "0.004062836989760399\n",
            "0.004062273073941469\n",
            "0.004061724990606308\n",
            "0.004061131272464991\n",
            "0.004060597158968449\n",
            "0.004060034174472094\n",
            "0.0040595149621367455\n",
            "0.004058921709656715\n",
            "0.004058387596160173\n",
            "0.004057779908180237\n",
            "0.004057230893522501\n",
            "0.004056667443364859\n",
            "0.004056133795529604\n",
            "0.004055555444210768\n",
            "0.004055006429553032\n",
            "0.004054428078234196\n",
            "0.004053894430398941\n",
            "0.004053330980241299\n",
            "0.004052722826600075\n",
            "0.0040522185154259205\n",
            "0.004051654599606991\n",
            "0.004051046911627054\n",
            "0.0040505132637917995\n",
            "0.004049979150295258\n",
            "0.004049430601298809\n",
            "0.004048837348818779\n",
            "0.004048303700983524\n",
            "0.004047769121825695\n",
            "0.004047175869345665\n",
            "0.004046657122671604\n",
            "0.004046093672513962\n",
            "0.004045500420033932\n",
            "0.00404496630653739\n",
            "0.004044447559863329\n",
            "0.0040438249707221985\n",
            "0.0040432908572256565\n",
            "0.004042742308229208\n",
            "0.004042178392410278\n",
            "0.004041674081236124\n",
            "0.0040410659275949\n",
            "0.0040405322797596455\n",
            "0.004039968363940716\n",
            "0.004039434716105461\n",
            "0.004038841463625431\n",
            "0.0040382929146289825\n",
            "0.00403772946447134\n",
            "0.004037210252135992\n",
            "0.004036587662994862\n",
            "0.004036023747175932\n",
            "0.004035534802824259\n",
            "0.004034955985844135\n",
            "0.0040344372391700745\n",
            "0.004033858422189951\n",
            "0.004033309873193502\n",
            "0.00403274642303586\n",
            "0.004032212309539318\n",
            "0.004031619057059288\n",
            "0.004031085409224033\n",
            "0.0040305363945662975\n",
            "0.004029987845569849\n",
            "0.004029409494251013\n",
            "0.0040289051830768585\n",
            "0.004028341732919216\n",
            "0.004027807619422674\n",
            "0.0040272739715874195\n",
            "0.004026695154607296\n",
            "0.004026132170110941\n",
            "0.004025568254292011\n",
            "0.004024989902973175\n",
            "0.00402445625513792\n",
            "0.004023907706141472\n",
            "0.004023284185677767\n",
            "0.004022809676826\n",
            "0.004022246226668358\n",
            "0.004021697677671909\n",
            "0.004021118860691786\n",
            "0.004020659253001213\n",
            "0.004020081367343664\n",
            "0.0040195174515247345\n",
            "0.004018953535705805\n",
            "0.004018434789031744\n",
            "0.0040178559720516205\n",
            "0.004017322324216366\n",
            "0.004016744438558817\n",
            "0.004016209859400988\n",
            "0.004015691112726927\n",
            "0.004015156999230385\n",
            "0.004014548845589161\n",
            "0.004014029633253813\n",
            "0.004013481084257364\n",
            "0.004012932535260916\n",
            "0.004012338817119598\n",
            "0.004011820070445538\n",
            "0.004011256620287895\n",
            "0.004010707605630159\n",
            "0.0040101888589560986\n",
            "0.0040095956064760685\n",
            "0.004009091295301914\n",
            "0.004008542746305466\n",
            "0.004007949493825436\n",
            "0.0040074298158288\n",
            "0.004006866365671158\n",
            "0.004006332717835903\n",
            "0.004005784168839455\n",
            "0.004005250055342913\n",
            "0.004004686139523983\n",
            "0.004004182294011116\n",
            "0.00400360394269228\n",
            "0.0040030996315181255\n",
            "0.0040025063790380955\n",
            "0.004001987166702747\n",
            "0.004001438617706299\n",
            "0.004000904504209757\n",
            "0.004000326152890921\n",
            "0.00399980740621686\n",
            "0.00399924349039793\n",
            "0.003998724743723869\n",
            "0.003998145926743746\n",
            "0.003997612278908491\n",
            "0.003997092600911856\n",
            "0.003996544051915407\n",
            "0.0039959801360964775\n",
            "0.003995461389422417\n",
            "0.003994912840425968\n",
            "0.003994363825768232\n",
            "0.003993770573288202\n",
            "0.003993281163275242\n",
            "0.003992732614278793\n",
            "0.003992169164121151\n",
            "0.003991605248302221\n",
            "0.003991101402789354\n",
            "0.003990581724792719\n",
            "0.003989988937973976\n",
            "0.00398943992331624\n",
            "0.00398895051330328\n",
            "0.003988372161984444\n",
            "0.0039878529496490955\n",
            "0.003987304400652647\n",
            "0.003986755385994911\n",
            "0.0039862217381596565\n",
            "0.003985702525824308\n",
            "0.003985139075666666\n",
            "0.003984619397670031\n",
            "0.003984085749834776\n",
            "0.0039835222996771336\n",
            "0.003982958849519491\n",
            "0.003982483875006437\n",
            "0.003981905523687601\n",
            "0.003981371410191059\n",
            "0.003980867564678192\n",
            "0.00398030411452055\n",
            "0.00397974019870162\n",
            "0.003979206550866365\n",
            "0.00397865753620863\n",
            "0.0039781383238732815\n",
            "0.003977619577199221\n",
            "0.003977025859057903\n",
            "0.003976477310061455\n",
            "0.003975973464548588\n",
            "0.003975453786551952\n",
            "0.003974905237555504\n",
            "0.0039743417873978615\n",
            "0.003973822575062513\n",
            "0.003973259124904871\n",
            "0.003972725011408329\n",
            "0.003972220700234175\n",
            "0.003971671685576439\n",
            "0.003971093334257603\n",
            "0.003970544785261154\n",
            "0.003970070276409388\n",
            "0.003969506360590458\n",
            "0.003969002049416304\n",
            "0.003968408796936274\n",
            "0.003967890050262213\n",
            "0.003967355936765671\n",
            "0.003966822288930416\n",
            "0.003966303076595068\n",
            "0.003965754061937332\n",
            "0.003965205512940884\n",
            "0.003964701201766729\n",
            "0.0039641521871089935\n",
            "0.003963647875934839\n",
            "0.003963099326938391\n",
            "0.003962535411119461\n",
            "0.0039620166644454\n",
            "0.00396142341196537\n",
            "0.003960919100791216\n",
            "0.003960355184972286\n",
            "0.003959851339459419\n",
            "0.003959346562623978\n",
            "0.003958783112466335\n",
            "0.003958234563469887\n",
            "0.003957729786634445\n",
            "0.003957211039960384\n",
            "0.003956617787480354\n",
            "0.003956054337322712\n",
            "0.003955564461648464\n",
            "0.00395506015047431\n",
            "0.003954541403800249\n",
            "0.003953977953642607\n",
            "0.0039534433744847775\n",
            "0.0039529395289719105\n",
            "0.003952390514314175\n",
            "0.003951841965317726\n",
            "0.003951322752982378\n",
            "0.003950773738324642\n",
            "0.003950269427150488\n",
            "0.003949735779315233\n",
            "0.003949186764657497\n",
            "0.003948667552322149\n",
            "0.003948133438825607\n",
            "0.0039476146921515465\n",
            "0.003947021439671516\n",
            "0.0039465464651584625\n",
            "0.00394598301500082\n",
            "0.003945463802665472\n",
            "0.003944959957152605\n",
            "0.003944381605833769\n",
            "0.003943862393498421\n",
            "0.003943313378840685\n",
            "0.003942838869988918\n",
            "0.0039422898553311825\n",
            "0.003941740840673447\n",
            "0.003941207192838192\n",
            "0.003940702881664038\n",
            "0.003940153867006302\n",
            "0.003939650021493435\n",
            "0.003939101006835699\n",
            "0.003938581794500351\n",
            "0.003938077483326197\n",
            "0.0039374991320073605\n",
            "0.003936950117349625\n",
            "0.0039364309050142765\n",
            "0.003935941960662603\n",
            "0.003935348242521286\n",
            "0.003934888634830713\n",
            "0.003934324719011784\n",
            "0.003933791071176529\n",
            "0.003933257423341274\n",
            "0.00393275311216712\n",
            "0.003932233899831772\n",
            "0.003931729588657618\n",
            "0.003931165672838688\n",
            "0.00393064646050334\n",
            "0.003930097911506891\n",
            "0.003929608501493931\n",
            "0.003929030150175095\n",
            "0.0039285109378397465\n",
            "0.003928006626665592\n",
            "0.003927502315491438\n",
            "0.003926953766494989\n",
            "0.003926434554159641\n",
            "0.003925900440663099\n",
            "0.003925366327166557\n",
            "0.003924847114831209\n",
            "0.003924327902495861\n",
            "0.003923793788999319\n",
            "0.003923289943486452\n",
            "0.003922770731151104\n",
            "0.003922192379832268\n",
            "0.003921732306480408\n",
            "0.003921168856322765\n",
            "0.003920649643987417\n",
            "0.003920100629329681\n",
            "0.003919596783816814\n",
            "0.003919107373803854\n",
            "0.003918573260307312\n",
            "0.003918068949133158\n",
            "0.003917505033314228\n",
            "0.00391698582097888\n",
            "0.003916451707482338\n",
            "0.003915917593985796\n",
            "0.003915472887456417\n",
            "0.003914909437298775\n",
            "0.003914375323802233\n",
            "0.003913826309144497\n",
            "0.00391332246363163\n",
            "0.003912788350135088\n",
            "0.003912254236638546\n",
            "0.003911750391125679\n",
            "0.003911246079951525\n",
            "0.003910696599632502\n",
            "0.003910148050636053\n",
            "0.003909658640623093\n",
            "0.003909110091626644\n",
            "0.0039086053147912025\n",
            "0.003908026963472366\n",
            "0.0039075524546206\n",
            "0.0039070481434464455\n",
            "0.003906543366611004\n",
            "0.0039059650152921677\n",
            "0.003905460936948657\n",
            "0.0039049270562827587\n",
            "0.003904407611116767\n",
            "0.0039039182011038065\n",
            "0.0039033994544297457\n",
            "0.00390286510810256\n",
            "0.0039023165591061115\n",
            "0.003901856718584895\n",
            "0.003901307936757803\n",
            "0.003900773823261261\n",
            "0.0039003142155706882\n",
            "0.0038997652009129524\n",
            "0.003899260889738798\n",
            "0.003898697206750512\n",
            "0.0038982077967375517\n",
            "0.0038976736832410097\n",
            "0.0038971100002527237\n",
            "0.0038965758867561817\n",
            "0.0038960715755820274\n",
            "0.003895611735060811\n",
            "0.0038950929883867502\n",
            "0.003894573310390115\n",
            "0.0038940992671996355\n",
            "0.003893505781888962\n",
            "0.003893001237884164\n",
            "0.0038924673572182655\n",
            "0.003891977947205305\n",
            "0.0038914289325475693\n",
            "0.0038909248542040586\n",
            "0.0038904352113604546\n",
            "0.0038898715283721685\n",
            "0.0038893823511898518\n",
            "0.0038888331037014723\n",
            "0.0038883734960108995\n",
            "0.0038878396153450012\n",
            "0.0038873055018484592\n",
            "0.0038868009578436613\n",
            "0.0038862673100084066\n",
            "0.0038857480976730585\n",
            "0.0038852733559906483\n",
            "0.003884769044816494\n",
            "0.003884249832481146\n",
            "0.003883715718984604\n",
            "0.0038832558784633875\n",
            "0.0038826330564916134\n",
            "0.003882187884300947\n",
            "0.003881639102473855\n",
            "0.0038811201229691505\n",
            "0.0038806453812867403\n",
            "0.003880126401782036\n",
            "0.0038795773871243\n",
            "0.003879072843119502\n",
            "0.0038785538636147976\n",
            "0.003878049785271287\n",
            "0.003877501003444195\n",
            "0.003876996226608753\n",
            "0.003876477014273405\n",
            "0.003876002738252282\n",
            "0.0038754683919250965\n",
            "0.0038749792147427797\n",
            "0.0038744299672544003\n",
            "0.0038739258889108896\n",
            "0.0038734658155590296\n",
            "0.003872917266562581\n",
            "0.0038723682519048452\n",
            "0.0038718937430530787\n",
            "0.0038713894318789244\n",
            "0.0038708553183823824\n",
            "0.0038703512400388718\n",
            "0.0038698469288647175\n",
            "0.0038693128153681755\n",
            "0.0038688231725245714\n",
            "0.0038682890590280294\n",
            "0.003867829218506813\n",
            "0.0038672953378409147\n",
            "0.0038667614571750164\n",
            "0.0038663013838231564\n",
            "0.003865752834826708\n",
            "0.0038651595823466778\n",
            "0.0038647290784865618\n",
            "0.0038641802966594696\n",
            "0.003863691119477153\n",
            "0.003863201243802905\n",
            "0.0038626971654593945\n",
            "0.0038621926214545965\n",
            "0.0038616436067968607\n",
            "0.0038611392956227064\n",
            "0.0038606796879321337\n",
            "0.0038601160049438477\n",
            "0.0038596410304307938\n",
            "0.0038591071497648954\n",
            "0.003858602838590741\n",
            "0.003858098527416587\n",
            "0.003857534844428301\n",
            "0.0038570156320929527\n",
            "0.0038565555587410927\n",
            "0.0038560216780751944\n",
            "0.003855561837553978\n",
            "0.00385504262521863\n",
            "0.0038545236457139254\n",
            "0.003854004666209221\n",
            "0.003853514790534973\n",
            "0.003852995578199625\n",
            "0.0038525061681866646\n",
            "0.0038519869558513165\n",
            "0.003851482877507806\n",
            "0.003850948764011264\n",
            "0.003850429318845272\n",
            "0.0038499843794852495\n",
            "0.003849435830488801\n",
            "0.0038489163853228092\n",
            "0.0038483974058181047\n",
            "0.003847952466458082\n",
            "0.0038474034518003464\n",
            "0.0038468842394649982\n",
            "0.003846409497782588\n",
            "0.00384589028544724\n",
            "0.003845370840281248\n",
            "0.003844896564260125\n",
            "0.0038443771190941334\n",
            "0.003843872807919979\n",
            "0.0038433538284152746\n",
            "0.0038428939878940582\n",
            "0.00384237477555871\n",
            "0.0038418557960540056\n",
            "0.0038413661532104015\n",
            "0.0038408616092056036\n",
            "0.0038403423968702555\n",
            "0.0038397787138819695\n",
            "0.003839333774521947\n",
            "0.0038388296961784363\n",
            "0.003838280448690057\n",
            "0.003837820840999484\n",
            "0.0038373013958334923\n",
            "0.0038367826491594315\n",
            "0.0038362927734851837\n",
            "0.0038357884623110294\n",
            "0.003835284151136875\n",
            "0.003834794508293271\n",
            "0.0038343050982803106\n",
            "0.0038338154554367065\n",
            "0.0038332517724484205\n",
            "0.0038327774964272976\n",
            "0.003832272719591856\n",
            "0.003831753507256508\n",
            "0.0038313085678964853\n",
            "0.0038307448849081993\n",
            "0.0038302852772176266\n",
            "0.003829750930890441\n",
            "0.0038292615208774805\n",
            "0.0038287274073809385\n",
            "0.0038282377645373344\n",
            "0.00382773345336318\n",
            "0.003827303182333708\n",
            "0.0038267546333372593\n",
            "0.0038262500893324614\n",
            "0.0038257755804806948\n",
            "0.003825226565822959\n",
            "0.0038247371558099985\n",
            "0.0038242919836193323\n",
            "0.0038237879052758217\n",
            "0.00382326846010983\n",
            "0.0038227641489356756\n",
            "0.003822244703769684\n",
            "0.0038217254914343357\n",
            "0.0038212211802601814\n",
            "0.0038207760080695152\n",
            "0.0038202719297260046\n",
            "0.0038197082467377186\n",
            "0.0038192931097000837\n",
            "0.0038187438622117043\n",
            "0.003818284021690488\n",
            "0.0038177799433469772\n",
            "0.0038172309286892414\n",
            "0.0038167412858456373\n",
            "0.003816207405179739\n",
            "0.003815702861174941\n",
            "0.003815258154645562\n",
            "0.0038147536106407642\n",
            "0.00381424929946661\n",
            "0.003813774324953556\n",
            "0.0038132553454488516\n",
            "0.00381273590028286\n",
            "0.0038122909609228373\n",
            "0.0038117719814181328\n",
            "0.0038112527690827847\n",
            "0.0038107482250779867\n",
            "0.0038102734833955765\n",
            "0.0038098138757050037\n",
            "0.0038092946633696556\n",
            "0.0038087901193648577\n",
            "0.003808315610513091\n",
            "0.0038077665958553553\n",
            "0.0038073216564953327\n",
            "0.003806772641837597\n",
            "0.0038063274696469307\n",
            "0.003805778454989195\n",
            "0.003805304178968072\n",
            "0.00380478473380208\n",
            "0.0038042801897972822\n",
            "0.0038038352504372597\n",
            "0.003803331172093749\n",
            "0.0038027968257665634\n",
            "0.003802277846261859\n",
            "0.0038018180057406425\n",
            "0.0038012987934052944\n",
            "0.0038008536212146282\n",
            "0.003800290171056986\n",
            "0.003799815196543932\n",
            "0.0037993404548615217\n",
            "0.0037988657131791115\n",
            "0.003798331832513213\n",
            "0.0037979017943143845\n",
            "0.003797367447987199\n",
            "0.0037968631368130445\n",
            "0.003796388627961278\n",
            "0.003795869182795286\n",
            "0.003795394441112876\n",
            "0.0037949199322611094\n",
            "0.0037944004870951176\n",
            "0.003793911077082157\n",
            "0.0037934212014079094\n",
            "0.003792946692556143\n",
            "0.003792427247390151\n",
            "0.0037919823080301285\n",
            "0.0037914481945335865\n",
            "0.00379097368568182\n",
            "0.0037904693745076656\n",
            "0.003790024435147643\n",
            "0.0037895049899816513\n",
            "0.003788985777646303\n",
            "0.003788511035963893\n",
            "0.003787991823628545\n",
            "0.0037875021807849407\n",
            "0.0037870274391025305\n",
            "0.003786567598581314\n",
            "0.0037860781885683537\n",
            "0.0037855440750718117\n",
            "0.0037850693333894014\n",
            "0.0037845501210540533\n",
            "0.003784090280532837\n",
            "0.003783600637689233\n",
            "0.003783066524192691\n",
            "0.003782577347010374\n",
            "0.00378210237249732\n",
            "0.0037815680261701345\n",
            "0.00378116755746305\n",
            "0.003780648810788989\n",
            "0.0037801589351147413\n",
            "0.003779669525101781\n",
            "0.003779135411605239\n",
            "0.0037787053734064102\n",
            "0.0037782005965709686\n",
            "0.003777711419388652\n",
            "0.003777221543714404\n",
            "0.00377671723254025\n",
            "0.0037762427236884832\n",
            "0.0037757677491754293\n",
            "0.0037752336356788874\n",
            "0.0037747290916740894\n",
            "0.0037742694839835167\n",
            "0.0037738094106316566\n",
            "0.0037732901982963085\n",
            "0.0037728154566138983\n",
            "0.003772296477109194\n",
            "0.0037718366365879774\n",
            "0.0037713469937443733\n",
            "0.0037708573509007692\n",
            "0.003770367708057165\n",
            "0.0037698782980442047\n",
            "0.0037693739868700504\n",
            "0.0037689139135181904\n",
            "0.00376843917183578\n",
            "0.003767919959500432\n",
            "0.0037674603518098593\n",
            "0.0037669558078050613\n",
            "0.0037664512638002634\n",
            "0.003765991423279047\n",
            "0.0037655020132660866\n",
            "0.003764982335269451\n",
            "0.0037645078264176846\n",
            "0.003764018416404724\n",
            "0.003763543674722314\n",
            "0.0037630393635481596\n",
            "0.0037625643890351057\n",
            "0.0037620896473526955\n",
            "0.003761585336178541\n",
            "0.003761140163987875\n",
            "0.003760606050491333\n",
            "0.0037601906806230545\n",
            "0.003759701270610094\n",
            "0.0037591522559523582\n",
            "0.0037587371189147234\n",
            "0.0037581881042569876\n",
            "0.0037577578332275152\n",
            "0.0037572532892227173\n",
            "0.0037567787803709507\n",
            "0.0037562595680356026\n",
            "0.0037557848263531923\n",
            "0.0037553543224930763\n",
            "0.003754820441827178\n",
            "0.0037543303333222866\n",
            "0.0037538858596235514\n",
            "0.0037533810827881098\n",
            "0.0037528916727751493\n",
            "0.003752431832253933\n",
            "0.003751897718757391\n",
            "0.0037514381110668182\n",
            "0.0037509482353925705\n",
            "0.0037504732608795166\n",
            "0.003749954281374812\n",
            "0.0037494793068617582\n",
            "0.0037490196991711855\n",
            "0.003748515387997031\n",
            "0.003748114686459303\n",
            "0.003747565671801567\n",
            "0.0037470911629498005\n",
            "0.0037466760259121656\n",
            "0.003746126778423786\n",
            "0.0037456522695720196\n",
            "0.003745147492736578\n",
            "0.003744702786207199\n",
            "0.003744228044524789\n",
            "0.003743708599358797\n",
            "0.0037432340905070305\n",
            "0.0037427148781716824\n",
            "0.0037422694731503725\n",
            "0.0037417798303067684\n",
            "0.003741275053471327\n",
            "0.003740815445780754\n",
            "0.00374032580293715\n",
            "0.0037398512940853834\n",
            "0.003739376785233617\n",
            "0.0037389167118817568\n",
            "0.0037384419701993465\n",
            "0.003737952560186386\n",
            "0.0037374477833509445\n",
            "0.003736973274499178\n",
            "0.003736498299986124\n",
            "0.0037360384594649076\n",
            "0.0037355341482907534\n",
            "0.0037350296042859554\n",
            "0.003734614234417677\n",
            "0.003734095022082329\n",
            "0.003733620047569275\n",
            "0.003733145771548152\n",
            "0.0037326705642044544\n",
            "0.0037321962881833315\n",
            "0.0037317066453397274\n",
            "0.003731231903657317\n",
            "0.0037307124584913254\n",
            "0.003730297088623047\n",
            "0.0037297778762876987\n",
            "0.0037293625064194202\n",
            "0.003728842828422785\n",
            "0.0037283683195710182\n",
            "0.003727893577888608\n",
            "0.0037274037022143602\n",
            "0.003726973896846175\n",
            "0.0037264542188495398\n",
            "0.003725994611158967\n",
            "0.0037255201023072004\n",
            "0.003725015325471759\n",
            "0.0037245703861117363\n",
            "0.003724066074937582\n",
            "0.003723591100424528\n",
            "0.0037231312599033117\n",
            "0.0037226269487291574\n",
            "0.003722182009369135\n",
            "0.0037216772325336933\n",
            "0.003721232060343027\n",
            "0.0037207722198218107\n",
            "0.0037202530074864626\n",
            "0.003719822969287634\n",
            "0.003719333093613386\n",
            "0.0037188436836004257\n",
            "0.0037183540407568216\n",
            "0.0037178942002356052\n",
            "0.003717434359714389\n",
            "0.003716959385201335\n",
            "0.003716469742357731\n",
            "0.0037159952335059643\n",
            "0.0037155349273234606\n",
            "0.003715075319632888\n",
            "0.00371457077562809\n",
            "0.003714095801115036\n",
            "0.0037136655300855637\n",
            "0.003713161451742053\n",
            "0.003712701378390193\n",
            "0.0037122564390301704\n",
            "0.0037117518950253725\n",
            "0.0037112622521817684\n",
            "0.0037108026444911957\n",
            "0.0037102976348251104\n",
            "0.0037098380271345377\n",
            "0.0037093928549438715\n",
            "0.003708918346092105\n",
            "0.003708458039909601\n",
            "0.0037079835310578346\n",
            "0.0037075087893754244\n",
            "0.0037069895770400763\n",
            "0.0037064997013658285\n",
            "0.003706069663167\n",
            "0.0037055800203233957\n",
            "0.003705090843141079\n",
            "0.0037046303041279316\n",
            "0.003704155795276165\n",
            "0.003703710390254855\n",
            "0.003703191177919507\n",
            "0.003702776040881872\n",
            "0.003702286398038268\n",
            "0.003701796755194664\n",
            "0.003701336681842804\n",
            "0.0037008621729910374\n",
            "0.0037004321347922087\n",
            "0.003699882887303829\n",
            "0.0036994677502661943\n",
            "0.003698977641761303\n",
            "0.00369850336574018\n",
            "0.0036980430595576763\n",
            "0.0036975385155528784\n",
            "0.0036971084773540497\n",
            "0.003696618601679802\n",
            "0.0036961291916668415\n",
            "0.003695728490129113\n",
            "0.0036951943766325712\n",
            "0.003694734536111355\n",
            "0.0036942744627594948\n",
            "0.003693814855068922\n",
            "0.003693339880555868\n",
            "0.0036928500048816204\n",
            "0.003692435100674629\n",
            "0.0036919303238391876\n",
            "0.003691485384479165\n",
            "0.0036909806076437235\n",
            "0.0036905209999531507\n",
            "0.0036900462582707405\n",
            "0.003689585952088237\n",
            "0.0036891414783895016\n",
            "0.0036886364687234163\n",
            "0.0036881621927022934\n",
            "0.003687657415866852\n",
            "0.003687256947159767\n",
            "0.0036867372691631317\n",
            "0.003686292562633753\n",
            "0.0036858622916042805\n",
            "0.0036853726487606764\n",
            "0.00368492747657001\n",
            "0.003684467636048794\n",
            "0.003683963092043996\n",
            "0.003683562623336911\n",
            "0.0036830431781709194\n",
            "0.003682553768157959\n",
            "0.003682138631120324\n",
            "0.0036816634237766266\n",
            "0.003681174013763666\n",
            "0.0036806990392506123\n",
            "0.0036802985705435276\n",
            "0.0036797940265387297\n",
            "0.0036793493200093508\n",
            "0.003678859444335103\n",
            "0.003678384469822049\n",
            "0.003677939297631383\n",
            "0.0036774794571101665\n",
            "0.003676990047097206\n",
            "0.0036765597760677338\n",
            "0.0036760400980710983\n",
            "0.0036756102927029133\n",
            "0.003675135551020503\n",
            "0.003674690145999193\n",
            "0.0036742002703249454\n",
            "0.0036737550981342793\n",
            "0.003673265688121319\n",
            "0.0036727909464389086\n",
            "0.003672301536425948\n",
            "0.003671871032565832\n",
            "0.0036714556626975536\n",
            "0.0036709660198539495\n",
            "0.0036704617086797953\n",
            "0.003670061007142067\n",
            "0.0036695862654596567\n",
            "0.0036690817214548588\n",
            "0.0036686367820948362\n",
            "0.003668176708742976\n",
            "0.003667672397568822\n",
            "0.0036672421265393496\n",
            "0.0036667524836957455\n",
            "0.0036663224454969168\n",
            "0.0036658472381532192\n",
            "0.0036653727293014526\n",
            "0.0036649422254413366\n",
            "0.0036644379142671824\n",
            "0.003663992742076516\n",
            "0.00366354756988585\n",
            "0.0036630877293646336\n",
            "0.0036625831853598356\n",
            "0.0036621231120079756\n",
            "0.003661693073809147\n",
            "0.003661247668787837\n",
            "0.0036607582587748766\n",
            "0.0036603277549147606\n",
            "0.003659868147224188\n",
            "0.003659453010186553\n",
            "0.0036589184310287237\n",
            "0.0036584881599992514\n",
            "0.0036579985171556473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdBCEddPCG1z"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now that we have defined the data loaders, model, loss function and optimizer, we are ready to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-8oEznp1T9t"
      },
      "source": [
        "def accuracy(outputs, labels):\r\n",
        "    _, preds = torch.max(outputs, dim=1)\r\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\r\n",
        "\r\n",
        "def train_and_evaluate(train_dl,\r\n",
        "                       valid_dl,\r\n",
        "                       model,\r\n",
        "                       opt,\r\n",
        "                       loss_func,\r\n",
        "                       epochs,\r\n",
        "                       device):\r\n",
        "  \r\n",
        "  train_history = []\r\n",
        "  val_history = []\r\n",
        "\r\n",
        "  for epoch in range(epochs):\r\n",
        "    print(f\"\\n\\nEPOCH {epoch}\\n\\n\")\r\n",
        "    model.train()\r\n",
        "    for i, (train_batch, labels) in enumerate(train_dl):\r\n",
        "      train_batch, labels = train_batch.to(device), labels.to(device)\r\n",
        "      preds = model(train_batch)\r\n",
        "      loss = loss_func(preds, labels)\r\n",
        "      acc = accuracy(preds, labels)\r\n",
        "      print(f\"Train Loss: {loss}    Train Acc:  {acc}  at batch {i}.\")\r\n",
        "      train_history.append((loss, acc))\r\n",
        "      opt.zero_grad()\r\n",
        "      loss.backward()\r\n",
        "      opt.step()\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "      for j, (val_batch, labels) in enumerate(valid_dl):\r\n",
        "        val_batch, labels = val_batch.to(device), labels.to(device)\r\n",
        "        preds = model(val_batch)\r\n",
        "        loss = loss_func(preds, labels)\r\n",
        "        acc = accuracy(preds, labels)\r\n",
        "        print(f\"Valid Loss: {loss}    Valid Acc:  {acc}  at batch {j}.\")\r\n",
        "        val_history.append((loss, acc))\r\n",
        "\r\n",
        "  return train_history, val_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unOeHM-s1M9x",
        "outputId": "c2c9a6b7-05eb-4f0f-8bcf-f283ab40ec50"
      },
      "source": [
        "input_size = 1 * 28 * 28\r\n",
        "num_classes = 10\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "model = Model(hidden_dims=[300],\r\n",
        "              input_size=input_size,\r\n",
        "              num_classes=num_classes).to(device)\r\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-2)\r\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\r\n",
        "valid_dl = DataLoader(train_ds, batch_size=32, shuffle=False)\r\n",
        "\r\n",
        "train_history, val_history = train_and_evaluate(train_dl=train_dl, \r\n",
        "                                                valid_dl=valid_dl,\r\n",
        "                                                model=model,\r\n",
        "                                                opt=opt,\r\n",
        "                                                loss_func=loss_fn,\r\n",
        "                                                epochs=3,\r\n",
        "                                                device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Valid Loss: 2.0704398155212402    Valid Acc:  0.5  at batch 1263.\n",
            "Valid Loss: 1.9882088899612427    Valid Acc:  0.59375  at batch 1264.\n",
            "Valid Loss: 1.9380813837051392    Valid Acc:  0.5625  at batch 1265.\n",
            "Valid Loss: 2.0191478729248047    Valid Acc:  0.59375  at batch 1266.\n",
            "Valid Loss: 1.9500341415405273    Valid Acc:  0.6875  at batch 1267.\n",
            "Valid Loss: 1.9100072383880615    Valid Acc:  0.75  at batch 1268.\n",
            "Valid Loss: 1.9692661762237549    Valid Acc:  0.6875  at batch 1269.\n",
            "Valid Loss: 1.8743853569030762    Valid Acc:  0.46875  at batch 1270.\n",
            "Valid Loss: 1.9732939004898071    Valid Acc:  0.625  at batch 1271.\n",
            "Valid Loss: 1.903684139251709    Valid Acc:  0.71875  at batch 1272.\n",
            "Valid Loss: 2.024416923522949    Valid Acc:  0.375  at batch 1273.\n",
            "Valid Loss: 1.9772415161132812    Valid Acc:  0.65625  at batch 1274.\n",
            "Valid Loss: 1.9340842962265015    Valid Acc:  0.59375  at batch 1275.\n",
            "Valid Loss: 1.9575815200805664    Valid Acc:  0.59375  at batch 1276.\n",
            "Valid Loss: 1.9432071447372437    Valid Acc:  0.625  at batch 1277.\n",
            "Valid Loss: 1.9999887943267822    Valid Acc:  0.5625  at batch 1278.\n",
            "Valid Loss: 1.981095314025879    Valid Acc:  0.5625  at batch 1279.\n",
            "Valid Loss: 1.971055269241333    Valid Acc:  0.65625  at batch 1280.\n",
            "Valid Loss: 1.9832168817520142    Valid Acc:  0.625  at batch 1281.\n",
            "Valid Loss: 1.9213424921035767    Valid Acc:  0.6875  at batch 1282.\n",
            "Valid Loss: 1.986875057220459    Valid Acc:  0.625  at batch 1283.\n",
            "Valid Loss: 1.9023804664611816    Valid Acc:  0.65625  at batch 1284.\n",
            "Valid Loss: 1.9191980361938477    Valid Acc:  0.5  at batch 1285.\n",
            "Valid Loss: 1.9883713722229004    Valid Acc:  0.59375  at batch 1286.\n",
            "Valid Loss: 1.9795061349868774    Valid Acc:  0.625  at batch 1287.\n",
            "Valid Loss: 1.9596444368362427    Valid Acc:  0.59375  at batch 1288.\n",
            "Valid Loss: 1.983398675918579    Valid Acc:  0.625  at batch 1289.\n",
            "Valid Loss: 1.9766845703125    Valid Acc:  0.5  at batch 1290.\n",
            "Valid Loss: 1.8791649341583252    Valid Acc:  0.78125  at batch 1291.\n",
            "Valid Loss: 2.015810012817383    Valid Acc:  0.59375  at batch 1292.\n",
            "Valid Loss: 1.9272476434707642    Valid Acc:  0.75  at batch 1293.\n",
            "Valid Loss: 1.935712218284607    Valid Acc:  0.75  at batch 1294.\n",
            "Valid Loss: 1.9917645454406738    Valid Acc:  0.5625  at batch 1295.\n",
            "Valid Loss: 1.9859545230865479    Valid Acc:  0.5  at batch 1296.\n",
            "Valid Loss: 1.989212155342102    Valid Acc:  0.59375  at batch 1297.\n",
            "Valid Loss: 1.9278817176818848    Valid Acc:  0.65625  at batch 1298.\n",
            "Valid Loss: 2.0251247882843018    Valid Acc:  0.46875  at batch 1299.\n",
            "Valid Loss: 1.9712032079696655    Valid Acc:  0.5625  at batch 1300.\n",
            "Valid Loss: 1.8934751749038696    Valid Acc:  0.65625  at batch 1301.\n",
            "Valid Loss: 1.949421763420105    Valid Acc:  0.53125  at batch 1302.\n",
            "Valid Loss: 1.939121961593628    Valid Acc:  0.78125  at batch 1303.\n",
            "Valid Loss: 2.0199344158172607    Valid Acc:  0.625  at batch 1304.\n",
            "Valid Loss: 1.9250752925872803    Valid Acc:  0.65625  at batch 1305.\n",
            "Valid Loss: 1.966843843460083    Valid Acc:  0.5625  at batch 1306.\n",
            "Valid Loss: 1.9372155666351318    Valid Acc:  0.6875  at batch 1307.\n",
            "Valid Loss: 1.9079712629318237    Valid Acc:  0.71875  at batch 1308.\n",
            "Valid Loss: 1.9534107446670532    Valid Acc:  0.65625  at batch 1309.\n",
            "Valid Loss: 1.9935024976730347    Valid Acc:  0.5625  at batch 1310.\n",
            "Valid Loss: 1.9089490175247192    Valid Acc:  0.75  at batch 1311.\n",
            "Valid Loss: 2.018573522567749    Valid Acc:  0.53125  at batch 1312.\n",
            "Valid Loss: 1.957987666130066    Valid Acc:  0.59375  at batch 1313.\n",
            "Valid Loss: 1.9475847482681274    Valid Acc:  0.6875  at batch 1314.\n",
            "Valid Loss: 1.992242455482483    Valid Acc:  0.59375  at batch 1315.\n",
            "Valid Loss: 1.9578396081924438    Valid Acc:  0.53125  at batch 1316.\n",
            "Valid Loss: 1.8967732191085815    Valid Acc:  0.625  at batch 1317.\n",
            "Valid Loss: 1.9331731796264648    Valid Acc:  0.59375  at batch 1318.\n",
            "Valid Loss: 1.9574295282363892    Valid Acc:  0.5625  at batch 1319.\n",
            "Valid Loss: 1.9651107788085938    Valid Acc:  0.59375  at batch 1320.\n",
            "Valid Loss: 1.9197914600372314    Valid Acc:  0.8125  at batch 1321.\n",
            "Valid Loss: 1.9256796836853027    Valid Acc:  0.71875  at batch 1322.\n",
            "Valid Loss: 2.002537965774536    Valid Acc:  0.46875  at batch 1323.\n",
            "Valid Loss: 1.9597861766815186    Valid Acc:  0.625  at batch 1324.\n",
            "Valid Loss: 2.018908977508545    Valid Acc:  0.5  at batch 1325.\n",
            "Valid Loss: 2.0041022300720215    Valid Acc:  0.59375  at batch 1326.\n",
            "Valid Loss: 1.9431427717208862    Valid Acc:  0.53125  at batch 1327.\n",
            "Valid Loss: 1.9563606977462769    Valid Acc:  0.65625  at batch 1328.\n",
            "Valid Loss: 1.9947057962417603    Valid Acc:  0.59375  at batch 1329.\n",
            "Valid Loss: 1.9784482717514038    Valid Acc:  0.625  at batch 1330.\n",
            "Valid Loss: 1.9936500787734985    Valid Acc:  0.53125  at batch 1331.\n",
            "Valid Loss: 1.8885996341705322    Valid Acc:  0.75  at batch 1332.\n",
            "Valid Loss: 1.878321886062622    Valid Acc:  0.78125  at batch 1333.\n",
            "Valid Loss: 2.0289735794067383    Valid Acc:  0.59375  at batch 1334.\n",
            "Valid Loss: 1.96713125705719    Valid Acc:  0.65625  at batch 1335.\n",
            "Valid Loss: 1.9415944814682007    Valid Acc:  0.5625  at batch 1336.\n",
            "Valid Loss: 1.9835306406021118    Valid Acc:  0.625  at batch 1337.\n",
            "Valid Loss: 1.9862140417099    Valid Acc:  0.65625  at batch 1338.\n",
            "Valid Loss: 1.9308664798736572    Valid Acc:  0.78125  at batch 1339.\n",
            "Valid Loss: 1.9693129062652588    Valid Acc:  0.6875  at batch 1340.\n",
            "Valid Loss: 2.0080859661102295    Valid Acc:  0.59375  at batch 1341.\n",
            "Valid Loss: 1.9382216930389404    Valid Acc:  0.625  at batch 1342.\n",
            "Valid Loss: 1.9641298055648804    Valid Acc:  0.71875  at batch 1343.\n",
            "Valid Loss: 1.935551404953003    Valid Acc:  0.625  at batch 1344.\n",
            "Valid Loss: 1.9601680040359497    Valid Acc:  0.5625  at batch 1345.\n",
            "Valid Loss: 1.906807780265808    Valid Acc:  0.59375  at batch 1346.\n",
            "Valid Loss: 2.0334720611572266    Valid Acc:  0.5625  at batch 1347.\n",
            "Valid Loss: 1.9836074113845825    Valid Acc:  0.46875  at batch 1348.\n",
            "Valid Loss: 1.9625725746154785    Valid Acc:  0.5625  at batch 1349.\n",
            "Valid Loss: 1.964544653892517    Valid Acc:  0.5625  at batch 1350.\n",
            "Valid Loss: 1.8809080123901367    Valid Acc:  0.71875  at batch 1351.\n",
            "Valid Loss: 2.0051169395446777    Valid Acc:  0.5  at batch 1352.\n",
            "Valid Loss: 1.9781380891799927    Valid Acc:  0.625  at batch 1353.\n",
            "Valid Loss: 1.9599462747573853    Valid Acc:  0.71875  at batch 1354.\n",
            "Valid Loss: 1.9727754592895508    Valid Acc:  0.65625  at batch 1355.\n",
            "Valid Loss: 1.977510929107666    Valid Acc:  0.5625  at batch 1356.\n",
            "Valid Loss: 1.9890482425689697    Valid Acc:  0.59375  at batch 1357.\n",
            "Valid Loss: 1.8979226350784302    Valid Acc:  0.78125  at batch 1358.\n",
            "Valid Loss: 1.9988127946853638    Valid Acc:  0.5625  at batch 1359.\n",
            "Valid Loss: 1.8918981552124023    Valid Acc:  0.8125  at batch 1360.\n",
            "Valid Loss: 1.9963624477386475    Valid Acc:  0.59375  at batch 1361.\n",
            "Valid Loss: 2.027963161468506    Valid Acc:  0.53125  at batch 1362.\n",
            "Valid Loss: 2.014481782913208    Valid Acc:  0.53125  at batch 1363.\n",
            "Valid Loss: 1.9856536388397217    Valid Acc:  0.46875  at batch 1364.\n",
            "Valid Loss: 1.9198319911956787    Valid Acc:  0.65625  at batch 1365.\n",
            "Valid Loss: 2.0021743774414062    Valid Acc:  0.625  at batch 1366.\n",
            "Valid Loss: 2.0182249546051025    Valid Acc:  0.4375  at batch 1367.\n",
            "Valid Loss: 2.0406863689422607    Valid Acc:  0.40625  at batch 1368.\n",
            "Valid Loss: 1.9532885551452637    Valid Acc:  0.625  at batch 1369.\n",
            "Valid Loss: 1.992403268814087    Valid Acc:  0.6875  at batch 1370.\n",
            "Valid Loss: 2.0546977519989014    Valid Acc:  0.53125  at batch 1371.\n",
            "Valid Loss: 1.9808194637298584    Valid Acc:  0.6875  at batch 1372.\n",
            "Valid Loss: 1.930200457572937    Valid Acc:  0.71875  at batch 1373.\n",
            "Valid Loss: 2.001861810684204    Valid Acc:  0.5625  at batch 1374.\n",
            "Valid Loss: 1.9290884733200073    Valid Acc:  0.53125  at batch 1375.\n",
            "Valid Loss: 1.9431504011154175    Valid Acc:  0.625  at batch 1376.\n",
            "Valid Loss: 1.9435242414474487    Valid Acc:  0.53125  at batch 1377.\n",
            "Valid Loss: 1.9272226095199585    Valid Acc:  0.65625  at batch 1378.\n",
            "Valid Loss: 1.9300349950790405    Valid Acc:  0.625  at batch 1379.\n",
            "Valid Loss: 2.027127265930176    Valid Acc:  0.59375  at batch 1380.\n",
            "Valid Loss: 1.9230678081512451    Valid Acc:  0.6875  at batch 1381.\n",
            "Valid Loss: 2.0007457733154297    Valid Acc:  0.5625  at batch 1382.\n",
            "Valid Loss: 1.984494924545288    Valid Acc:  0.5625  at batch 1383.\n",
            "Valid Loss: 2.0001840591430664    Valid Acc:  0.6875  at batch 1384.\n",
            "Valid Loss: 2.0057315826416016    Valid Acc:  0.375  at batch 1385.\n",
            "Valid Loss: 1.9492785930633545    Valid Acc:  0.65625  at batch 1386.\n",
            "Valid Loss: 1.9343292713165283    Valid Acc:  0.65625  at batch 1387.\n",
            "Valid Loss: 1.9448490142822266    Valid Acc:  0.65625  at batch 1388.\n",
            "Valid Loss: 1.9677813053131104    Valid Acc:  0.5625  at batch 1389.\n",
            "Valid Loss: 1.9966953992843628    Valid Acc:  0.65625  at batch 1390.\n",
            "Valid Loss: 1.9630851745605469    Valid Acc:  0.65625  at batch 1391.\n",
            "Valid Loss: 1.9829566478729248    Valid Acc:  0.6875  at batch 1392.\n",
            "Valid Loss: 1.965537428855896    Valid Acc:  0.6875  at batch 1393.\n",
            "Valid Loss: 2.048506498336792    Valid Acc:  0.53125  at batch 1394.\n",
            "Valid Loss: 2.0028433799743652    Valid Acc:  0.6875  at batch 1395.\n",
            "Valid Loss: 1.936644434928894    Valid Acc:  0.5625  at batch 1396.\n",
            "Valid Loss: 1.9128682613372803    Valid Acc:  0.71875  at batch 1397.\n",
            "Valid Loss: 1.971318006515503    Valid Acc:  0.59375  at batch 1398.\n",
            "Valid Loss: 1.9249851703643799    Valid Acc:  0.625  at batch 1399.\n",
            "Valid Loss: 1.9576334953308105    Valid Acc:  0.625  at batch 1400.\n",
            "Valid Loss: 1.9724916219711304    Valid Acc:  0.625  at batch 1401.\n",
            "Valid Loss: 1.9554561376571655    Valid Acc:  0.65625  at batch 1402.\n",
            "Valid Loss: 2.012277364730835    Valid Acc:  0.5  at batch 1403.\n",
            "Valid Loss: 1.9618297815322876    Valid Acc:  0.71875  at batch 1404.\n",
            "Valid Loss: 2.0026869773864746    Valid Acc:  0.59375  at batch 1405.\n",
            "Valid Loss: 2.0394387245178223    Valid Acc:  0.4375  at batch 1406.\n",
            "Valid Loss: 1.9917511940002441    Valid Acc:  0.5625  at batch 1407.\n",
            "Valid Loss: 1.893498420715332    Valid Acc:  0.6875  at batch 1408.\n",
            "Valid Loss: 2.019669532775879    Valid Acc:  0.625  at batch 1409.\n",
            "Valid Loss: 1.9758095741271973    Valid Acc:  0.59375  at batch 1410.\n",
            "Valid Loss: 1.9530938863754272    Valid Acc:  0.75  at batch 1411.\n",
            "Valid Loss: 1.9458684921264648    Valid Acc:  0.59375  at batch 1412.\n",
            "Valid Loss: 1.9748550653457642    Valid Acc:  0.5  at batch 1413.\n",
            "Valid Loss: 2.002675771713257    Valid Acc:  0.71875  at batch 1414.\n",
            "Valid Loss: 2.011892557144165    Valid Acc:  0.5  at batch 1415.\n",
            "Valid Loss: 2.0299997329711914    Valid Acc:  0.46875  at batch 1416.\n",
            "Valid Loss: 1.9916017055511475    Valid Acc:  0.625  at batch 1417.\n",
            "Valid Loss: 2.014652967453003    Valid Acc:  0.5625  at batch 1418.\n",
            "Valid Loss: 2.020962953567505    Valid Acc:  0.53125  at batch 1419.\n",
            "Valid Loss: 1.9943017959594727    Valid Acc:  0.59375  at batch 1420.\n",
            "Valid Loss: 1.9911431074142456    Valid Acc:  0.59375  at batch 1421.\n",
            "Valid Loss: 2.0087833404541016    Valid Acc:  0.5  at batch 1422.\n",
            "Valid Loss: 1.9650678634643555    Valid Acc:  0.6875  at batch 1423.\n",
            "Valid Loss: 1.9360116720199585    Valid Acc:  0.625  at batch 1424.\n",
            "Valid Loss: 1.9644091129302979    Valid Acc:  0.625  at batch 1425.\n",
            "Valid Loss: 1.9636534452438354    Valid Acc:  0.65625  at batch 1426.\n",
            "Valid Loss: 1.9182977676391602    Valid Acc:  0.5625  at batch 1427.\n",
            "Valid Loss: 1.965279459953308    Valid Acc:  0.5625  at batch 1428.\n",
            "Valid Loss: 2.0026936531066895    Valid Acc:  0.5  at batch 1429.\n",
            "Valid Loss: 1.987585186958313    Valid Acc:  0.5625  at batch 1430.\n",
            "Valid Loss: 1.9356807470321655    Valid Acc:  0.6875  at batch 1431.\n",
            "Valid Loss: 1.9513412714004517    Valid Acc:  0.625  at batch 1432.\n",
            "Valid Loss: 1.9790420532226562    Valid Acc:  0.53125  at batch 1433.\n",
            "Valid Loss: 2.0137240886688232    Valid Acc:  0.53125  at batch 1434.\n",
            "Valid Loss: 2.006509304046631    Valid Acc:  0.6875  at batch 1435.\n",
            "Valid Loss: 1.9023187160491943    Valid Acc:  0.6875  at batch 1436.\n",
            "Valid Loss: 1.9676703214645386    Valid Acc:  0.6875  at batch 1437.\n",
            "Valid Loss: 1.9909155368804932    Valid Acc:  0.5  at batch 1438.\n",
            "Valid Loss: 1.9288772344589233    Valid Acc:  0.65625  at batch 1439.\n",
            "Valid Loss: 2.084211826324463    Valid Acc:  0.4375  at batch 1440.\n",
            "Valid Loss: 1.9746700525283813    Valid Acc:  0.625  at batch 1441.\n",
            "Valid Loss: 1.927695393562317    Valid Acc:  0.625  at batch 1442.\n",
            "Valid Loss: 1.9178534746170044    Valid Acc:  0.59375  at batch 1443.\n",
            "Valid Loss: 1.9312788248062134    Valid Acc:  0.65625  at batch 1444.\n",
            "Valid Loss: 2.0119519233703613    Valid Acc:  0.625  at batch 1445.\n",
            "Valid Loss: 1.9427835941314697    Valid Acc:  0.59375  at batch 1446.\n",
            "Valid Loss: 1.918327808380127    Valid Acc:  0.75  at batch 1447.\n",
            "Valid Loss: 1.9378284215927124    Valid Acc:  0.59375  at batch 1448.\n",
            "Valid Loss: 1.9927003383636475    Valid Acc:  0.53125  at batch 1449.\n",
            "Valid Loss: 2.0602097511291504    Valid Acc:  0.53125  at batch 1450.\n",
            "Valid Loss: 2.0207440853118896    Valid Acc:  0.40625  at batch 1451.\n",
            "Valid Loss: 1.953129529953003    Valid Acc:  0.65625  at batch 1452.\n",
            "Valid Loss: 1.9480305910110474    Valid Acc:  0.71875  at batch 1453.\n",
            "Valid Loss: 1.9885282516479492    Valid Acc:  0.59375  at batch 1454.\n",
            "Valid Loss: 1.9447656869888306    Valid Acc:  0.59375  at batch 1455.\n",
            "Valid Loss: 1.9465463161468506    Valid Acc:  0.6875  at batch 1456.\n",
            "Valid Loss: 2.0032973289489746    Valid Acc:  0.59375  at batch 1457.\n",
            "Valid Loss: 1.9622114896774292    Valid Acc:  0.59375  at batch 1458.\n",
            "Valid Loss: 1.9454344511032104    Valid Acc:  0.625  at batch 1459.\n",
            "Valid Loss: 1.9513120651245117    Valid Acc:  0.65625  at batch 1460.\n",
            "Valid Loss: 2.0541772842407227    Valid Acc:  0.53125  at batch 1461.\n",
            "Valid Loss: 1.9475626945495605    Valid Acc:  0.59375  at batch 1462.\n",
            "Valid Loss: 1.946807861328125    Valid Acc:  0.71875  at batch 1463.\n",
            "Valid Loss: 1.939232349395752    Valid Acc:  0.75  at batch 1464.\n",
            "Valid Loss: 2.0444252490997314    Valid Acc:  0.4375  at batch 1465.\n",
            "Valid Loss: 2.0087456703186035    Valid Acc:  0.5  at batch 1466.\n",
            "Valid Loss: 1.928290605545044    Valid Acc:  0.71875  at batch 1467.\n",
            "Valid Loss: 1.9599189758300781    Valid Acc:  0.625  at batch 1468.\n",
            "Valid Loss: 1.950438141822815    Valid Acc:  0.65625  at batch 1469.\n",
            "Valid Loss: 1.926456093788147    Valid Acc:  0.625  at batch 1470.\n",
            "Valid Loss: 2.0305049419403076    Valid Acc:  0.40625  at batch 1471.\n",
            "Valid Loss: 2.031888723373413    Valid Acc:  0.4375  at batch 1472.\n",
            "Valid Loss: 1.9189380407333374    Valid Acc:  0.6875  at batch 1473.\n",
            "Valid Loss: 1.9675647020339966    Valid Acc:  0.59375  at batch 1474.\n",
            "Valid Loss: 2.058436632156372    Valid Acc:  0.34375  at batch 1475.\n",
            "Valid Loss: 1.9775335788726807    Valid Acc:  0.625  at batch 1476.\n",
            "Valid Loss: 1.9220491647720337    Valid Acc:  0.71875  at batch 1477.\n",
            "Valid Loss: 1.9629528522491455    Valid Acc:  0.625  at batch 1478.\n",
            "Valid Loss: 1.9241461753845215    Valid Acc:  0.625  at batch 1479.\n",
            "Valid Loss: 1.9308106899261475    Valid Acc:  0.71875  at batch 1480.\n",
            "Valid Loss: 1.965955376625061    Valid Acc:  0.5625  at batch 1481.\n",
            "Valid Loss: 1.9945534467697144    Valid Acc:  0.65625  at batch 1482.\n",
            "Valid Loss: 1.9659367799758911    Valid Acc:  0.65625  at batch 1483.\n",
            "Valid Loss: 2.001149892807007    Valid Acc:  0.53125  at batch 1484.\n",
            "Valid Loss: 1.9850077629089355    Valid Acc:  0.59375  at batch 1485.\n",
            "Valid Loss: 1.9325438737869263    Valid Acc:  0.625  at batch 1486.\n",
            "Valid Loss: 2.0008535385131836    Valid Acc:  0.59375  at batch 1487.\n",
            "Valid Loss: 1.9465566873550415    Valid Acc:  0.625  at batch 1488.\n",
            "Valid Loss: 2.0098659992218018    Valid Acc:  0.4375  at batch 1489.\n",
            "Valid Loss: 2.0078999996185303    Valid Acc:  0.5625  at batch 1490.\n",
            "Valid Loss: 1.9407871961593628    Valid Acc:  0.625  at batch 1491.\n",
            "Valid Loss: 2.0104990005493164    Valid Acc:  0.65625  at batch 1492.\n",
            "Valid Loss: 1.9719926118850708    Valid Acc:  0.5625  at batch 1493.\n",
            "Valid Loss: 1.918228030204773    Valid Acc:  0.6875  at batch 1494.\n",
            "Valid Loss: 1.935397982597351    Valid Acc:  0.65625  at batch 1495.\n",
            "Valid Loss: 1.989139437675476    Valid Acc:  0.75  at batch 1496.\n",
            "Valid Loss: 2.0140984058380127    Valid Acc:  0.5625  at batch 1497.\n",
            "Valid Loss: 1.9104701280593872    Valid Acc:  0.6875  at batch 1498.\n",
            "Valid Loss: 2.0421504974365234    Valid Acc:  0.5  at batch 1499.\n",
            "Valid Loss: 1.9612858295440674    Valid Acc:  0.59375  at batch 1500.\n",
            "Valid Loss: 1.9340646266937256    Valid Acc:  0.5625  at batch 1501.\n",
            "Valid Loss: 1.9975991249084473    Valid Acc:  0.53125  at batch 1502.\n",
            "Valid Loss: 1.9885509014129639    Valid Acc:  0.59375  at batch 1503.\n",
            "Valid Loss: 2.0077874660491943    Valid Acc:  0.5625  at batch 1504.\n",
            "Valid Loss: 2.003974199295044    Valid Acc:  0.65625  at batch 1505.\n",
            "Valid Loss: 1.8685266971588135    Valid Acc:  0.84375  at batch 1506.\n",
            "Valid Loss: 1.983208179473877    Valid Acc:  0.5  at batch 1507.\n",
            "Valid Loss: 1.9557840824127197    Valid Acc:  0.65625  at batch 1508.\n",
            "Valid Loss: 1.969688057899475    Valid Acc:  0.65625  at batch 1509.\n",
            "Valid Loss: 1.984147310256958    Valid Acc:  0.625  at batch 1510.\n",
            "Valid Loss: 2.0129921436309814    Valid Acc:  0.59375  at batch 1511.\n",
            "Valid Loss: 2.0123865604400635    Valid Acc:  0.5  at batch 1512.\n",
            "Valid Loss: 1.9800066947937012    Valid Acc:  0.625  at batch 1513.\n",
            "Valid Loss: 1.9192702770233154    Valid Acc:  0.71875  at batch 1514.\n",
            "Valid Loss: 1.9487894773483276    Valid Acc:  0.75  at batch 1515.\n",
            "Valid Loss: 1.9459469318389893    Valid Acc:  0.625  at batch 1516.\n",
            "Valid Loss: 1.9677083492279053    Valid Acc:  0.75  at batch 1517.\n",
            "Valid Loss: 1.9174293279647827    Valid Acc:  0.625  at batch 1518.\n",
            "Valid Loss: 2.0442142486572266    Valid Acc:  0.5625  at batch 1519.\n",
            "Valid Loss: 1.9484825134277344    Valid Acc:  0.53125  at batch 1520.\n",
            "Valid Loss: 2.018099784851074    Valid Acc:  0.5625  at batch 1521.\n",
            "Valid Loss: 1.9141989946365356    Valid Acc:  0.59375  at batch 1522.\n",
            "Valid Loss: 1.965240240097046    Valid Acc:  0.78125  at batch 1523.\n",
            "Valid Loss: 1.9690697193145752    Valid Acc:  0.625  at batch 1524.\n",
            "Valid Loss: 1.9735690355300903    Valid Acc:  0.625  at batch 1525.\n",
            "Valid Loss: 1.9986692667007446    Valid Acc:  0.65625  at batch 1526.\n",
            "Valid Loss: 1.8886009454727173    Valid Acc:  0.78125  at batch 1527.\n",
            "Valid Loss: 2.0135889053344727    Valid Acc:  0.4375  at batch 1528.\n",
            "Valid Loss: 1.9966932535171509    Valid Acc:  0.59375  at batch 1529.\n",
            "Valid Loss: 1.9243934154510498    Valid Acc:  0.59375  at batch 1530.\n",
            "Valid Loss: 1.9394105672836304    Valid Acc:  0.625  at batch 1531.\n",
            "Valid Loss: 1.9332690238952637    Valid Acc:  0.65625  at batch 1532.\n",
            "Valid Loss: 1.933400273323059    Valid Acc:  0.71875  at batch 1533.\n",
            "Valid Loss: 1.9906312227249146    Valid Acc:  0.53125  at batch 1534.\n",
            "Valid Loss: 1.9951695203781128    Valid Acc:  0.625  at batch 1535.\n",
            "Valid Loss: 1.9895657300949097    Valid Acc:  0.5625  at batch 1536.\n",
            "Valid Loss: 1.9025670289993286    Valid Acc:  0.75  at batch 1537.\n",
            "Valid Loss: 1.970160961151123    Valid Acc:  0.5625  at batch 1538.\n",
            "Valid Loss: 1.9288318157196045    Valid Acc:  0.625  at batch 1539.\n",
            "Valid Loss: 2.0300400257110596    Valid Acc:  0.46875  at batch 1540.\n",
            "Valid Loss: 1.9321030378341675    Valid Acc:  0.65625  at batch 1541.\n",
            "Valid Loss: 2.038804292678833    Valid Acc:  0.5  at batch 1542.\n",
            "Valid Loss: 1.939204216003418    Valid Acc:  0.6875  at batch 1543.\n",
            "Valid Loss: 2.0581932067871094    Valid Acc:  0.5  at batch 1544.\n",
            "Valid Loss: 1.888100028038025    Valid Acc:  0.78125  at batch 1545.\n",
            "Valid Loss: 1.997589349746704    Valid Acc:  0.5625  at batch 1546.\n",
            "Valid Loss: 1.9652315378189087    Valid Acc:  0.625  at batch 1547.\n",
            "Valid Loss: 1.9475412368774414    Valid Acc:  0.6875  at batch 1548.\n",
            "Valid Loss: 1.8812451362609863    Valid Acc:  0.71875  at batch 1549.\n",
            "Valid Loss: 2.005303144454956    Valid Acc:  0.4375  at batch 1550.\n",
            "Valid Loss: 1.9903523921966553    Valid Acc:  0.59375  at batch 1551.\n",
            "Valid Loss: 1.969350814819336    Valid Acc:  0.625  at batch 1552.\n",
            "Valid Loss: 1.9579490423202515    Valid Acc:  0.59375  at batch 1553.\n",
            "Valid Loss: 1.9177136421203613    Valid Acc:  0.6875  at batch 1554.\n",
            "Valid Loss: 1.959118366241455    Valid Acc:  0.5625  at batch 1555.\n",
            "Valid Loss: 1.9370136260986328    Valid Acc:  0.75  at batch 1556.\n",
            "Valid Loss: 1.9101307392120361    Valid Acc:  0.6875  at batch 1557.\n",
            "Valid Loss: 1.9932767152786255    Valid Acc:  0.53125  at batch 1558.\n",
            "Valid Loss: 1.9363105297088623    Valid Acc:  0.75  at batch 1559.\n",
            "Valid Loss: 1.961654782295227    Valid Acc:  0.5625  at batch 1560.\n",
            "Valid Loss: 1.984830379486084    Valid Acc:  0.59375  at batch 1561.\n",
            "Valid Loss: 1.9092730283737183    Valid Acc:  0.75  at batch 1562.\n",
            "\n",
            "\n",
            "EPOCH 1\n",
            "\n",
            "\n",
            "Train Loss: 1.9673389196395874    Train Acc:  0.625  at batch 0.\n",
            "Train Loss: 1.9258265495300293    Train Acc:  0.6875  at batch 1.\n",
            "Train Loss: 1.9550631046295166    Train Acc:  0.53125  at batch 2.\n",
            "Train Loss: 1.9529515504837036    Train Acc:  0.5625  at batch 3.\n",
            "Train Loss: 1.9257068634033203    Train Acc:  0.640625  at batch 4.\n",
            "Train Loss: 1.943010687828064    Train Acc:  0.546875  at batch 5.\n",
            "Train Loss: 1.9568283557891846    Train Acc:  0.40625  at batch 6.\n",
            "Train Loss: 1.9767532348632812    Train Acc:  0.46875  at batch 7.\n",
            "Train Loss: 1.9956847429275513    Train Acc:  0.578125  at batch 8.\n",
            "Train Loss: 1.990587592124939    Train Acc:  0.484375  at batch 9.\n",
            "Train Loss: 1.9420318603515625    Train Acc:  0.609375  at batch 10.\n",
            "Train Loss: 1.9416710138320923    Train Acc:  0.59375  at batch 11.\n",
            "Train Loss: 1.9882851839065552    Train Acc:  0.53125  at batch 12.\n",
            "Train Loss: 1.9850776195526123    Train Acc:  0.609375  at batch 13.\n",
            "Train Loss: 1.9808052778244019    Train Acc:  0.53125  at batch 14.\n",
            "Train Loss: 1.9833933115005493    Train Acc:  0.578125  at batch 15.\n",
            "Train Loss: 1.977743148803711    Train Acc:  0.5625  at batch 16.\n",
            "Train Loss: 1.941973090171814    Train Acc:  0.65625  at batch 17.\n",
            "Train Loss: 1.9303090572357178    Train Acc:  0.703125  at batch 18.\n",
            "Train Loss: 1.8788636922836304    Train Acc:  0.625  at batch 19.\n",
            "Train Loss: 1.9768952131271362    Train Acc:  0.453125  at batch 20.\n",
            "Train Loss: 1.9560832977294922    Train Acc:  0.546875  at batch 21.\n",
            "Train Loss: 1.9496374130249023    Train Acc:  0.546875  at batch 22.\n",
            "Train Loss: 1.9630860090255737    Train Acc:  0.546875  at batch 23.\n",
            "Train Loss: 1.975382924079895    Train Acc:  0.578125  at batch 24.\n",
            "Train Loss: 1.9153262376785278    Train Acc:  0.59375  at batch 25.\n",
            "Train Loss: 1.8969237804412842    Train Acc:  0.703125  at batch 26.\n",
            "Train Loss: 1.884792685508728    Train Acc:  0.640625  at batch 27.\n",
            "Train Loss: 2.0207550525665283    Train Acc:  0.40625  at batch 28.\n",
            "Train Loss: 1.904778003692627    Train Acc:  0.671875  at batch 29.\n",
            "Train Loss: 1.9115070104599    Train Acc:  0.5625  at batch 30.\n",
            "Train Loss: 2.004465341567993    Train Acc:  0.390625  at batch 31.\n",
            "Train Loss: 1.9392918348312378    Train Acc:  0.703125  at batch 32.\n",
            "Train Loss: 1.9711487293243408    Train Acc:  0.578125  at batch 33.\n",
            "Train Loss: 1.9601621627807617    Train Acc:  0.53125  at batch 34.\n",
            "Train Loss: 1.9174162149429321    Train Acc:  0.671875  at batch 35.\n",
            "Train Loss: 1.9282373189926147    Train Acc:  0.59375  at batch 36.\n",
            "Train Loss: 1.9340145587921143    Train Acc:  0.640625  at batch 37.\n",
            "Train Loss: 1.9813002347946167    Train Acc:  0.609375  at batch 38.\n",
            "Train Loss: 1.9253665208816528    Train Acc:  0.703125  at batch 39.\n",
            "Train Loss: 1.9079129695892334    Train Acc:  0.75  at batch 40.\n",
            "Train Loss: 1.9651726484298706    Train Acc:  0.609375  at batch 41.\n",
            "Train Loss: 1.8994252681732178    Train Acc:  0.59375  at batch 42.\n",
            "Train Loss: 1.9566274881362915    Train Acc:  0.578125  at batch 43.\n",
            "Train Loss: 1.9787771701812744    Train Acc:  0.5  at batch 44.\n",
            "Train Loss: 2.0001614093780518    Train Acc:  0.5  at batch 45.\n",
            "Train Loss: 1.9084175825119019    Train Acc:  0.59375  at batch 46.\n",
            "Train Loss: 1.8993598222732544    Train Acc:  0.734375  at batch 47.\n",
            "Train Loss: 1.9186937808990479    Train Acc:  0.671875  at batch 48.\n",
            "Train Loss: 1.9420944452285767    Train Acc:  0.703125  at batch 49.\n",
            "Train Loss: 1.9555027484893799    Train Acc:  0.640625  at batch 50.\n",
            "Train Loss: 1.9461603164672852    Train Acc:  0.59375  at batch 51.\n",
            "Train Loss: 1.916909098625183    Train Acc:  0.703125  at batch 52.\n",
            "Train Loss: 1.9146760702133179    Train Acc:  0.609375  at batch 53.\n",
            "Train Loss: 1.905633568763733    Train Acc:  0.640625  at batch 54.\n",
            "Train Loss: 1.9336024522781372    Train Acc:  0.59375  at batch 55.\n",
            "Train Loss: 1.9208953380584717    Train Acc:  0.65625  at batch 56.\n",
            "Train Loss: 1.9396719932556152    Train Acc:  0.578125  at batch 57.\n",
            "Train Loss: 1.924382209777832    Train Acc:  0.625  at batch 58.\n",
            "Train Loss: 1.9105669260025024    Train Acc:  0.6875  at batch 59.\n",
            "Train Loss: 1.9409335851669312    Train Acc:  0.5  at batch 60.\n",
            "Train Loss: 1.9111167192459106    Train Acc:  0.59375  at batch 61.\n",
            "Train Loss: 1.9222451448440552    Train Acc:  0.53125  at batch 62.\n",
            "Train Loss: 1.989492654800415    Train Acc:  0.53125  at batch 63.\n",
            "Train Loss: 1.91547691822052    Train Acc:  0.578125  at batch 64.\n",
            "Train Loss: 1.9525092840194702    Train Acc:  0.59375  at batch 65.\n",
            "Train Loss: 1.920133352279663    Train Acc:  0.609375  at batch 66.\n",
            "Train Loss: 1.9826481342315674    Train Acc:  0.5625  at batch 67.\n",
            "Train Loss: 1.9738978147506714    Train Acc:  0.578125  at batch 68.\n",
            "Train Loss: 1.9223350286483765    Train Acc:  0.75  at batch 69.\n",
            "Train Loss: 1.9157847166061401    Train Acc:  0.640625  at batch 70.\n",
            "Train Loss: 1.9218121767044067    Train Acc:  0.65625  at batch 71.\n",
            "Train Loss: 1.878963589668274    Train Acc:  0.6875  at batch 72.\n",
            "Train Loss: 1.9180827140808105    Train Acc:  0.578125  at batch 73.\n",
            "Train Loss: 1.932849407196045    Train Acc:  0.578125  at batch 74.\n",
            "Train Loss: 1.9777350425720215    Train Acc:  0.5625  at batch 75.\n",
            "Train Loss: 1.9191246032714844    Train Acc:  0.671875  at batch 76.\n",
            "Train Loss: 1.937333345413208    Train Acc:  0.640625  at batch 77.\n",
            "Train Loss: 1.9678574800491333    Train Acc:  0.609375  at batch 78.\n",
            "Train Loss: 1.943979024887085    Train Acc:  0.65625  at batch 79.\n",
            "Train Loss: 1.9591753482818604    Train Acc:  0.609375  at batch 80.\n",
            "Train Loss: 1.935920238494873    Train Acc:  0.609375  at batch 81.\n",
            "Train Loss: 1.9703383445739746    Train Acc:  0.625  at batch 82.\n",
            "Train Loss: 1.914921522140503    Train Acc:  0.6875  at batch 83.\n",
            "Train Loss: 1.9125241041183472    Train Acc:  0.734375  at batch 84.\n",
            "Train Loss: 1.9301180839538574    Train Acc:  0.703125  at batch 85.\n",
            "Train Loss: 1.880407452583313    Train Acc:  0.6875  at batch 86.\n",
            "Train Loss: 1.8927080631256104    Train Acc:  0.78125  at batch 87.\n",
            "Train Loss: 1.9349229335784912    Train Acc:  0.625  at batch 88.\n",
            "Train Loss: 1.897727608680725    Train Acc:  0.703125  at batch 89.\n",
            "Train Loss: 1.8763378858566284    Train Acc:  0.6875  at batch 90.\n",
            "Train Loss: 1.9864426851272583    Train Acc:  0.546875  at batch 91.\n",
            "Train Loss: 1.898113489151001    Train Acc:  0.609375  at batch 92.\n",
            "Train Loss: 1.8755438327789307    Train Acc:  0.671875  at batch 93.\n",
            "Train Loss: 1.8995702266693115    Train Acc:  0.65625  at batch 94.\n",
            "Train Loss: 1.9006860256195068    Train Acc:  0.625  at batch 95.\n",
            "Train Loss: 1.8855602741241455    Train Acc:  0.71875  at batch 96.\n",
            "Train Loss: 1.915082335472107    Train Acc:  0.609375  at batch 97.\n",
            "Train Loss: 1.9913350343704224    Train Acc:  0.484375  at batch 98.\n",
            "Train Loss: 1.8965188264846802    Train Acc:  0.71875  at batch 99.\n",
            "Train Loss: 1.8772566318511963    Train Acc:  0.734375  at batch 100.\n",
            "Train Loss: 1.898596167564392    Train Acc:  0.625  at batch 101.\n",
            "Train Loss: 1.9276456832885742    Train Acc:  0.609375  at batch 102.\n",
            "Train Loss: 1.8959230184555054    Train Acc:  0.578125  at batch 103.\n",
            "Train Loss: 1.858235239982605    Train Acc:  0.65625  at batch 104.\n",
            "Train Loss: 1.9201267957687378    Train Acc:  0.578125  at batch 105.\n",
            "Train Loss: 1.9160864353179932    Train Acc:  0.625  at batch 106.\n",
            "Train Loss: 1.8715145587921143    Train Acc:  0.625  at batch 107.\n",
            "Train Loss: 1.9377379417419434    Train Acc:  0.515625  at batch 108.\n",
            "Train Loss: 1.858555793762207    Train Acc:  0.671875  at batch 109.\n",
            "Train Loss: 1.8709288835525513    Train Acc:  0.625  at batch 110.\n",
            "Train Loss: 1.9137394428253174    Train Acc:  0.609375  at batch 111.\n",
            "Train Loss: 1.8634065389633179    Train Acc:  0.6875  at batch 112.\n",
            "Train Loss: 1.8480075597763062    Train Acc:  0.734375  at batch 113.\n",
            "Train Loss: 1.8132355213165283    Train Acc:  0.765625  at batch 114.\n",
            "Train Loss: 1.9401580095291138    Train Acc:  0.625  at batch 115.\n",
            "Train Loss: 1.9421058893203735    Train Acc:  0.5  at batch 116.\n",
            "Train Loss: 1.862021803855896    Train Acc:  0.703125  at batch 117.\n",
            "Train Loss: 1.9560785293579102    Train Acc:  0.53125  at batch 118.\n",
            "Train Loss: 1.8694851398468018    Train Acc:  0.703125  at batch 119.\n",
            "Train Loss: 1.875630497932434    Train Acc:  0.6875  at batch 120.\n",
            "Train Loss: 1.8696271181106567    Train Acc:  0.640625  at batch 121.\n",
            "Train Loss: 1.9343690872192383    Train Acc:  0.59375  at batch 122.\n",
            "Train Loss: 1.8961453437805176    Train Acc:  0.640625  at batch 123.\n",
            "Train Loss: 1.8576695919036865    Train Acc:  0.65625  at batch 124.\n",
            "Train Loss: 1.9862382411956787    Train Acc:  0.484375  at batch 125.\n",
            "Train Loss: 1.933049201965332    Train Acc:  0.625  at batch 126.\n",
            "Train Loss: 1.9367924928665161    Train Acc:  0.609375  at batch 127.\n",
            "Train Loss: 1.8976625204086304    Train Acc:  0.6875  at batch 128.\n",
            "Train Loss: 1.8856563568115234    Train Acc:  0.609375  at batch 129.\n",
            "Train Loss: 1.9000775814056396    Train Acc:  0.71875  at batch 130.\n",
            "Train Loss: 1.8436074256896973    Train Acc:  0.640625  at batch 131.\n",
            "Train Loss: 1.847886562347412    Train Acc:  0.640625  at batch 132.\n",
            "Train Loss: 1.8726561069488525    Train Acc:  0.65625  at batch 133.\n",
            "Train Loss: 1.9628260135650635    Train Acc:  0.46875  at batch 134.\n",
            "Train Loss: 1.8369078636169434    Train Acc:  0.625  at batch 135.\n",
            "Train Loss: 1.8511470556259155    Train Acc:  0.734375  at batch 136.\n",
            "Train Loss: 1.9234569072723389    Train Acc:  0.5  at batch 137.\n",
            "Train Loss: 1.9025280475616455    Train Acc:  0.546875  at batch 138.\n",
            "Train Loss: 1.9523721933364868    Train Acc:  0.5  at batch 139.\n",
            "Train Loss: 1.851186990737915    Train Acc:  0.71875  at batch 140.\n",
            "Train Loss: 1.8418097496032715    Train Acc:  0.65625  at batch 141.\n",
            "Train Loss: 1.846940040588379    Train Acc:  0.65625  at batch 142.\n",
            "Train Loss: 1.8689597845077515    Train Acc:  0.5625  at batch 143.\n",
            "Train Loss: 1.8704828023910522    Train Acc:  0.65625  at batch 144.\n",
            "Train Loss: 1.8804128170013428    Train Acc:  0.65625  at batch 145.\n",
            "Train Loss: 1.82965087890625    Train Acc:  0.71875  at batch 146.\n",
            "Train Loss: 1.875014305114746    Train Acc:  0.671875  at batch 147.\n",
            "Train Loss: 1.7906361818313599    Train Acc:  0.625  at batch 148.\n",
            "Train Loss: 1.8794407844543457    Train Acc:  0.6875  at batch 149.\n",
            "Train Loss: 1.8629295825958252    Train Acc:  0.640625  at batch 150.\n",
            "Train Loss: 1.9095120429992676    Train Acc:  0.5625  at batch 151.\n",
            "Train Loss: 1.8933875560760498    Train Acc:  0.65625  at batch 152.\n",
            "Train Loss: 1.823438286781311    Train Acc:  0.765625  at batch 153.\n",
            "Train Loss: 1.8508309125900269    Train Acc:  0.609375  at batch 154.\n",
            "Train Loss: 1.8459205627441406    Train Acc:  0.609375  at batch 155.\n",
            "Train Loss: 1.9180610179901123    Train Acc:  0.609375  at batch 156.\n",
            "Train Loss: 1.8946141004562378    Train Acc:  0.578125  at batch 157.\n",
            "Train Loss: 1.82710862159729    Train Acc:  0.75  at batch 158.\n",
            "Train Loss: 1.9395748376846313    Train Acc:  0.546875  at batch 159.\n",
            "Train Loss: 1.8449658155441284    Train Acc:  0.6875  at batch 160.\n",
            "Train Loss: 1.7680089473724365    Train Acc:  0.796875  at batch 161.\n",
            "Train Loss: 1.8639675378799438    Train Acc:  0.671875  at batch 162.\n",
            "Train Loss: 1.8265022039413452    Train Acc:  0.734375  at batch 163.\n",
            "Train Loss: 1.7951806783676147    Train Acc:  0.640625  at batch 164.\n",
            "Train Loss: 1.8737692832946777    Train Acc:  0.6875  at batch 165.\n",
            "Train Loss: 1.9027830362319946    Train Acc:  0.578125  at batch 166.\n",
            "Train Loss: 1.8926637172698975    Train Acc:  0.609375  at batch 167.\n",
            "Train Loss: 1.8794844150543213    Train Acc:  0.59375  at batch 168.\n",
            "Train Loss: 1.786794900894165    Train Acc:  0.78125  at batch 169.\n",
            "Train Loss: 1.90889310836792    Train Acc:  0.609375  at batch 170.\n",
            "Train Loss: 1.8869608640670776    Train Acc:  0.609375  at batch 171.\n",
            "Train Loss: 1.947701096534729    Train Acc:  0.609375  at batch 172.\n",
            "Train Loss: 1.8595472574234009    Train Acc:  0.71875  at batch 173.\n",
            "Train Loss: 1.8547505140304565    Train Acc:  0.625  at batch 174.\n",
            "Train Loss: 1.8157342672348022    Train Acc:  0.625  at batch 175.\n",
            "Train Loss: 1.822919249534607    Train Acc:  0.671875  at batch 176.\n",
            "Train Loss: 1.820884108543396    Train Acc:  0.6875  at batch 177.\n",
            "Train Loss: 1.858825922012329    Train Acc:  0.671875  at batch 178.\n",
            "Train Loss: 1.8357677459716797    Train Acc:  0.671875  at batch 179.\n",
            "Train Loss: 1.865173101425171    Train Acc:  0.734375  at batch 180.\n",
            "Train Loss: 1.7630103826522827    Train Acc:  0.765625  at batch 181.\n",
            "Train Loss: 1.8459564447402954    Train Acc:  0.671875  at batch 182.\n",
            "Train Loss: 1.8581815958023071    Train Acc:  0.640625  at batch 183.\n",
            "Train Loss: 1.89686918258667    Train Acc:  0.578125  at batch 184.\n",
            "Train Loss: 1.9174689054489136    Train Acc:  0.59375  at batch 185.\n",
            "Train Loss: 1.8591409921646118    Train Acc:  0.609375  at batch 186.\n",
            "Train Loss: 1.8753645420074463    Train Acc:  0.6875  at batch 187.\n",
            "Train Loss: 1.816727638244629    Train Acc:  0.765625  at batch 188.\n",
            "Train Loss: 1.8629451990127563    Train Acc:  0.71875  at batch 189.\n",
            "Train Loss: 1.8243356943130493    Train Acc:  0.71875  at batch 190.\n",
            "Train Loss: 1.796316385269165    Train Acc:  0.765625  at batch 191.\n",
            "Train Loss: 1.8609157800674438    Train Acc:  0.609375  at batch 192.\n",
            "Train Loss: 1.8343392610549927    Train Acc:  0.78125  at batch 193.\n",
            "Train Loss: 1.8189318180084229    Train Acc:  0.71875  at batch 194.\n",
            "Train Loss: 1.800392508506775    Train Acc:  0.6875  at batch 195.\n",
            "Train Loss: 1.8425357341766357    Train Acc:  0.625  at batch 196.\n",
            "Train Loss: 1.8443495035171509    Train Acc:  0.65625  at batch 197.\n",
            "Train Loss: 1.8410158157348633    Train Acc:  0.625  at batch 198.\n",
            "Train Loss: 1.7859734296798706    Train Acc:  0.640625  at batch 199.\n",
            "Train Loss: 1.7989450693130493    Train Acc:  0.65625  at batch 200.\n",
            "Train Loss: 1.7296321392059326    Train Acc:  0.703125  at batch 201.\n",
            "Train Loss: 1.785056233406067    Train Acc:  0.6875  at batch 202.\n",
            "Train Loss: 1.8552241325378418    Train Acc:  0.609375  at batch 203.\n",
            "Train Loss: 1.8359678983688354    Train Acc:  0.640625  at batch 204.\n",
            "Train Loss: 1.7341995239257812    Train Acc:  0.75  at batch 205.\n",
            "Train Loss: 1.8299850225448608    Train Acc:  0.59375  at batch 206.\n",
            "Train Loss: 1.7675527334213257    Train Acc:  0.703125  at batch 207.\n",
            "Train Loss: 1.8268733024597168    Train Acc:  0.671875  at batch 208.\n",
            "Train Loss: 1.8164393901824951    Train Acc:  0.703125  at batch 209.\n",
            "Train Loss: 1.7612345218658447    Train Acc:  0.6875  at batch 210.\n",
            "Train Loss: 1.844032645225525    Train Acc:  0.578125  at batch 211.\n",
            "Train Loss: 1.7864134311676025    Train Acc:  0.625  at batch 212.\n",
            "Train Loss: 1.8734887838363647    Train Acc:  0.5625  at batch 213.\n",
            "Train Loss: 1.8337970972061157    Train Acc:  0.609375  at batch 214.\n",
            "Train Loss: 1.8050816059112549    Train Acc:  0.625  at batch 215.\n",
            "Train Loss: 1.7384514808654785    Train Acc:  0.71875  at batch 216.\n",
            "Train Loss: 1.882789134979248    Train Acc:  0.5  at batch 217.\n",
            "Train Loss: 1.8163622617721558    Train Acc:  0.625  at batch 218.\n",
            "Train Loss: 1.7736704349517822    Train Acc:  0.6875  at batch 219.\n",
            "Train Loss: 1.788845181465149    Train Acc:  0.671875  at batch 220.\n",
            "Train Loss: 1.806462287902832    Train Acc:  0.640625  at batch 221.\n",
            "Train Loss: 1.7992273569107056    Train Acc:  0.671875  at batch 222.\n",
            "Train Loss: 1.8539321422576904    Train Acc:  0.5625  at batch 223.\n",
            "Train Loss: 1.8450933694839478    Train Acc:  0.609375  at batch 224.\n",
            "Train Loss: 1.8202508687973022    Train Acc:  0.671875  at batch 225.\n",
            "Train Loss: 1.835170865058899    Train Acc:  0.5625  at batch 226.\n",
            "Train Loss: 1.8209435939788818    Train Acc:  0.625  at batch 227.\n",
            "Train Loss: 1.8399633169174194    Train Acc:  0.59375  at batch 228.\n",
            "Train Loss: 1.8797721862792969    Train Acc:  0.609375  at batch 229.\n",
            "Train Loss: 1.7994418144226074    Train Acc:  0.65625  at batch 230.\n",
            "Train Loss: 1.8006519079208374    Train Acc:  0.71875  at batch 231.\n",
            "Train Loss: 1.7658666372299194    Train Acc:  0.6875  at batch 232.\n",
            "Train Loss: 1.7931989431381226    Train Acc:  0.59375  at batch 233.\n",
            "Train Loss: 1.8196669816970825    Train Acc:  0.625  at batch 234.\n",
            "Train Loss: 1.7550089359283447    Train Acc:  0.65625  at batch 235.\n",
            "Train Loss: 1.8317627906799316    Train Acc:  0.5625  at batch 236.\n",
            "Train Loss: 1.7928203344345093    Train Acc:  0.59375  at batch 237.\n",
            "Train Loss: 1.7584974765777588    Train Acc:  0.71875  at batch 238.\n",
            "Train Loss: 1.7882487773895264    Train Acc:  0.578125  at batch 239.\n",
            "Train Loss: 1.8863611221313477    Train Acc:  0.578125  at batch 240.\n",
            "Train Loss: 1.8332035541534424    Train Acc:  0.65625  at batch 241.\n",
            "Train Loss: 1.7939807176589966    Train Acc:  0.671875  at batch 242.\n",
            "Train Loss: 1.8031117916107178    Train Acc:  0.71875  at batch 243.\n",
            "Train Loss: 1.8507689237594604    Train Acc:  0.6875  at batch 244.\n",
            "Train Loss: 1.8465681076049805    Train Acc:  0.609375  at batch 245.\n",
            "Train Loss: 1.8408889770507812    Train Acc:  0.6875  at batch 246.\n",
            "Train Loss: 1.8199114799499512    Train Acc:  0.65625  at batch 247.\n",
            "Train Loss: 1.8108327388763428    Train Acc:  0.75  at batch 248.\n",
            "Train Loss: 1.7338706254959106    Train Acc:  0.71875  at batch 249.\n",
            "Train Loss: 1.7902387380599976    Train Acc:  0.65625  at batch 250.\n",
            "Train Loss: 1.7349680662155151    Train Acc:  0.734375  at batch 251.\n",
            "Train Loss: 1.8795137405395508    Train Acc:  0.53125  at batch 252.\n",
            "Train Loss: 1.8584485054016113    Train Acc:  0.578125  at batch 253.\n",
            "Train Loss: 1.7669434547424316    Train Acc:  0.640625  at batch 254.\n",
            "Train Loss: 1.7938441038131714    Train Acc:  0.609375  at batch 255.\n",
            "Train Loss: 1.8248035907745361    Train Acc:  0.578125  at batch 256.\n",
            "Train Loss: 1.8359591960906982    Train Acc:  0.71875  at batch 257.\n",
            "Train Loss: 1.8166930675506592    Train Acc:  0.6875  at batch 258.\n",
            "Train Loss: 1.8249262571334839    Train Acc:  0.59375  at batch 259.\n",
            "Train Loss: 1.7510000467300415    Train Acc:  0.625  at batch 260.\n",
            "Train Loss: 1.8887944221496582    Train Acc:  0.5625  at batch 261.\n",
            "Train Loss: 1.7504141330718994    Train Acc:  0.734375  at batch 262.\n",
            "Train Loss: 1.8374062776565552    Train Acc:  0.59375  at batch 263.\n",
            "Train Loss: 1.8084368705749512    Train Acc:  0.578125  at batch 264.\n",
            "Train Loss: 1.7985013723373413    Train Acc:  0.640625  at batch 265.\n",
            "Train Loss: 1.7864351272583008    Train Acc:  0.765625  at batch 266.\n",
            "Train Loss: 1.7817476987838745    Train Acc:  0.640625  at batch 267.\n",
            "Train Loss: 1.7663941383361816    Train Acc:  0.734375  at batch 268.\n",
            "Train Loss: 1.7885674238204956    Train Acc:  0.671875  at batch 269.\n",
            "Train Loss: 1.828059434890747    Train Acc:  0.71875  at batch 270.\n",
            "Train Loss: 1.845568299293518    Train Acc:  0.625  at batch 271.\n",
            "Train Loss: 1.8179681301116943    Train Acc:  0.6875  at batch 272.\n",
            "Train Loss: 1.7243905067443848    Train Acc:  0.703125  at batch 273.\n",
            "Train Loss: 1.7014997005462646    Train Acc:  0.71875  at batch 274.\n",
            "Train Loss: 1.7954193353652954    Train Acc:  0.546875  at batch 275.\n",
            "Train Loss: 1.800994634628296    Train Acc:  0.59375  at batch 276.\n",
            "Train Loss: 1.7946972846984863    Train Acc:  0.609375  at batch 277.\n",
            "Train Loss: 1.7832196950912476    Train Acc:  0.765625  at batch 278.\n",
            "Train Loss: 1.8344906568527222    Train Acc:  0.5625  at batch 279.\n",
            "Train Loss: 1.7437070608139038    Train Acc:  0.6875  at batch 280.\n",
            "Train Loss: 1.8046573400497437    Train Acc:  0.59375  at batch 281.\n",
            "Train Loss: 1.8117022514343262    Train Acc:  0.546875  at batch 282.\n",
            "Train Loss: 1.6423687934875488    Train Acc:  0.796875  at batch 283.\n",
            "Train Loss: 1.8317631483078003    Train Acc:  0.609375  at batch 284.\n",
            "Train Loss: 1.7913095951080322    Train Acc:  0.59375  at batch 285.\n",
            "Train Loss: 1.8221908807754517    Train Acc:  0.65625  at batch 286.\n",
            "Train Loss: 1.70148766040802    Train Acc:  0.75  at batch 287.\n",
            "Train Loss: 1.737128496170044    Train Acc:  0.6875  at batch 288.\n",
            "Train Loss: 1.6615676879882812    Train Acc:  0.765625  at batch 289.\n",
            "Train Loss: 1.699245572090149    Train Acc:  0.671875  at batch 290.\n",
            "Train Loss: 1.853882908821106    Train Acc:  0.5625  at batch 291.\n",
            "Train Loss: 1.741381287574768    Train Acc:  0.671875  at batch 292.\n",
            "Train Loss: 1.7809808254241943    Train Acc:  0.625  at batch 293.\n",
            "Train Loss: 1.7324095964431763    Train Acc:  0.734375  at batch 294.\n",
            "Train Loss: 1.7677407264709473    Train Acc:  0.609375  at batch 295.\n",
            "Train Loss: 1.7784408330917358    Train Acc:  0.609375  at batch 296.\n",
            "Train Loss: 1.7864680290222168    Train Acc:  0.671875  at batch 297.\n",
            "Train Loss: 1.7606959342956543    Train Acc:  0.703125  at batch 298.\n",
            "Train Loss: 1.7797937393188477    Train Acc:  0.65625  at batch 299.\n",
            "Train Loss: 1.818023920059204    Train Acc:  0.59375  at batch 300.\n",
            "Train Loss: 1.741182804107666    Train Acc:  0.75  at batch 301.\n",
            "Train Loss: 1.8321974277496338    Train Acc:  0.5625  at batch 302.\n",
            "Train Loss: 1.7965412139892578    Train Acc:  0.640625  at batch 303.\n",
            "Train Loss: 1.7916570901870728    Train Acc:  0.625  at batch 304.\n",
            "Train Loss: 1.7480024099349976    Train Acc:  0.703125  at batch 305.\n",
            "Train Loss: 1.7786415815353394    Train Acc:  0.578125  at batch 306.\n",
            "Train Loss: 1.6769378185272217    Train Acc:  0.65625  at batch 307.\n",
            "Train Loss: 1.7740634679794312    Train Acc:  0.578125  at batch 308.\n",
            "Train Loss: 1.736689567565918    Train Acc:  0.625  at batch 309.\n",
            "Train Loss: 1.7884644269943237    Train Acc:  0.671875  at batch 310.\n",
            "Train Loss: 1.6178011894226074    Train Acc:  0.75  at batch 311.\n",
            "Train Loss: 1.7100569009780884    Train Acc:  0.671875  at batch 312.\n",
            "Train Loss: 1.8109920024871826    Train Acc:  0.515625  at batch 313.\n",
            "Train Loss: 1.7621352672576904    Train Acc:  0.6875  at batch 314.\n",
            "Train Loss: 1.7502562999725342    Train Acc:  0.625  at batch 315.\n",
            "Train Loss: 1.6818081140518188    Train Acc:  0.71875  at batch 316.\n",
            "Train Loss: 1.741344690322876    Train Acc:  0.578125  at batch 317.\n",
            "Train Loss: 1.7848676443099976    Train Acc:  0.59375  at batch 318.\n",
            "Train Loss: 1.7623428106307983    Train Acc:  0.6875  at batch 319.\n",
            "Train Loss: 1.8087009191513062    Train Acc:  0.5625  at batch 320.\n",
            "Train Loss: 1.7523337602615356    Train Acc:  0.65625  at batch 321.\n",
            "Train Loss: 1.7895634174346924    Train Acc:  0.59375  at batch 322.\n",
            "Train Loss: 1.7047115564346313    Train Acc:  0.78125  at batch 323.\n",
            "Train Loss: 1.786753535270691    Train Acc:  0.640625  at batch 324.\n",
            "Train Loss: 1.7685050964355469    Train Acc:  0.578125  at batch 325.\n",
            "Train Loss: 1.6830226182937622    Train Acc:  0.640625  at batch 326.\n",
            "Train Loss: 1.7148280143737793    Train Acc:  0.625  at batch 327.\n",
            "Train Loss: 1.7963885068893433    Train Acc:  0.53125  at batch 328.\n",
            "Train Loss: 1.7346309423446655    Train Acc:  0.625  at batch 329.\n",
            "Train Loss: 1.7548273801803589    Train Acc:  0.609375  at batch 330.\n",
            "Train Loss: 1.7220324277877808    Train Acc:  0.703125  at batch 331.\n",
            "Train Loss: 1.720008134841919    Train Acc:  0.671875  at batch 332.\n",
            "Train Loss: 1.7641977071762085    Train Acc:  0.703125  at batch 333.\n",
            "Train Loss: 1.7789876461029053    Train Acc:  0.640625  at batch 334.\n",
            "Train Loss: 1.7549989223480225    Train Acc:  0.671875  at batch 335.\n",
            "Train Loss: 1.7291818857192993    Train Acc:  0.640625  at batch 336.\n",
            "Train Loss: 1.7476749420166016    Train Acc:  0.671875  at batch 337.\n",
            "Train Loss: 1.711484432220459    Train Acc:  0.640625  at batch 338.\n",
            "Train Loss: 1.6947894096374512    Train Acc:  0.671875  at batch 339.\n",
            "Train Loss: 1.7518192529678345    Train Acc:  0.59375  at batch 340.\n",
            "Train Loss: 1.7836517095565796    Train Acc:  0.578125  at batch 341.\n",
            "Train Loss: 1.7494198083877563    Train Acc:  0.625  at batch 342.\n",
            "Train Loss: 1.705006718635559    Train Acc:  0.625  at batch 343.\n",
            "Train Loss: 1.7896875143051147    Train Acc:  0.65625  at batch 344.\n",
            "Train Loss: 1.7592823505401611    Train Acc:  0.671875  at batch 345.\n",
            "Train Loss: 1.7171107530593872    Train Acc:  0.59375  at batch 346.\n",
            "Train Loss: 1.7046953439712524    Train Acc:  0.65625  at batch 347.\n",
            "Train Loss: 1.7656739950180054    Train Acc:  0.640625  at batch 348.\n",
            "Train Loss: 1.6551742553710938    Train Acc:  0.765625  at batch 349.\n",
            "Train Loss: 1.7242887020111084    Train Acc:  0.640625  at batch 350.\n",
            "Train Loss: 1.698923945426941    Train Acc:  0.671875  at batch 351.\n",
            "Train Loss: 1.6401785612106323    Train Acc:  0.671875  at batch 352.\n",
            "Train Loss: 1.7791790962219238    Train Acc:  0.53125  at batch 353.\n",
            "Train Loss: 1.7137584686279297    Train Acc:  0.703125  at batch 354.\n",
            "Train Loss: 1.72156822681427    Train Acc:  0.578125  at batch 355.\n",
            "Train Loss: 1.7357193231582642    Train Acc:  0.609375  at batch 356.\n",
            "Train Loss: 1.7305384874343872    Train Acc:  0.65625  at batch 357.\n",
            "Train Loss: 1.707069754600525    Train Acc:  0.71875  at batch 358.\n",
            "Train Loss: 1.6743091344833374    Train Acc:  0.75  at batch 359.\n",
            "Train Loss: 1.6939102411270142    Train Acc:  0.703125  at batch 360.\n",
            "Train Loss: 1.824678897857666    Train Acc:  0.609375  at batch 361.\n",
            "Train Loss: 1.7038495540618896    Train Acc:  0.6875  at batch 362.\n",
            "Train Loss: 1.630862832069397    Train Acc:  0.6875  at batch 363.\n",
            "Train Loss: 1.7037302255630493    Train Acc:  0.765625  at batch 364.\n",
            "Train Loss: 1.7280832529067993    Train Acc:  0.703125  at batch 365.\n",
            "Train Loss: 1.7751718759536743    Train Acc:  0.6875  at batch 366.\n",
            "Train Loss: 1.735185146331787    Train Acc:  0.671875  at batch 367.\n",
            "Train Loss: 1.71963632106781    Train Acc:  0.65625  at batch 368.\n",
            "Train Loss: 1.740476131439209    Train Acc:  0.71875  at batch 369.\n",
            "Train Loss: 1.7131773233413696    Train Acc:  0.703125  at batch 370.\n",
            "Train Loss: 1.674217939376831    Train Acc:  0.640625  at batch 371.\n",
            "Train Loss: 1.7926251888275146    Train Acc:  0.671875  at batch 372.\n",
            "Train Loss: 1.6169605255126953    Train Acc:  0.734375  at batch 373.\n",
            "Train Loss: 1.652477502822876    Train Acc:  0.703125  at batch 374.\n",
            "Train Loss: 1.7586989402770996    Train Acc:  0.734375  at batch 375.\n",
            "Train Loss: 1.7650364637374878    Train Acc:  0.609375  at batch 376.\n",
            "Train Loss: 1.7768323421478271    Train Acc:  0.625  at batch 377.\n",
            "Train Loss: 1.6991095542907715    Train Acc:  0.6875  at batch 378.\n",
            "Train Loss: 1.6804906129837036    Train Acc:  0.671875  at batch 379.\n",
            "Train Loss: 1.69405198097229    Train Acc:  0.65625  at batch 380.\n",
            "Train Loss: 1.7476153373718262    Train Acc:  0.640625  at batch 381.\n",
            "Train Loss: 1.7454569339752197    Train Acc:  0.53125  at batch 382.\n",
            "Train Loss: 1.705145239830017    Train Acc:  0.671875  at batch 383.\n",
            "Train Loss: 1.7194619178771973    Train Acc:  0.6875  at batch 384.\n",
            "Train Loss: 1.695975661277771    Train Acc:  0.625  at batch 385.\n",
            "Train Loss: 1.7427163124084473    Train Acc:  0.578125  at batch 386.\n",
            "Train Loss: 1.697289228439331    Train Acc:  0.671875  at batch 387.\n",
            "Train Loss: 1.7277956008911133    Train Acc:  0.71875  at batch 388.\n",
            "Train Loss: 1.6878230571746826    Train Acc:  0.65625  at batch 389.\n",
            "Train Loss: 1.6413111686706543    Train Acc:  0.765625  at batch 390.\n",
            "Train Loss: 1.7431676387786865    Train Acc:  0.59375  at batch 391.\n",
            "Train Loss: 1.7482843399047852    Train Acc:  0.53125  at batch 392.\n",
            "Train Loss: 1.758551001548767    Train Acc:  0.65625  at batch 393.\n",
            "Train Loss: 1.6916544437408447    Train Acc:  0.6875  at batch 394.\n",
            "Train Loss: 1.6828449964523315    Train Acc:  0.71875  at batch 395.\n",
            "Train Loss: 1.6203263998031616    Train Acc:  0.75  at batch 396.\n",
            "Train Loss: 1.7144060134887695    Train Acc:  0.625  at batch 397.\n",
            "Train Loss: 1.6589465141296387    Train Acc:  0.75  at batch 398.\n",
            "Train Loss: 1.6714130640029907    Train Acc:  0.640625  at batch 399.\n",
            "Train Loss: 1.667685866355896    Train Acc:  0.671875  at batch 400.\n",
            "Train Loss: 1.7019914388656616    Train Acc:  0.625  at batch 401.\n",
            "Train Loss: 1.6401101350784302    Train Acc:  0.75  at batch 402.\n",
            "Train Loss: 1.688496470451355    Train Acc:  0.640625  at batch 403.\n",
            "Train Loss: 1.7731152772903442    Train Acc:  0.609375  at batch 404.\n",
            "Train Loss: 1.6828479766845703    Train Acc:  0.734375  at batch 405.\n",
            "Train Loss: 1.5968291759490967    Train Acc:  0.75  at batch 406.\n",
            "Train Loss: 1.6225038766860962    Train Acc:  0.828125  at batch 407.\n",
            "Train Loss: 1.7059412002563477    Train Acc:  0.78125  at batch 408.\n",
            "Train Loss: 1.616046667098999    Train Acc:  0.6875  at batch 409.\n",
            "Train Loss: 1.6596450805664062    Train Acc:  0.703125  at batch 410.\n",
            "Train Loss: 1.7658437490463257    Train Acc:  0.59375  at batch 411.\n",
            "Train Loss: 1.7182503938674927    Train Acc:  0.65625  at batch 412.\n",
            "Train Loss: 1.6901053190231323    Train Acc:  0.703125  at batch 413.\n",
            "Train Loss: 1.6717767715454102    Train Acc:  0.765625  at batch 414.\n",
            "Train Loss: 1.5715913772583008    Train Acc:  0.796875  at batch 415.\n",
            "Train Loss: 1.6982989311218262    Train Acc:  0.640625  at batch 416.\n",
            "Train Loss: 1.662718415260315    Train Acc:  0.765625  at batch 417.\n",
            "Train Loss: 1.6925276517868042    Train Acc:  0.71875  at batch 418.\n",
            "Train Loss: 1.7096935510635376    Train Acc:  0.578125  at batch 419.\n",
            "Train Loss: 1.6516376733779907    Train Acc:  0.765625  at batch 420.\n",
            "Train Loss: 1.7210209369659424    Train Acc:  0.71875  at batch 421.\n",
            "Train Loss: 1.7002418041229248    Train Acc:  0.609375  at batch 422.\n",
            "Train Loss: 1.6799737215042114    Train Acc:  0.6875  at batch 423.\n",
            "Train Loss: 1.6295969486236572    Train Acc:  0.734375  at batch 424.\n",
            "Train Loss: 1.613534927368164    Train Acc:  0.71875  at batch 425.\n",
            "Train Loss: 1.7409641742706299    Train Acc:  0.609375  at batch 426.\n",
            "Train Loss: 1.6365553140640259    Train Acc:  0.75  at batch 427.\n",
            "Train Loss: 1.7321863174438477    Train Acc:  0.765625  at batch 428.\n",
            "Train Loss: 1.7058299779891968    Train Acc:  0.6875  at batch 429.\n",
            "Train Loss: 1.6257835626602173    Train Acc:  0.765625  at batch 430.\n",
            "Train Loss: 1.6703113317489624    Train Acc:  0.75  at batch 431.\n",
            "Train Loss: 1.5899543762207031    Train Acc:  0.796875  at batch 432.\n",
            "Train Loss: 1.638575792312622    Train Acc:  0.671875  at batch 433.\n",
            "Train Loss: 1.59992253780365    Train Acc:  0.6875  at batch 434.\n",
            "Train Loss: 1.6382476091384888    Train Acc:  0.671875  at batch 435.\n",
            "Train Loss: 1.6726714372634888    Train Acc:  0.6875  at batch 436.\n",
            "Train Loss: 1.6643215417861938    Train Acc:  0.6875  at batch 437.\n",
            "Train Loss: 1.6007179021835327    Train Acc:  0.765625  at batch 438.\n",
            "Train Loss: 1.61894953250885    Train Acc:  0.671875  at batch 439.\n",
            "Train Loss: 1.6423784494400024    Train Acc:  0.734375  at batch 440.\n",
            "Train Loss: 1.6270396709442139    Train Acc:  0.765625  at batch 441.\n",
            "Train Loss: 1.6025630235671997    Train Acc:  0.671875  at batch 442.\n",
            "Train Loss: 1.5710594654083252    Train Acc:  0.71875  at batch 443.\n",
            "Train Loss: 1.7302981615066528    Train Acc:  0.59375  at batch 444.\n",
            "Train Loss: 1.7212486267089844    Train Acc:  0.609375  at batch 445.\n",
            "Train Loss: 1.6227750778198242    Train Acc:  0.71875  at batch 446.\n",
            "Train Loss: 1.6910537481307983    Train Acc:  0.578125  at batch 447.\n",
            "Train Loss: 1.6926438808441162    Train Acc:  0.625  at batch 448.\n",
            "Train Loss: 1.7061796188354492    Train Acc:  0.6875  at batch 449.\n",
            "Train Loss: 1.6381962299346924    Train Acc:  0.71875  at batch 450.\n",
            "Train Loss: 1.6918087005615234    Train Acc:  0.6875  at batch 451.\n",
            "Train Loss: 1.6576957702636719    Train Acc:  0.625  at batch 452.\n",
            "Train Loss: 1.6967811584472656    Train Acc:  0.703125  at batch 453.\n",
            "Train Loss: 1.6947312355041504    Train Acc:  0.65625  at batch 454.\n",
            "Train Loss: 1.5915470123291016    Train Acc:  0.765625  at batch 455.\n",
            "Train Loss: 1.6662852764129639    Train Acc:  0.671875  at batch 456.\n",
            "Train Loss: 1.6762166023254395    Train Acc:  0.78125  at batch 457.\n",
            "Train Loss: 1.5955029726028442    Train Acc:  0.796875  at batch 458.\n",
            "Train Loss: 1.6669158935546875    Train Acc:  0.6875  at batch 459.\n",
            "Train Loss: 1.5523886680603027    Train Acc:  0.828125  at batch 460.\n",
            "Train Loss: 1.6470961570739746    Train Acc:  0.640625  at batch 461.\n",
            "Train Loss: 1.5903522968292236    Train Acc:  0.734375  at batch 462.\n",
            "Train Loss: 1.6282477378845215    Train Acc:  0.671875  at batch 463.\n",
            "Train Loss: 1.618895411491394    Train Acc:  0.65625  at batch 464.\n",
            "Train Loss: 1.6046245098114014    Train Acc:  0.75  at batch 465.\n",
            "Train Loss: 1.678802728652954    Train Acc:  0.65625  at batch 466.\n",
            "Train Loss: 1.670080542564392    Train Acc:  0.65625  at batch 467.\n",
            "Train Loss: 1.747633695602417    Train Acc:  0.671875  at batch 468.\n",
            "Train Loss: 1.681787371635437    Train Acc:  0.65625  at batch 469.\n",
            "Train Loss: 1.6256202459335327    Train Acc:  0.71875  at batch 470.\n",
            "Train Loss: 1.58829927444458    Train Acc:  0.734375  at batch 471.\n",
            "Train Loss: 1.7069673538208008    Train Acc:  0.625  at batch 472.\n",
            "Train Loss: 1.6985077857971191    Train Acc:  0.5625  at batch 473.\n",
            "Train Loss: 1.6375231742858887    Train Acc:  0.734375  at batch 474.\n",
            "Train Loss: 1.6352595090866089    Train Acc:  0.78125  at batch 475.\n",
            "Train Loss: 1.800489902496338    Train Acc:  0.5625  at batch 476.\n",
            "Train Loss: 1.6233007907867432    Train Acc:  0.703125  at batch 477.\n",
            "Train Loss: 1.6128928661346436    Train Acc:  0.71875  at batch 478.\n",
            "Train Loss: 1.6917693614959717    Train Acc:  0.65625  at batch 479.\n",
            "Train Loss: 1.638782024383545    Train Acc:  0.6875  at batch 480.\n",
            "Train Loss: 1.6449857950210571    Train Acc:  0.71875  at batch 481.\n",
            "Train Loss: 1.721778154373169    Train Acc:  0.671875  at batch 482.\n",
            "Train Loss: 1.607283353805542    Train Acc:  0.703125  at batch 483.\n",
            "Train Loss: 1.5751714706420898    Train Acc:  0.84375  at batch 484.\n",
            "Train Loss: 1.6655585765838623    Train Acc:  0.546875  at batch 485.\n",
            "Train Loss: 1.592980146408081    Train Acc:  0.734375  at batch 486.\n",
            "Train Loss: 1.6109205484390259    Train Acc:  0.71875  at batch 487.\n",
            "Train Loss: 1.5964879989624023    Train Acc:  0.8125  at batch 488.\n",
            "Train Loss: 1.643051028251648    Train Acc:  0.734375  at batch 489.\n",
            "Train Loss: 1.6083248853683472    Train Acc:  0.765625  at batch 490.\n",
            "Train Loss: 1.5265977382659912    Train Acc:  0.78125  at batch 491.\n",
            "Train Loss: 1.604429841041565    Train Acc:  0.71875  at batch 492.\n",
            "Train Loss: 1.6558010578155518    Train Acc:  0.71875  at batch 493.\n",
            "Train Loss: 1.5989036560058594    Train Acc:  0.734375  at batch 494.\n",
            "Train Loss: 1.5473417043685913    Train Acc:  0.78125  at batch 495.\n",
            "Train Loss: 1.7651748657226562    Train Acc:  0.546875  at batch 496.\n",
            "Train Loss: 1.606893539428711    Train Acc:  0.6875  at batch 497.\n",
            "Train Loss: 1.5897953510284424    Train Acc:  0.796875  at batch 498.\n",
            "Train Loss: 1.695011019706726    Train Acc:  0.703125  at batch 499.\n",
            "Train Loss: 1.633020043373108    Train Acc:  0.578125  at batch 500.\n",
            "Train Loss: 1.6524840593338013    Train Acc:  0.640625  at batch 501.\n",
            "Train Loss: 1.6342289447784424    Train Acc:  0.703125  at batch 502.\n",
            "Train Loss: 1.6340103149414062    Train Acc:  0.75  at batch 503.\n",
            "Train Loss: 1.6215991973876953    Train Acc:  0.75  at batch 504.\n",
            "Train Loss: 1.4618875980377197    Train Acc:  0.765625  at batch 505.\n",
            "Train Loss: 1.5545647144317627    Train Acc:  0.703125  at batch 506.\n",
            "Train Loss: 1.6669397354125977    Train Acc:  0.671875  at batch 507.\n",
            "Train Loss: 1.5240976810455322    Train Acc:  0.71875  at batch 508.\n",
            "Train Loss: 1.6231112480163574    Train Acc:  0.71875  at batch 509.\n",
            "Train Loss: 1.6328550577163696    Train Acc:  0.625  at batch 510.\n",
            "Train Loss: 1.6615846157073975    Train Acc:  0.609375  at batch 511.\n",
            "Train Loss: 1.5522249937057495    Train Acc:  0.671875  at batch 512.\n",
            "Train Loss: 1.6530556678771973    Train Acc:  0.5625  at batch 513.\n",
            "Train Loss: 1.6496623754501343    Train Acc:  0.640625  at batch 514.\n",
            "Train Loss: 1.5659877061843872    Train Acc:  0.734375  at batch 515.\n",
            "Train Loss: 1.5882923603057861    Train Acc:  0.6875  at batch 516.\n",
            "Train Loss: 1.5442017316818237    Train Acc:  0.703125  at batch 517.\n",
            "Train Loss: 1.6246792078018188    Train Acc:  0.703125  at batch 518.\n",
            "Train Loss: 1.640763521194458    Train Acc:  0.609375  at batch 519.\n",
            "Train Loss: 1.6295310258865356    Train Acc:  0.671875  at batch 520.\n",
            "Train Loss: 1.4613213539123535    Train Acc:  0.71875  at batch 521.\n",
            "Train Loss: 1.5857832431793213    Train Acc:  0.625  at batch 522.\n",
            "Train Loss: 1.5782026052474976    Train Acc:  0.71875  at batch 523.\n",
            "Train Loss: 1.640633463859558    Train Acc:  0.671875  at batch 524.\n",
            "Train Loss: 1.5649797916412354    Train Acc:  0.71875  at batch 525.\n",
            "Train Loss: 1.5660320520401    Train Acc:  0.65625  at batch 526.\n",
            "Train Loss: 1.6170424222946167    Train Acc:  0.671875  at batch 527.\n",
            "Train Loss: 1.6401090621948242    Train Acc:  0.6875  at batch 528.\n",
            "Train Loss: 1.4944928884506226    Train Acc:  0.78125  at batch 529.\n",
            "Train Loss: 1.6381921768188477    Train Acc:  0.578125  at batch 530.\n",
            "Train Loss: 1.590831995010376    Train Acc:  0.734375  at batch 531.\n",
            "Train Loss: 1.534543514251709    Train Acc:  0.71875  at batch 532.\n",
            "Train Loss: 1.578721523284912    Train Acc:  0.65625  at batch 533.\n",
            "Train Loss: 1.6039682626724243    Train Acc:  0.71875  at batch 534.\n",
            "Train Loss: 1.5606591701507568    Train Acc:  0.734375  at batch 535.\n",
            "Train Loss: 1.5075085163116455    Train Acc:  0.734375  at batch 536.\n",
            "Train Loss: 1.6126867532730103    Train Acc:  0.625  at batch 537.\n",
            "Train Loss: 1.5928351879119873    Train Acc:  0.640625  at batch 538.\n",
            "Train Loss: 1.5712448358535767    Train Acc:  0.78125  at batch 539.\n",
            "Train Loss: 1.668296456336975    Train Acc:  0.515625  at batch 540.\n",
            "Train Loss: 1.6136982440948486    Train Acc:  0.609375  at batch 541.\n",
            "Train Loss: 1.4398696422576904    Train Acc:  0.796875  at batch 542.\n",
            "Train Loss: 1.5706408023834229    Train Acc:  0.703125  at batch 543.\n",
            "Train Loss: 1.643491506576538    Train Acc:  0.640625  at batch 544.\n",
            "Train Loss: 1.5723146200180054    Train Acc:  0.65625  at batch 545.\n",
            "Train Loss: 1.5008646249771118    Train Acc:  0.6875  at batch 546.\n",
            "Train Loss: 1.4785264730453491    Train Acc:  0.78125  at batch 547.\n",
            "Train Loss: 1.5359079837799072    Train Acc:  0.75  at batch 548.\n",
            "Train Loss: 1.5200262069702148    Train Acc:  0.796875  at batch 549.\n",
            "Train Loss: 1.520309567451477    Train Acc:  0.734375  at batch 550.\n",
            "Train Loss: 1.6220344305038452    Train Acc:  0.5625  at batch 551.\n",
            "Train Loss: 1.5221784114837646    Train Acc:  0.75  at batch 552.\n",
            "Train Loss: 1.593563199043274    Train Acc:  0.59375  at batch 553.\n",
            "Train Loss: 1.5627528429031372    Train Acc:  0.703125  at batch 554.\n",
            "Train Loss: 1.5431718826293945    Train Acc:  0.71875  at batch 555.\n",
            "Train Loss: 1.5284762382507324    Train Acc:  0.65625  at batch 556.\n",
            "Train Loss: 1.5923185348510742    Train Acc:  0.765625  at batch 557.\n",
            "Train Loss: 1.6466184854507446    Train Acc:  0.640625  at batch 558.\n",
            "Train Loss: 1.6155146360397339    Train Acc:  0.671875  at batch 559.\n",
            "Train Loss: 1.5578725337982178    Train Acc:  0.703125  at batch 560.\n",
            "Train Loss: 1.6028306484222412    Train Acc:  0.671875  at batch 561.\n",
            "Train Loss: 1.5737563371658325    Train Acc:  0.671875  at batch 562.\n",
            "Train Loss: 1.3986036777496338    Train Acc:  0.8125  at batch 563.\n",
            "Train Loss: 1.5845023393630981    Train Acc:  0.71875  at batch 564.\n",
            "Train Loss: 1.5223878622055054    Train Acc:  0.6875  at batch 565.\n",
            "Train Loss: 1.5723252296447754    Train Acc:  0.640625  at batch 566.\n",
            "Train Loss: 1.6101895570755005    Train Acc:  0.578125  at batch 567.\n",
            "Train Loss: 1.623348593711853    Train Acc:  0.625  at batch 568.\n",
            "Train Loss: 1.6421356201171875    Train Acc:  0.703125  at batch 569.\n",
            "Train Loss: 1.5470679998397827    Train Acc:  0.75  at batch 570.\n",
            "Train Loss: 1.5536161661148071    Train Acc:  0.546875  at batch 571.\n",
            "Train Loss: 1.5483957529067993    Train Acc:  0.703125  at batch 572.\n",
            "Train Loss: 1.598726511001587    Train Acc:  0.703125  at batch 573.\n",
            "Train Loss: 1.5789874792099    Train Acc:  0.765625  at batch 574.\n",
            "Train Loss: 1.5419903993606567    Train Acc:  0.78125  at batch 575.\n",
            "Train Loss: 1.5119792222976685    Train Acc:  0.71875  at batch 576.\n",
            "Train Loss: 1.5767945051193237    Train Acc:  0.671875  at batch 577.\n",
            "Train Loss: 1.4447804689407349    Train Acc:  0.75  at batch 578.\n",
            "Train Loss: 1.6823010444641113    Train Acc:  0.609375  at batch 579.\n",
            "Train Loss: 1.6031701564788818    Train Acc:  0.6875  at batch 580.\n",
            "Train Loss: 1.5297901630401611    Train Acc:  0.703125  at batch 581.\n",
            "Train Loss: 1.493560552597046    Train Acc:  0.828125  at batch 582.\n",
            "Train Loss: 1.4725083112716675    Train Acc:  0.671875  at batch 583.\n",
            "Train Loss: 1.5534753799438477    Train Acc:  0.703125  at batch 584.\n",
            "Train Loss: 1.511183738708496    Train Acc:  0.765625  at batch 585.\n",
            "Train Loss: 1.5527582168579102    Train Acc:  0.8125  at batch 586.\n",
            "Train Loss: 1.6436433792114258    Train Acc:  0.609375  at batch 587.\n",
            "Train Loss: 1.550950050354004    Train Acc:  0.734375  at batch 588.\n",
            "Train Loss: 1.632429599761963    Train Acc:  0.625  at batch 589.\n",
            "Train Loss: 1.5186244249343872    Train Acc:  0.6875  at batch 590.\n",
            "Train Loss: 1.4894850254058838    Train Acc:  0.8125  at batch 591.\n",
            "Train Loss: 1.5237040519714355    Train Acc:  0.765625  at batch 592.\n",
            "Train Loss: 1.516153335571289    Train Acc:  0.71875  at batch 593.\n",
            "Train Loss: 1.5528010129928589    Train Acc:  0.75  at batch 594.\n",
            "Train Loss: 1.5735156536102295    Train Acc:  0.703125  at batch 595.\n",
            "Train Loss: 1.5813255310058594    Train Acc:  0.65625  at batch 596.\n",
            "Train Loss: 1.3414406776428223    Train Acc:  0.828125  at batch 597.\n",
            "Train Loss: 1.6622979640960693    Train Acc:  0.578125  at batch 598.\n",
            "Train Loss: 1.5627223253250122    Train Acc:  0.71875  at batch 599.\n",
            "Train Loss: 1.4490302801132202    Train Acc:  0.890625  at batch 600.\n",
            "Train Loss: 1.5488578081130981    Train Acc:  0.6875  at batch 601.\n",
            "Train Loss: 1.5104209184646606    Train Acc:  0.640625  at batch 602.\n",
            "Train Loss: 1.472419261932373    Train Acc:  0.71875  at batch 603.\n",
            "Train Loss: 1.5216350555419922    Train Acc:  0.6875  at batch 604.\n",
            "Train Loss: 1.5358033180236816    Train Acc:  0.71875  at batch 605.\n",
            "Train Loss: 1.4913610219955444    Train Acc:  0.734375  at batch 606.\n",
            "Train Loss: 1.4770798683166504    Train Acc:  0.734375  at batch 607.\n",
            "Train Loss: 1.722438931465149    Train Acc:  0.59375  at batch 608.\n",
            "Train Loss: 1.474995732307434    Train Acc:  0.78125  at batch 609.\n",
            "Train Loss: 1.4651390314102173    Train Acc:  0.703125  at batch 610.\n",
            "Train Loss: 1.5173887014389038    Train Acc:  0.765625  at batch 611.\n",
            "Train Loss: 1.590519666671753    Train Acc:  0.625  at batch 612.\n",
            "Train Loss: 1.439776062965393    Train Acc:  0.765625  at batch 613.\n",
            "Train Loss: 1.4534554481506348    Train Acc:  0.703125  at batch 614.\n",
            "Train Loss: 1.511422872543335    Train Acc:  0.71875  at batch 615.\n",
            "Train Loss: 1.4826043844223022    Train Acc:  0.71875  at batch 616.\n",
            "Train Loss: 1.488552451133728    Train Acc:  0.6875  at batch 617.\n",
            "Train Loss: 1.642087697982788    Train Acc:  0.609375  at batch 618.\n",
            "Train Loss: 1.5311834812164307    Train Acc:  0.640625  at batch 619.\n",
            "Train Loss: 1.5952383279800415    Train Acc:  0.671875  at batch 620.\n",
            "Train Loss: 1.6824798583984375    Train Acc:  0.625  at batch 621.\n",
            "Train Loss: 1.4918278455734253    Train Acc:  0.71875  at batch 622.\n",
            "Train Loss: 1.4631984233856201    Train Acc:  0.78125  at batch 623.\n",
            "Train Loss: 1.5029388666152954    Train Acc:  0.765625  at batch 624.\n",
            "Train Loss: 1.5087133646011353    Train Acc:  0.671875  at batch 625.\n",
            "Train Loss: 1.56699538230896    Train Acc:  0.6875  at batch 626.\n",
            "Train Loss: 1.418692946434021    Train Acc:  0.75  at batch 627.\n",
            "Train Loss: 1.4423288106918335    Train Acc:  0.703125  at batch 628.\n",
            "Train Loss: 1.5377000570297241    Train Acc:  0.671875  at batch 629.\n",
            "Train Loss: 1.5032672882080078    Train Acc:  0.65625  at batch 630.\n",
            "Train Loss: 1.5390057563781738    Train Acc:  0.6875  at batch 631.\n",
            "Train Loss: 1.510959267616272    Train Acc:  0.703125  at batch 632.\n",
            "Train Loss: 1.498529314994812    Train Acc:  0.765625  at batch 633.\n",
            "Train Loss: 1.5000085830688477    Train Acc:  0.671875  at batch 634.\n",
            "Train Loss: 1.5091571807861328    Train Acc:  0.78125  at batch 635.\n",
            "Train Loss: 1.4464091062545776    Train Acc:  0.6875  at batch 636.\n",
            "Train Loss: 1.5278071165084839    Train Acc:  0.78125  at batch 637.\n",
            "Train Loss: 1.5261770486831665    Train Acc:  0.765625  at batch 638.\n",
            "Train Loss: 1.4705411195755005    Train Acc:  0.765625  at batch 639.\n",
            "Train Loss: 1.508528470993042    Train Acc:  0.71875  at batch 640.\n",
            "Train Loss: 1.5532547235488892    Train Acc:  0.71875  at batch 641.\n",
            "Train Loss: 1.483177900314331    Train Acc:  0.78125  at batch 642.\n",
            "Train Loss: 1.4464712142944336    Train Acc:  0.78125  at batch 643.\n",
            "Train Loss: 1.4955459833145142    Train Acc:  0.71875  at batch 644.\n",
            "Train Loss: 1.4407904148101807    Train Acc:  0.671875  at batch 645.\n",
            "Train Loss: 1.6162482500076294    Train Acc:  0.625  at batch 646.\n",
            "Train Loss: 1.4275867938995361    Train Acc:  0.71875  at batch 647.\n",
            "Train Loss: 1.5331594944000244    Train Acc:  0.671875  at batch 648.\n",
            "Train Loss: 1.4260116815567017    Train Acc:  0.734375  at batch 649.\n",
            "Train Loss: 1.3815209865570068    Train Acc:  0.796875  at batch 650.\n",
            "Train Loss: 1.5416547060012817    Train Acc:  0.671875  at batch 651.\n",
            "Train Loss: 1.5139600038528442    Train Acc:  0.65625  at batch 652.\n",
            "Train Loss: 1.4764838218688965    Train Acc:  0.734375  at batch 653.\n",
            "Train Loss: 1.436730146408081    Train Acc:  0.734375  at batch 654.\n",
            "Train Loss: 1.4707105159759521    Train Acc:  0.734375  at batch 655.\n",
            "Train Loss: 1.4759703874588013    Train Acc:  0.6875  at batch 656.\n",
            "Train Loss: 1.5337963104248047    Train Acc:  0.703125  at batch 657.\n",
            "Train Loss: 1.4850702285766602    Train Acc:  0.765625  at batch 658.\n",
            "Train Loss: 1.4401881694793701    Train Acc:  0.671875  at batch 659.\n",
            "Train Loss: 1.4757494926452637    Train Acc:  0.640625  at batch 660.\n",
            "Train Loss: 1.453838586807251    Train Acc:  0.65625  at batch 661.\n",
            "Train Loss: 1.53938889503479    Train Acc:  0.6875  at batch 662.\n",
            "Train Loss: 1.5599696636199951    Train Acc:  0.625  at batch 663.\n",
            "Train Loss: 1.4236648082733154    Train Acc:  0.796875  at batch 664.\n",
            "Train Loss: 1.5737134218215942    Train Acc:  0.671875  at batch 665.\n",
            "Train Loss: 1.5815417766571045    Train Acc:  0.59375  at batch 666.\n",
            "Train Loss: 1.520810604095459    Train Acc:  0.625  at batch 667.\n",
            "Train Loss: 1.5465173721313477    Train Acc:  0.65625  at batch 668.\n",
            "Train Loss: 1.4702818393707275    Train Acc:  0.71875  at batch 669.\n",
            "Train Loss: 1.5116405487060547    Train Acc:  0.765625  at batch 670.\n",
            "Train Loss: 1.5351474285125732    Train Acc:  0.6875  at batch 671.\n",
            "Train Loss: 1.5146838426589966    Train Acc:  0.671875  at batch 672.\n",
            "Train Loss: 1.4469926357269287    Train Acc:  0.71875  at batch 673.\n",
            "Train Loss: 1.3704535961151123    Train Acc:  0.84375  at batch 674.\n",
            "Train Loss: 1.4359883069992065    Train Acc:  0.6875  at batch 675.\n",
            "Train Loss: 1.3925338983535767    Train Acc:  0.765625  at batch 676.\n",
            "Train Loss: 1.5127862691879272    Train Acc:  0.71875  at batch 677.\n",
            "Train Loss: 1.4982569217681885    Train Acc:  0.625  at batch 678.\n",
            "Train Loss: 1.5611919164657593    Train Acc:  0.5625  at batch 679.\n",
            "Train Loss: 1.52388334274292    Train Acc:  0.6875  at batch 680.\n",
            "Train Loss: 1.5808545351028442    Train Acc:  0.609375  at batch 681.\n",
            "Train Loss: 1.532947063446045    Train Acc:  0.6875  at batch 682.\n",
            "Train Loss: 1.4139708280563354    Train Acc:  0.71875  at batch 683.\n",
            "Train Loss: 1.4407086372375488    Train Acc:  0.765625  at batch 684.\n",
            "Train Loss: 1.4448343515396118    Train Acc:  0.65625  at batch 685.\n",
            "Train Loss: 1.4502912759780884    Train Acc:  0.671875  at batch 686.\n",
            "Train Loss: 1.5174643993377686    Train Acc:  0.734375  at batch 687.\n",
            "Train Loss: 1.5075743198394775    Train Acc:  0.671875  at batch 688.\n",
            "Train Loss: 1.4859540462493896    Train Acc:  0.71875  at batch 689.\n",
            "Train Loss: 1.4892522096633911    Train Acc:  0.765625  at batch 690.\n",
            "Train Loss: 1.428966760635376    Train Acc:  0.75  at batch 691.\n",
            "Train Loss: 1.5693604946136475    Train Acc:  0.671875  at batch 692.\n",
            "Train Loss: 1.363656759262085    Train Acc:  0.84375  at batch 693.\n",
            "Train Loss: 1.4949212074279785    Train Acc:  0.71875  at batch 694.\n",
            "Train Loss: 1.4596965312957764    Train Acc:  0.734375  at batch 695.\n",
            "Train Loss: 1.4352459907531738    Train Acc:  0.703125  at batch 696.\n",
            "Train Loss: 1.5158907175064087    Train Acc:  0.71875  at batch 697.\n",
            "Train Loss: 1.463375449180603    Train Acc:  0.734375  at batch 698.\n",
            "Train Loss: 1.448814868927002    Train Acc:  0.8125  at batch 699.\n",
            "Train Loss: 1.4715214967727661    Train Acc:  0.78125  at batch 700.\n",
            "Train Loss: 1.3799773454666138    Train Acc:  0.859375  at batch 701.\n",
            "Train Loss: 1.4788408279418945    Train Acc:  0.734375  at batch 702.\n",
            "Train Loss: 1.5216346979141235    Train Acc:  0.703125  at batch 703.\n",
            "Train Loss: 1.4868974685668945    Train Acc:  0.78125  at batch 704.\n",
            "Train Loss: 1.371255874633789    Train Acc:  0.84375  at batch 705.\n",
            "Train Loss: 1.4774295091629028    Train Acc:  0.703125  at batch 706.\n",
            "Train Loss: 1.4360212087631226    Train Acc:  0.71875  at batch 707.\n",
            "Train Loss: 1.455592393875122    Train Acc:  0.671875  at batch 708.\n",
            "Train Loss: 1.431890845298767    Train Acc:  0.796875  at batch 709.\n",
            "Train Loss: 1.3946400880813599    Train Acc:  0.796875  at batch 710.\n",
            "Train Loss: 1.388543725013733    Train Acc:  0.75  at batch 711.\n",
            "Train Loss: 1.488200068473816    Train Acc:  0.703125  at batch 712.\n",
            "Train Loss: 1.509338140487671    Train Acc:  0.6875  at batch 713.\n",
            "Train Loss: 1.4511277675628662    Train Acc:  0.71875  at batch 714.\n",
            "Train Loss: 1.4292725324630737    Train Acc:  0.75  at batch 715.\n",
            "Train Loss: 1.509190559387207    Train Acc:  0.65625  at batch 716.\n",
            "Train Loss: 1.5373115539550781    Train Acc:  0.578125  at batch 717.\n",
            "Train Loss: 1.4501992464065552    Train Acc:  0.796875  at batch 718.\n",
            "Train Loss: 1.4275810718536377    Train Acc:  0.734375  at batch 719.\n",
            "Train Loss: 1.3193564414978027    Train Acc:  0.84375  at batch 720.\n",
            "Train Loss: 1.540258765220642    Train Acc:  0.640625  at batch 721.\n",
            "Train Loss: 1.4395077228546143    Train Acc:  0.703125  at batch 722.\n",
            "Train Loss: 1.4789924621582031    Train Acc:  0.71875  at batch 723.\n",
            "Train Loss: 1.4030632972717285    Train Acc:  0.734375  at batch 724.\n",
            "Train Loss: 1.5214639902114868    Train Acc:  0.671875  at batch 725.\n",
            "Train Loss: 1.4302659034729004    Train Acc:  0.65625  at batch 726.\n",
            "Train Loss: 1.3848391771316528    Train Acc:  0.765625  at batch 727.\n",
            "Train Loss: 1.5289757251739502    Train Acc:  0.59375  at batch 728.\n",
            "Train Loss: 1.4061319828033447    Train Acc:  0.75  at batch 729.\n",
            "Train Loss: 1.447000503540039    Train Acc:  0.703125  at batch 730.\n",
            "Train Loss: 1.4526591300964355    Train Acc:  0.71875  at batch 731.\n",
            "Train Loss: 1.4126055240631104    Train Acc:  0.75  at batch 732.\n",
            "Train Loss: 1.4335436820983887    Train Acc:  0.71875  at batch 733.\n",
            "Train Loss: 1.4400408267974854    Train Acc:  0.625  at batch 734.\n",
            "Train Loss: 1.4269897937774658    Train Acc:  0.75  at batch 735.\n",
            "Train Loss: 1.466126561164856    Train Acc:  0.703125  at batch 736.\n",
            "Train Loss: 1.394720196723938    Train Acc:  0.71875  at batch 737.\n",
            "Train Loss: 1.3640464544296265    Train Acc:  0.65625  at batch 738.\n",
            "Train Loss: 1.3448320627212524    Train Acc:  0.78125  at batch 739.\n",
            "Train Loss: 1.3484089374542236    Train Acc:  0.765625  at batch 740.\n",
            "Train Loss: 1.465009331703186    Train Acc:  0.65625  at batch 741.\n",
            "Train Loss: 1.3721370697021484    Train Acc:  0.78125  at batch 742.\n",
            "Train Loss: 1.4456406831741333    Train Acc:  0.71875  at batch 743.\n",
            "Train Loss: 1.3779677152633667    Train Acc:  0.78125  at batch 744.\n",
            "Train Loss: 1.4767746925354004    Train Acc:  0.71875  at batch 745.\n",
            "Train Loss: 1.372683048248291    Train Acc:  0.765625  at batch 746.\n",
            "Train Loss: 1.4720280170440674    Train Acc:  0.65625  at batch 747.\n",
            "Train Loss: 1.3796662092208862    Train Acc:  0.71875  at batch 748.\n",
            "Train Loss: 1.4795937538146973    Train Acc:  0.71875  at batch 749.\n",
            "Train Loss: 1.3899118900299072    Train Acc:  0.765625  at batch 750.\n",
            "Train Loss: 1.3335875272750854    Train Acc:  0.734375  at batch 751.\n",
            "Train Loss: 1.3690969944000244    Train Acc:  0.75  at batch 752.\n",
            "Train Loss: 1.4825334548950195    Train Acc:  0.703125  at batch 753.\n",
            "Train Loss: 1.3815916776657104    Train Acc:  0.65625  at batch 754.\n",
            "Train Loss: 1.3896338939666748    Train Acc:  0.71875  at batch 755.\n",
            "Train Loss: 1.4862016439437866    Train Acc:  0.703125  at batch 756.\n",
            "Train Loss: 1.349002480506897    Train Acc:  0.78125  at batch 757.\n",
            "Train Loss: 1.3918293714523315    Train Acc:  0.71875  at batch 758.\n",
            "Train Loss: 1.4603841304779053    Train Acc:  0.65625  at batch 759.\n",
            "Train Loss: 1.4045414924621582    Train Acc:  0.796875  at batch 760.\n",
            "Train Loss: 1.3466413021087646    Train Acc:  0.71875  at batch 761.\n",
            "Train Loss: 1.3616153001785278    Train Acc:  0.828125  at batch 762.\n",
            "Train Loss: 1.3791792392730713    Train Acc:  0.703125  at batch 763.\n",
            "Train Loss: 1.375188946723938    Train Acc:  0.703125  at batch 764.\n",
            "Train Loss: 1.462479591369629    Train Acc:  0.71875  at batch 765.\n",
            "Train Loss: 1.4339790344238281    Train Acc:  0.703125  at batch 766.\n",
            "Train Loss: 1.452213168144226    Train Acc:  0.640625  at batch 767.\n",
            "Train Loss: 1.4873327016830444    Train Acc:  0.640625  at batch 768.\n",
            "Train Loss: 1.4623901844024658    Train Acc:  0.6875  at batch 769.\n",
            "Train Loss: 1.5190203189849854    Train Acc:  0.609375  at batch 770.\n",
            "Train Loss: 1.333371877670288    Train Acc:  0.859375  at batch 771.\n",
            "Train Loss: 1.3844918012619019    Train Acc:  0.75  at batch 772.\n",
            "Train Loss: 1.265114665031433    Train Acc:  0.796875  at batch 773.\n",
            "Train Loss: 1.4339631795883179    Train Acc:  0.703125  at batch 774.\n",
            "Train Loss: 1.394190788269043    Train Acc:  0.765625  at batch 775.\n",
            "Train Loss: 1.4481627941131592    Train Acc:  0.6875  at batch 776.\n",
            "Train Loss: 1.2494045495986938    Train Acc:  0.828125  at batch 777.\n",
            "Train Loss: 1.4429636001586914    Train Acc:  0.671875  at batch 778.\n",
            "Train Loss: 1.5104639530181885    Train Acc:  0.6875  at batch 779.\n",
            "Train Loss: 1.3137009143829346    Train Acc:  0.765625  at batch 780.\n",
            "Train Loss: 1.185577630996704    Train Acc:  0.875  at batch 781.\n",
            "Valid Loss: 1.349794626235962    Valid Acc:  0.65625  at batch 0.\n",
            "Valid Loss: 1.3070365190505981    Valid Acc:  0.78125  at batch 1.\n",
            "Valid Loss: 1.3965524435043335    Valid Acc:  0.6875  at batch 2.\n",
            "Valid Loss: 1.390191912651062    Valid Acc:  0.71875  at batch 3.\n",
            "Valid Loss: 1.352728009223938    Valid Acc:  0.75  at batch 4.\n",
            "Valid Loss: 1.6122386455535889    Valid Acc:  0.5  at batch 5.\n",
            "Valid Loss: 1.4293566942214966    Valid Acc:  0.78125  at batch 6.\n",
            "Valid Loss: 1.4977995157241821    Valid Acc:  0.46875  at batch 7.\n",
            "Valid Loss: 1.336215853691101    Valid Acc:  0.84375  at batch 8.\n",
            "Valid Loss: 1.284624695777893    Valid Acc:  0.75  at batch 9.\n",
            "Valid Loss: 1.4645378589630127    Valid Acc:  0.65625  at batch 10.\n",
            "Valid Loss: 1.4587088823318481    Valid Acc:  0.59375  at batch 11.\n",
            "Valid Loss: 1.424055576324463    Valid Acc:  0.59375  at batch 12.\n",
            "Valid Loss: 1.4289056062698364    Valid Acc:  0.59375  at batch 13.\n",
            "Valid Loss: 1.2901110649108887    Valid Acc:  0.65625  at batch 14.\n",
            "Valid Loss: 1.3785793781280518    Valid Acc:  0.65625  at batch 15.\n",
            "Valid Loss: 1.4077062606811523    Valid Acc:  0.71875  at batch 16.\n",
            "Valid Loss: 1.4799473285675049    Valid Acc:  0.625  at batch 17.\n",
            "Valid Loss: 1.529213547706604    Valid Acc:  0.65625  at batch 18.\n",
            "Valid Loss: 1.4510327577590942    Valid Acc:  0.75  at batch 19.\n",
            "Valid Loss: 1.454810380935669    Valid Acc:  0.625  at batch 20.\n",
            "Valid Loss: 1.4313900470733643    Valid Acc:  0.71875  at batch 21.\n",
            "Valid Loss: 1.3704113960266113    Valid Acc:  0.75  at batch 22.\n",
            "Valid Loss: 1.3298500776290894    Valid Acc:  0.71875  at batch 23.\n",
            "Valid Loss: 1.4176057577133179    Valid Acc:  0.6875  at batch 24.\n",
            "Valid Loss: 1.2705402374267578    Valid Acc:  0.8125  at batch 25.\n",
            "Valid Loss: 1.4861708879470825    Valid Acc:  0.53125  at batch 26.\n",
            "Valid Loss: 1.4653570652008057    Valid Acc:  0.65625  at batch 27.\n",
            "Valid Loss: 1.2440656423568726    Valid Acc:  0.8125  at batch 28.\n",
            "Valid Loss: 1.3811691999435425    Valid Acc:  0.625  at batch 29.\n",
            "Valid Loss: 1.3778575658798218    Valid Acc:  0.75  at batch 30.\n",
            "Valid Loss: 1.3187874555587769    Valid Acc:  0.6875  at batch 31.\n",
            "Valid Loss: 1.3564136028289795    Valid Acc:  0.71875  at batch 32.\n",
            "Valid Loss: 1.3971434831619263    Valid Acc:  0.65625  at batch 33.\n",
            "Valid Loss: 1.4649178981781006    Valid Acc:  0.53125  at batch 34.\n",
            "Valid Loss: 1.322950005531311    Valid Acc:  0.78125  at batch 35.\n",
            "Valid Loss: 1.568066954612732    Valid Acc:  0.65625  at batch 36.\n",
            "Valid Loss: 1.5497515201568604    Valid Acc:  0.53125  at batch 37.\n",
            "Valid Loss: 1.4351658821105957    Valid Acc:  0.625  at batch 38.\n",
            "Valid Loss: 1.273012399673462    Valid Acc:  0.75  at batch 39.\n",
            "Valid Loss: 1.3164697885513306    Valid Acc:  0.71875  at batch 40.\n",
            "Valid Loss: 1.4064817428588867    Valid Acc:  0.71875  at batch 41.\n",
            "Valid Loss: 1.2989904880523682    Valid Acc:  0.75  at batch 42.\n",
            "Valid Loss: 1.328732967376709    Valid Acc:  0.65625  at batch 43.\n",
            "Valid Loss: 1.5054816007614136    Valid Acc:  0.65625  at batch 44.\n",
            "Valid Loss: 1.372239351272583    Valid Acc:  0.71875  at batch 45.\n",
            "Valid Loss: 1.3433520793914795    Valid Acc:  0.71875  at batch 46.\n",
            "Valid Loss: 1.4932329654693604    Valid Acc:  0.65625  at batch 47.\n",
            "Valid Loss: 1.4172314405441284    Valid Acc:  0.625  at batch 48.\n",
            "Valid Loss: 1.390648603439331    Valid Acc:  0.6875  at batch 49.\n",
            "Valid Loss: 1.3190252780914307    Valid Acc:  0.6875  at batch 50.\n",
            "Valid Loss: 1.395169973373413    Valid Acc:  0.6875  at batch 51.\n",
            "Valid Loss: 1.3837778568267822    Valid Acc:  0.65625  at batch 52.\n",
            "Valid Loss: 1.3560181856155396    Valid Acc:  0.65625  at batch 53.\n",
            "Valid Loss: 1.574471354484558    Valid Acc:  0.65625  at batch 54.\n",
            "Valid Loss: 1.433894395828247    Valid Acc:  0.65625  at batch 55.\n",
            "Valid Loss: 1.4748173952102661    Valid Acc:  0.71875  at batch 56.\n",
            "Valid Loss: 1.2703747749328613    Valid Acc:  0.71875  at batch 57.\n",
            "Valid Loss: 1.3319512605667114    Valid Acc:  0.71875  at batch 58.\n",
            "Valid Loss: 1.3654420375823975    Valid Acc:  0.6875  at batch 59.\n",
            "Valid Loss: 1.3481858968734741    Valid Acc:  0.75  at batch 60.\n",
            "Valid Loss: 1.450437068939209    Valid Acc:  0.625  at batch 61.\n",
            "Valid Loss: 1.3638405799865723    Valid Acc:  0.75  at batch 62.\n",
            "Valid Loss: 1.3788328170776367    Valid Acc:  0.625  at batch 63.\n",
            "Valid Loss: 1.5114775896072388    Valid Acc:  0.53125  at batch 64.\n",
            "Valid Loss: 1.3765312433242798    Valid Acc:  0.6875  at batch 65.\n",
            "Valid Loss: 1.338386058807373    Valid Acc:  0.6875  at batch 66.\n",
            "Valid Loss: 1.3685383796691895    Valid Acc:  0.625  at batch 67.\n",
            "Valid Loss: 1.577349305152893    Valid Acc:  0.59375  at batch 68.\n",
            "Valid Loss: 1.2439286708831787    Valid Acc:  0.78125  at batch 69.\n",
            "Valid Loss: 1.449459195137024    Valid Acc:  0.65625  at batch 70.\n",
            "Valid Loss: 1.2609491348266602    Valid Acc:  0.84375  at batch 71.\n",
            "Valid Loss: 1.3801227807998657    Valid Acc:  0.71875  at batch 72.\n",
            "Valid Loss: 1.2402387857437134    Valid Acc:  0.78125  at batch 73.\n",
            "Valid Loss: 1.5582489967346191    Valid Acc:  0.5625  at batch 74.\n",
            "Valid Loss: 1.199122428894043    Valid Acc:  0.71875  at batch 75.\n",
            "Valid Loss: 1.2477179765701294    Valid Acc:  0.78125  at batch 76.\n",
            "Valid Loss: 1.4134867191314697    Valid Acc:  0.65625  at batch 77.\n",
            "Valid Loss: 1.4348046779632568    Valid Acc:  0.625  at batch 78.\n",
            "Valid Loss: 1.488724946975708    Valid Acc:  0.53125  at batch 79.\n",
            "Valid Loss: 1.4625539779663086    Valid Acc:  0.65625  at batch 80.\n",
            "Valid Loss: 1.6156187057495117    Valid Acc:  0.59375  at batch 81.\n",
            "Valid Loss: 1.5503779649734497    Valid Acc:  0.59375  at batch 82.\n",
            "Valid Loss: 1.4620006084442139    Valid Acc:  0.65625  at batch 83.\n",
            "Valid Loss: 1.4247533082962036    Valid Acc:  0.75  at batch 84.\n",
            "Valid Loss: 1.4164354801177979    Valid Acc:  0.71875  at batch 85.\n",
            "Valid Loss: 1.4720146656036377    Valid Acc:  0.71875  at batch 86.\n",
            "Valid Loss: 1.3147107362747192    Valid Acc:  0.6875  at batch 87.\n",
            "Valid Loss: 1.4083412885665894    Valid Acc:  0.65625  at batch 88.\n",
            "Valid Loss: 1.4151724576950073    Valid Acc:  0.6875  at batch 89.\n",
            "Valid Loss: 1.464392066001892    Valid Acc:  0.65625  at batch 90.\n",
            "Valid Loss: 1.4890787601470947    Valid Acc:  0.5625  at batch 91.\n",
            "Valid Loss: 1.3934751749038696    Valid Acc:  0.8125  at batch 92.\n",
            "Valid Loss: 1.5579614639282227    Valid Acc:  0.5  at batch 93.\n",
            "Valid Loss: 1.4247524738311768    Valid Acc:  0.65625  at batch 94.\n",
            "Valid Loss: 1.4397122859954834    Valid Acc:  0.6875  at batch 95.\n",
            "Valid Loss: 1.4526094198226929    Valid Acc:  0.71875  at batch 96.\n",
            "Valid Loss: 1.2946752309799194    Valid Acc:  0.75  at batch 97.\n",
            "Valid Loss: 1.4574127197265625    Valid Acc:  0.65625  at batch 98.\n",
            "Valid Loss: 1.527681827545166    Valid Acc:  0.625  at batch 99.\n",
            "Valid Loss: 1.292887568473816    Valid Acc:  0.78125  at batch 100.\n",
            "Valid Loss: 1.4352984428405762    Valid Acc:  0.6875  at batch 101.\n",
            "Valid Loss: 1.2651222944259644    Valid Acc:  0.75  at batch 102.\n",
            "Valid Loss: 1.563374400138855    Valid Acc:  0.65625  at batch 103.\n",
            "Valid Loss: 1.5083353519439697    Valid Acc:  0.59375  at batch 104.\n",
            "Valid Loss: 1.4424002170562744    Valid Acc:  0.625  at batch 105.\n",
            "Valid Loss: 1.357755422592163    Valid Acc:  0.6875  at batch 106.\n",
            "Valid Loss: 1.4787293672561646    Valid Acc:  0.5625  at batch 107.\n",
            "Valid Loss: 1.5608261823654175    Valid Acc:  0.625  at batch 108.\n",
            "Valid Loss: 1.5358035564422607    Valid Acc:  0.5625  at batch 109.\n",
            "Valid Loss: 1.3648388385772705    Valid Acc:  0.6875  at batch 110.\n",
            "Valid Loss: 1.227026343345642    Valid Acc:  0.8125  at batch 111.\n",
            "Valid Loss: 1.4599835872650146    Valid Acc:  0.8125  at batch 112.\n",
            "Valid Loss: 1.5680378675460815    Valid Acc:  0.5625  at batch 113.\n",
            "Valid Loss: 1.3085174560546875    Valid Acc:  0.75  at batch 114.\n",
            "Valid Loss: 1.3031741380691528    Valid Acc:  0.75  at batch 115.\n",
            "Valid Loss: 1.461670160293579    Valid Acc:  0.5625  at batch 116.\n",
            "Valid Loss: 1.4717644453048706    Valid Acc:  0.5625  at batch 117.\n",
            "Valid Loss: 1.4047554731369019    Valid Acc:  0.59375  at batch 118.\n",
            "Valid Loss: 1.5227469205856323    Valid Acc:  0.65625  at batch 119.\n",
            "Valid Loss: 1.306107997894287    Valid Acc:  0.71875  at batch 120.\n",
            "Valid Loss: 1.4610337018966675    Valid Acc:  0.6875  at batch 121.\n",
            "Valid Loss: 1.377199411392212    Valid Acc:  0.65625  at batch 122.\n",
            "Valid Loss: 1.5229134559631348    Valid Acc:  0.5625  at batch 123.\n",
            "Valid Loss: 1.4940824508666992    Valid Acc:  0.5625  at batch 124.\n",
            "Valid Loss: 1.3644077777862549    Valid Acc:  0.625  at batch 125.\n",
            "Valid Loss: 1.3462111949920654    Valid Acc:  0.71875  at batch 126.\n",
            "Valid Loss: 1.475960612297058    Valid Acc:  0.78125  at batch 127.\n",
            "Valid Loss: 1.3594189882278442    Valid Acc:  0.8125  at batch 128.\n",
            "Valid Loss: 1.3282015323638916    Valid Acc:  0.6875  at batch 129.\n",
            "Valid Loss: 1.4401910305023193    Valid Acc:  0.65625  at batch 130.\n",
            "Valid Loss: 1.4348185062408447    Valid Acc:  0.65625  at batch 131.\n",
            "Valid Loss: 1.410196304321289    Valid Acc:  0.71875  at batch 132.\n",
            "Valid Loss: 1.3806660175323486    Valid Acc:  0.75  at batch 133.\n",
            "Valid Loss: 1.4783555269241333    Valid Acc:  0.625  at batch 134.\n",
            "Valid Loss: 1.3778094053268433    Valid Acc:  0.71875  at batch 135.\n",
            "Valid Loss: 1.4153355360031128    Valid Acc:  0.75  at batch 136.\n",
            "Valid Loss: 1.3484302759170532    Valid Acc:  0.65625  at batch 137.\n",
            "Valid Loss: 1.5647469758987427    Valid Acc:  0.5  at batch 138.\n",
            "Valid Loss: 1.3386846780776978    Valid Acc:  0.65625  at batch 139.\n",
            "Valid Loss: 1.5864335298538208    Valid Acc:  0.625  at batch 140.\n",
            "Valid Loss: 1.3962467908859253    Valid Acc:  0.65625  at batch 141.\n",
            "Valid Loss: 1.3998523950576782    Valid Acc:  0.75  at batch 142.\n",
            "Valid Loss: 1.4679540395736694    Valid Acc:  0.53125  at batch 143.\n",
            "Valid Loss: 1.3660470247268677    Valid Acc:  0.65625  at batch 144.\n",
            "Valid Loss: 1.3498939275741577    Valid Acc:  0.75  at batch 145.\n",
            "Valid Loss: 1.4687621593475342    Valid Acc:  0.625  at batch 146.\n",
            "Valid Loss: 1.60596764087677    Valid Acc:  0.625  at batch 147.\n",
            "Valid Loss: 1.4483752250671387    Valid Acc:  0.65625  at batch 148.\n",
            "Valid Loss: 1.6373815536499023    Valid Acc:  0.53125  at batch 149.\n",
            "Valid Loss: 1.3328776359558105    Valid Acc:  0.78125  at batch 150.\n",
            "Valid Loss: 1.2833812236785889    Valid Acc:  0.8125  at batch 151.\n",
            "Valid Loss: 1.4955636262893677    Valid Acc:  0.5625  at batch 152.\n",
            "Valid Loss: 1.4472968578338623    Valid Acc:  0.75  at batch 153.\n",
            "Valid Loss: 1.3064205646514893    Valid Acc:  0.75  at batch 154.\n",
            "Valid Loss: 1.5113298892974854    Valid Acc:  0.65625  at batch 155.\n",
            "Valid Loss: 1.3907005786895752    Valid Acc:  0.65625  at batch 156.\n",
            "Valid Loss: 1.3807071447372437    Valid Acc:  0.6875  at batch 157.\n",
            "Valid Loss: 1.5131415128707886    Valid Acc:  0.53125  at batch 158.\n",
            "Valid Loss: 1.4740824699401855    Valid Acc:  0.625  at batch 159.\n",
            "Valid Loss: 1.3382768630981445    Valid Acc:  0.75  at batch 160.\n",
            "Valid Loss: 1.2703150510787964    Valid Acc:  0.6875  at batch 161.\n",
            "Valid Loss: 1.638704538345337    Valid Acc:  0.5  at batch 162.\n",
            "Valid Loss: 1.2297358512878418    Valid Acc:  0.75  at batch 163.\n",
            "Valid Loss: 1.5183093547821045    Valid Acc:  0.6875  at batch 164.\n",
            "Valid Loss: 1.3493138551712036    Valid Acc:  0.71875  at batch 165.\n",
            "Valid Loss: 1.434202790260315    Valid Acc:  0.71875  at batch 166.\n",
            "Valid Loss: 1.4631708860397339    Valid Acc:  0.625  at batch 167.\n",
            "Valid Loss: 1.3470466136932373    Valid Acc:  0.75  at batch 168.\n",
            "Valid Loss: 1.3903290033340454    Valid Acc:  0.75  at batch 169.\n",
            "Valid Loss: 1.3483730554580688    Valid Acc:  0.6875  at batch 170.\n",
            "Valid Loss: 1.3488537073135376    Valid Acc:  0.75  at batch 171.\n",
            "Valid Loss: 1.3002548217773438    Valid Acc:  0.75  at batch 172.\n",
            "Valid Loss: 1.4207732677459717    Valid Acc:  0.78125  at batch 173.\n",
            "Valid Loss: 1.5770597457885742    Valid Acc:  0.46875  at batch 174.\n",
            "Valid Loss: 1.5168328285217285    Valid Acc:  0.5625  at batch 175.\n",
            "Valid Loss: 1.592390537261963    Valid Acc:  0.5  at batch 176.\n",
            "Valid Loss: 1.233270525932312    Valid Acc:  0.8125  at batch 177.\n",
            "Valid Loss: 1.4156806468963623    Valid Acc:  0.65625  at batch 178.\n",
            "Valid Loss: 1.5378135442733765    Valid Acc:  0.5625  at batch 179.\n",
            "Valid Loss: 1.4498205184936523    Valid Acc:  0.59375  at batch 180.\n",
            "Valid Loss: 1.4364488124847412    Valid Acc:  0.71875  at batch 181.\n",
            "Valid Loss: 1.5006647109985352    Valid Acc:  0.5625  at batch 182.\n",
            "Valid Loss: 1.3145372867584229    Valid Acc:  0.78125  at batch 183.\n",
            "Valid Loss: 1.4111636877059937    Valid Acc:  0.71875  at batch 184.\n",
            "Valid Loss: 1.4298593997955322    Valid Acc:  0.6875  at batch 185.\n",
            "Valid Loss: 1.375699520111084    Valid Acc:  0.71875  at batch 186.\n",
            "Valid Loss: 1.3080493211746216    Valid Acc:  0.71875  at batch 187.\n",
            "Valid Loss: 1.3428313732147217    Valid Acc:  0.75  at batch 188.\n",
            "Valid Loss: 1.6020317077636719    Valid Acc:  0.59375  at batch 189.\n",
            "Valid Loss: 1.404637098312378    Valid Acc:  0.6875  at batch 190.\n",
            "Valid Loss: 1.6024295091629028    Valid Acc:  0.5  at batch 191.\n",
            "Valid Loss: 1.4076051712036133    Valid Acc:  0.65625  at batch 192.\n",
            "Valid Loss: 1.308309555053711    Valid Acc:  0.8125  at batch 193.\n",
            "Valid Loss: 1.4773575067520142    Valid Acc:  0.65625  at batch 194.\n",
            "Valid Loss: 1.2879685163497925    Valid Acc:  0.71875  at batch 195.\n",
            "Valid Loss: 1.4485844373703003    Valid Acc:  0.625  at batch 196.\n",
            "Valid Loss: 1.5295428037643433    Valid Acc:  0.625  at batch 197.\n",
            "Valid Loss: 1.619053840637207    Valid Acc:  0.625  at batch 198.\n",
            "Valid Loss: 1.4324376583099365    Valid Acc:  0.625  at batch 199.\n",
            "Valid Loss: 1.3623557090759277    Valid Acc:  0.84375  at batch 200.\n",
            "Valid Loss: 1.5427435636520386    Valid Acc:  0.5625  at batch 201.\n",
            "Valid Loss: 1.252353549003601    Valid Acc:  0.8125  at batch 202.\n",
            "Valid Loss: 1.6086777448654175    Valid Acc:  0.59375  at batch 203.\n",
            "Valid Loss: 1.53877592086792    Valid Acc:  0.625  at batch 204.\n",
            "Valid Loss: 1.4107154607772827    Valid Acc:  0.59375  at batch 205.\n",
            "Valid Loss: 1.4072277545928955    Valid Acc:  0.71875  at batch 206.\n",
            "Valid Loss: 1.4281829595565796    Valid Acc:  0.59375  at batch 207.\n",
            "Valid Loss: 1.5122729539871216    Valid Acc:  0.5625  at batch 208.\n",
            "Valid Loss: 1.3699604272842407    Valid Acc:  0.71875  at batch 209.\n",
            "Valid Loss: 1.358841896057129    Valid Acc:  0.71875  at batch 210.\n",
            "Valid Loss: 1.4858133792877197    Valid Acc:  0.6875  at batch 211.\n",
            "Valid Loss: 1.3031411170959473    Valid Acc:  0.75  at batch 212.\n",
            "Valid Loss: 1.3933608531951904    Valid Acc:  0.71875  at batch 213.\n",
            "Valid Loss: 1.2774064540863037    Valid Acc:  0.84375  at batch 214.\n",
            "Valid Loss: 1.4766467809677124    Valid Acc:  0.625  at batch 215.\n",
            "Valid Loss: 1.4137861728668213    Valid Acc:  0.75  at batch 216.\n",
            "Valid Loss: 1.2247976064682007    Valid Acc:  0.75  at batch 217.\n",
            "Valid Loss: 1.4617893695831299    Valid Acc:  0.65625  at batch 218.\n",
            "Valid Loss: 1.5769519805908203    Valid Acc:  0.5625  at batch 219.\n",
            "Valid Loss: 1.4169890880584717    Valid Acc:  0.6875  at batch 220.\n",
            "Valid Loss: 1.4721014499664307    Valid Acc:  0.5625  at batch 221.\n",
            "Valid Loss: 1.4253664016723633    Valid Acc:  0.53125  at batch 222.\n",
            "Valid Loss: 1.418418288230896    Valid Acc:  0.65625  at batch 223.\n",
            "Valid Loss: 1.3617154359817505    Valid Acc:  0.78125  at batch 224.\n",
            "Valid Loss: 1.2750879526138306    Valid Acc:  0.65625  at batch 225.\n",
            "Valid Loss: 1.4319441318511963    Valid Acc:  0.625  at batch 226.\n",
            "Valid Loss: 1.4203661680221558    Valid Acc:  0.65625  at batch 227.\n",
            "Valid Loss: 1.386962890625    Valid Acc:  0.59375  at batch 228.\n",
            "Valid Loss: 1.4037878513336182    Valid Acc:  0.59375  at batch 229.\n",
            "Valid Loss: 1.4089256525039673    Valid Acc:  0.75  at batch 230.\n",
            "Valid Loss: 1.4808502197265625    Valid Acc:  0.625  at batch 231.\n",
            "Valid Loss: 1.4336681365966797    Valid Acc:  0.6875  at batch 232.\n",
            "Valid Loss: 1.3220378160476685    Valid Acc:  0.75  at batch 233.\n",
            "Valid Loss: 1.449243426322937    Valid Acc:  0.71875  at batch 234.\n",
            "Valid Loss: 1.3828619718551636    Valid Acc:  0.71875  at batch 235.\n",
            "Valid Loss: 1.355696678161621    Valid Acc:  0.71875  at batch 236.\n",
            "Valid Loss: 1.3242368698120117    Valid Acc:  0.8125  at batch 237.\n",
            "Valid Loss: 1.3120096921920776    Valid Acc:  0.84375  at batch 238.\n",
            "Valid Loss: 1.3511015176773071    Valid Acc:  0.78125  at batch 239.\n",
            "Valid Loss: 1.383832335472107    Valid Acc:  0.65625  at batch 240.\n",
            "Valid Loss: 1.3321821689605713    Valid Acc:  0.71875  at batch 241.\n",
            "Valid Loss: 1.5952740907669067    Valid Acc:  0.4375  at batch 242.\n",
            "Valid Loss: 1.3587543964385986    Valid Acc:  0.75  at batch 243.\n",
            "Valid Loss: 1.4320824146270752    Valid Acc:  0.6875  at batch 244.\n",
            "Valid Loss: 1.4982043504714966    Valid Acc:  0.59375  at batch 245.\n",
            "Valid Loss: 1.4245392084121704    Valid Acc:  0.75  at batch 246.\n",
            "Valid Loss: 1.4128209352493286    Valid Acc:  0.65625  at batch 247.\n",
            "Valid Loss: 1.4944854974746704    Valid Acc:  0.5625  at batch 248.\n",
            "Valid Loss: 1.5187119245529175    Valid Acc:  0.59375  at batch 249.\n",
            "Valid Loss: 1.367583990097046    Valid Acc:  0.6875  at batch 250.\n",
            "Valid Loss: 1.3564121723175049    Valid Acc:  0.6875  at batch 251.\n",
            "Valid Loss: 1.2383416891098022    Valid Acc:  0.8125  at batch 252.\n",
            "Valid Loss: 1.4597184658050537    Valid Acc:  0.625  at batch 253.\n",
            "Valid Loss: 1.4120186567306519    Valid Acc:  0.6875  at batch 254.\n",
            "Valid Loss: 1.4768095016479492    Valid Acc:  0.71875  at batch 255.\n",
            "Valid Loss: 1.3781250715255737    Valid Acc:  0.65625  at batch 256.\n",
            "Valid Loss: 1.4016562700271606    Valid Acc:  0.59375  at batch 257.\n",
            "Valid Loss: 1.4064586162567139    Valid Acc:  0.6875  at batch 258.\n",
            "Valid Loss: 1.3624733686447144    Valid Acc:  0.71875  at batch 259.\n",
            "Valid Loss: 1.3825355768203735    Valid Acc:  0.75  at batch 260.\n",
            "Valid Loss: 1.3002550601959229    Valid Acc:  0.6875  at batch 261.\n",
            "Valid Loss: 1.3763492107391357    Valid Acc:  0.65625  at batch 262.\n",
            "Valid Loss: 1.3748975992202759    Valid Acc:  0.78125  at batch 263.\n",
            "Valid Loss: 1.5382364988327026    Valid Acc:  0.625  at batch 264.\n",
            "Valid Loss: 1.3706135749816895    Valid Acc:  0.59375  at batch 265.\n",
            "Valid Loss: 1.644651174545288    Valid Acc:  0.4375  at batch 266.\n",
            "Valid Loss: 1.2686904668807983    Valid Acc:  0.78125  at batch 267.\n",
            "Valid Loss: 1.2576013803482056    Valid Acc:  0.78125  at batch 268.\n",
            "Valid Loss: 1.2079704999923706    Valid Acc:  0.8125  at batch 269.\n",
            "Valid Loss: 1.39284086227417    Valid Acc:  0.8125  at batch 270.\n",
            "Valid Loss: 1.4467018842697144    Valid Acc:  0.71875  at batch 271.\n",
            "Valid Loss: 1.378153681755066    Valid Acc:  0.75  at batch 272.\n",
            "Valid Loss: 1.3075777292251587    Valid Acc:  0.84375  at batch 273.\n",
            "Valid Loss: 1.3566691875457764    Valid Acc:  0.6875  at batch 274.\n",
            "Valid Loss: 1.4261118173599243    Valid Acc:  0.75  at batch 275.\n",
            "Valid Loss: 1.4311918020248413    Valid Acc:  0.65625  at batch 276.\n",
            "Valid Loss: 1.3402265310287476    Valid Acc:  0.75  at batch 277.\n",
            "Valid Loss: 1.3655056953430176    Valid Acc:  0.6875  at batch 278.\n",
            "Valid Loss: 1.3935363292694092    Valid Acc:  0.6875  at batch 279.\n",
            "Valid Loss: 1.203112006187439    Valid Acc:  0.8125  at batch 280.\n",
            "Valid Loss: 1.4154258966445923    Valid Acc:  0.625  at batch 281.\n",
            "Valid Loss: 1.468272089958191    Valid Acc:  0.625  at batch 282.\n",
            "Valid Loss: 1.5030088424682617    Valid Acc:  0.75  at batch 283.\n",
            "Valid Loss: 1.4770610332489014    Valid Acc:  0.71875  at batch 284.\n",
            "Valid Loss: 1.410627007484436    Valid Acc:  0.6875  at batch 285.\n",
            "Valid Loss: 1.4858427047729492    Valid Acc:  0.625  at batch 286.\n",
            "Valid Loss: 1.3172575235366821    Valid Acc:  0.71875  at batch 287.\n",
            "Valid Loss: 1.3504738807678223    Valid Acc:  0.625  at batch 288.\n",
            "Valid Loss: 1.5479772090911865    Valid Acc:  0.625  at batch 289.\n",
            "Valid Loss: 1.4733949899673462    Valid Acc:  0.625  at batch 290.\n",
            "Valid Loss: 1.3796147108078003    Valid Acc:  0.6875  at batch 291.\n",
            "Valid Loss: 1.4902597665786743    Valid Acc:  0.59375  at batch 292.\n",
            "Valid Loss: 1.5477780103683472    Valid Acc:  0.53125  at batch 293.\n",
            "Valid Loss: 1.484832763671875    Valid Acc:  0.6875  at batch 294.\n",
            "Valid Loss: 1.3660343885421753    Valid Acc:  0.71875  at batch 295.\n",
            "Valid Loss: 1.3254202604293823    Valid Acc:  0.71875  at batch 296.\n",
            "Valid Loss: 1.2372771501541138    Valid Acc:  0.78125  at batch 297.\n",
            "Valid Loss: 1.4114696979522705    Valid Acc:  0.78125  at batch 298.\n",
            "Valid Loss: 1.351216197013855    Valid Acc:  0.875  at batch 299.\n",
            "Valid Loss: 1.4086931943893433    Valid Acc:  0.78125  at batch 300.\n",
            "Valid Loss: 1.4157410860061646    Valid Acc:  0.59375  at batch 301.\n",
            "Valid Loss: 1.4290531873703003    Valid Acc:  0.65625  at batch 302.\n",
            "Valid Loss: 1.499061942100525    Valid Acc:  0.625  at batch 303.\n",
            "Valid Loss: 1.3949183225631714    Valid Acc:  0.65625  at batch 304.\n",
            "Valid Loss: 1.3461298942565918    Valid Acc:  0.75  at batch 305.\n",
            "Valid Loss: 1.4115580320358276    Valid Acc:  0.65625  at batch 306.\n",
            "Valid Loss: 1.3715182542800903    Valid Acc:  0.6875  at batch 307.\n",
            "Valid Loss: 1.3490242958068848    Valid Acc:  0.84375  at batch 308.\n",
            "Valid Loss: 1.4387447834014893    Valid Acc:  0.75  at batch 309.\n",
            "Valid Loss: 1.3442671298980713    Valid Acc:  0.75  at batch 310.\n",
            "Valid Loss: 1.448728322982788    Valid Acc:  0.78125  at batch 311.\n",
            "Valid Loss: 1.3829858303070068    Valid Acc:  0.75  at batch 312.\n",
            "Valid Loss: 1.417594075202942    Valid Acc:  0.59375  at batch 313.\n",
            "Valid Loss: 1.405665636062622    Valid Acc:  0.625  at batch 314.\n",
            "Valid Loss: 1.5005204677581787    Valid Acc:  0.625  at batch 315.\n",
            "Valid Loss: 1.384206771850586    Valid Acc:  0.6875  at batch 316.\n",
            "Valid Loss: 1.3522875308990479    Valid Acc:  0.78125  at batch 317.\n",
            "Valid Loss: 1.3786962032318115    Valid Acc:  0.625  at batch 318.\n",
            "Valid Loss: 1.3168110847473145    Valid Acc:  0.78125  at batch 319.\n",
            "Valid Loss: 1.2823565006256104    Valid Acc:  0.78125  at batch 320.\n",
            "Valid Loss: 1.4761946201324463    Valid Acc:  0.75  at batch 321.\n",
            "Valid Loss: 1.2968721389770508    Valid Acc:  0.75  at batch 322.\n",
            "Valid Loss: 1.4147378206253052    Valid Acc:  0.625  at batch 323.\n",
            "Valid Loss: 1.450715184211731    Valid Acc:  0.625  at batch 324.\n",
            "Valid Loss: 1.359001636505127    Valid Acc:  0.78125  at batch 325.\n",
            "Valid Loss: 1.2753081321716309    Valid Acc:  0.84375  at batch 326.\n",
            "Valid Loss: 1.4472181797027588    Valid Acc:  0.625  at batch 327.\n",
            "Valid Loss: 1.353510856628418    Valid Acc:  0.75  at batch 328.\n",
            "Valid Loss: 1.538496971130371    Valid Acc:  0.53125  at batch 329.\n",
            "Valid Loss: 1.378279447555542    Valid Acc:  0.71875  at batch 330.\n",
            "Valid Loss: 1.3665852546691895    Valid Acc:  0.75  at batch 331.\n",
            "Valid Loss: 1.429412603378296    Valid Acc:  0.59375  at batch 332.\n",
            "Valid Loss: 1.596269965171814    Valid Acc:  0.46875  at batch 333.\n",
            "Valid Loss: 1.330742359161377    Valid Acc:  0.71875  at batch 334.\n",
            "Valid Loss: 1.3345087766647339    Valid Acc:  0.75  at batch 335.\n",
            "Valid Loss: 1.2564746141433716    Valid Acc:  0.8125  at batch 336.\n",
            "Valid Loss: 1.3727480173110962    Valid Acc:  0.71875  at batch 337.\n",
            "Valid Loss: 1.33858323097229    Valid Acc:  0.6875  at batch 338.\n",
            "Valid Loss: 1.3798233270645142    Valid Acc:  0.71875  at batch 339.\n",
            "Valid Loss: 1.4999490976333618    Valid Acc:  0.59375  at batch 340.\n",
            "Valid Loss: 1.44826340675354    Valid Acc:  0.59375  at batch 341.\n",
            "Valid Loss: 1.356310248374939    Valid Acc:  0.75  at batch 342.\n",
            "Valid Loss: 1.3077460527420044    Valid Acc:  0.6875  at batch 343.\n",
            "Valid Loss: 1.3373268842697144    Valid Acc:  0.71875  at batch 344.\n",
            "Valid Loss: 1.474352478981018    Valid Acc:  0.71875  at batch 345.\n",
            "Valid Loss: 1.4928735494613647    Valid Acc:  0.6875  at batch 346.\n",
            "Valid Loss: 1.3150792121887207    Valid Acc:  0.65625  at batch 347.\n",
            "Valid Loss: 1.2817494869232178    Valid Acc:  0.75  at batch 348.\n",
            "Valid Loss: 1.4724454879760742    Valid Acc:  0.625  at batch 349.\n",
            "Valid Loss: 1.4055668115615845    Valid Acc:  0.71875  at batch 350.\n",
            "Valid Loss: 1.4365113973617554    Valid Acc:  0.65625  at batch 351.\n",
            "Valid Loss: 1.7148747444152832    Valid Acc:  0.40625  at batch 352.\n",
            "Valid Loss: 1.336925983428955    Valid Acc:  0.875  at batch 353.\n",
            "Valid Loss: 1.4445383548736572    Valid Acc:  0.53125  at batch 354.\n",
            "Valid Loss: 1.2745366096496582    Valid Acc:  0.75  at batch 355.\n",
            "Valid Loss: 1.3955342769622803    Valid Acc:  0.71875  at batch 356.\n",
            "Valid Loss: 1.4534573554992676    Valid Acc:  0.75  at batch 357.\n",
            "Valid Loss: 1.3958265781402588    Valid Acc:  0.6875  at batch 358.\n",
            "Valid Loss: 1.3595669269561768    Valid Acc:  0.6875  at batch 359.\n",
            "Valid Loss: 1.4192836284637451    Valid Acc:  0.6875  at batch 360.\n",
            "Valid Loss: 1.4301066398620605    Valid Acc:  0.65625  at batch 361.\n",
            "Valid Loss: 1.2937266826629639    Valid Acc:  0.90625  at batch 362.\n",
            "Valid Loss: 1.4007558822631836    Valid Acc:  0.59375  at batch 363.\n",
            "Valid Loss: 1.6507556438446045    Valid Acc:  0.59375  at batch 364.\n",
            "Valid Loss: 1.343526005744934    Valid Acc:  0.78125  at batch 365.\n",
            "Valid Loss: 1.4792979955673218    Valid Acc:  0.625  at batch 366.\n",
            "Valid Loss: 1.4297081232070923    Valid Acc:  0.6875  at batch 367.\n",
            "Valid Loss: 1.4435341358184814    Valid Acc:  0.71875  at batch 368.\n",
            "Valid Loss: 1.2523396015167236    Valid Acc:  0.78125  at batch 369.\n",
            "Valid Loss: 1.3947210311889648    Valid Acc:  0.78125  at batch 370.\n",
            "Valid Loss: 1.38835871219635    Valid Acc:  0.71875  at batch 371.\n",
            "Valid Loss: 1.153504729270935    Valid Acc:  0.75  at batch 372.\n",
            "Valid Loss: 1.3552932739257812    Valid Acc:  0.71875  at batch 373.\n",
            "Valid Loss: 1.2705435752868652    Valid Acc:  0.78125  at batch 374.\n",
            "Valid Loss: 1.4470527172088623    Valid Acc:  0.59375  at batch 375.\n",
            "Valid Loss: 1.3763569593429565    Valid Acc:  0.6875  at batch 376.\n",
            "Valid Loss: 1.4150744676589966    Valid Acc:  0.59375  at batch 377.\n",
            "Valid Loss: 1.3728479146957397    Valid Acc:  0.71875  at batch 378.\n",
            "Valid Loss: 1.5350372791290283    Valid Acc:  0.59375  at batch 379.\n",
            "Valid Loss: 1.4209015369415283    Valid Acc:  0.75  at batch 380.\n",
            "Valid Loss: 1.5678869485855103    Valid Acc:  0.625  at batch 381.\n",
            "Valid Loss: 1.3998305797576904    Valid Acc:  0.78125  at batch 382.\n",
            "Valid Loss: 1.3193471431732178    Valid Acc:  0.71875  at batch 383.\n",
            "Valid Loss: 1.3098061084747314    Valid Acc:  0.65625  at batch 384.\n",
            "Valid Loss: 1.514705777168274    Valid Acc:  0.5625  at batch 385.\n",
            "Valid Loss: 1.4407622814178467    Valid Acc:  0.625  at batch 386.\n",
            "Valid Loss: 1.3617668151855469    Valid Acc:  0.78125  at batch 387.\n",
            "Valid Loss: 1.511177659034729    Valid Acc:  0.65625  at batch 388.\n",
            "Valid Loss: 1.4547834396362305    Valid Acc:  0.5625  at batch 389.\n",
            "Valid Loss: 1.441153645515442    Valid Acc:  0.6875  at batch 390.\n",
            "Valid Loss: 1.3622405529022217    Valid Acc:  0.75  at batch 391.\n",
            "Valid Loss: 1.4172881841659546    Valid Acc:  0.75  at batch 392.\n",
            "Valid Loss: 1.4977244138717651    Valid Acc:  0.59375  at batch 393.\n",
            "Valid Loss: 1.4061131477355957    Valid Acc:  0.75  at batch 394.\n",
            "Valid Loss: 1.497318983078003    Valid Acc:  0.5  at batch 395.\n",
            "Valid Loss: 1.30671226978302    Valid Acc:  0.6875  at batch 396.\n",
            "Valid Loss: 1.5577744245529175    Valid Acc:  0.6875  at batch 397.\n",
            "Valid Loss: 1.3250113725662231    Valid Acc:  0.78125  at batch 398.\n",
            "Valid Loss: 1.3910186290740967    Valid Acc:  0.6875  at batch 399.\n",
            "Valid Loss: 1.5427889823913574    Valid Acc:  0.5625  at batch 400.\n",
            "Valid Loss: 1.4672027826309204    Valid Acc:  0.65625  at batch 401.\n",
            "Valid Loss: 1.4775100946426392    Valid Acc:  0.59375  at batch 402.\n",
            "Valid Loss: 1.4945622682571411    Valid Acc:  0.5  at batch 403.\n",
            "Valid Loss: 1.5629687309265137    Valid Acc:  0.625  at batch 404.\n",
            "Valid Loss: 1.3466191291809082    Valid Acc:  0.6875  at batch 405.\n",
            "Valid Loss: 1.5895419120788574    Valid Acc:  0.65625  at batch 406.\n",
            "Valid Loss: 1.4403527975082397    Valid Acc:  0.625  at batch 407.\n",
            "Valid Loss: 1.3327363729476929    Valid Acc:  0.78125  at batch 408.\n",
            "Valid Loss: 1.3837076425552368    Valid Acc:  0.75  at batch 409.\n",
            "Valid Loss: 1.4623737335205078    Valid Acc:  0.53125  at batch 410.\n",
            "Valid Loss: 1.3448593616485596    Valid Acc:  0.75  at batch 411.\n",
            "Valid Loss: 1.3692326545715332    Valid Acc:  0.71875  at batch 412.\n",
            "Valid Loss: 1.469253659248352    Valid Acc:  0.6875  at batch 413.\n",
            "Valid Loss: 1.3804851770401    Valid Acc:  0.71875  at batch 414.\n",
            "Valid Loss: 1.409508228302002    Valid Acc:  0.6875  at batch 415.\n",
            "Valid Loss: 1.5316367149353027    Valid Acc:  0.59375  at batch 416.\n",
            "Valid Loss: 1.333911657333374    Valid Acc:  0.8125  at batch 417.\n",
            "Valid Loss: 1.3785821199417114    Valid Acc:  0.71875  at batch 418.\n",
            "Valid Loss: 1.2747198343276978    Valid Acc:  0.75  at batch 419.\n",
            "Valid Loss: 1.2550591230392456    Valid Acc:  0.71875  at batch 420.\n",
            "Valid Loss: 1.365620732307434    Valid Acc:  0.65625  at batch 421.\n",
            "Valid Loss: 1.4363799095153809    Valid Acc:  0.65625  at batch 422.\n",
            "Valid Loss: 1.4016368389129639    Valid Acc:  0.71875  at batch 423.\n",
            "Valid Loss: 1.29465651512146    Valid Acc:  0.75  at batch 424.\n",
            "Valid Loss: 1.3343392610549927    Valid Acc:  0.65625  at batch 425.\n",
            "Valid Loss: 1.4646110534667969    Valid Acc:  0.59375  at batch 426.\n",
            "Valid Loss: 1.3983203172683716    Valid Acc:  0.625  at batch 427.\n",
            "Valid Loss: 1.4648921489715576    Valid Acc:  0.625  at batch 428.\n",
            "Valid Loss: 1.2440096139907837    Valid Acc:  0.78125  at batch 429.\n",
            "Valid Loss: 1.5720549821853638    Valid Acc:  0.5625  at batch 430.\n",
            "Valid Loss: 1.3945213556289673    Valid Acc:  0.6875  at batch 431.\n",
            "Valid Loss: 1.4014623165130615    Valid Acc:  0.6875  at batch 432.\n",
            "Valid Loss: 1.3412282466888428    Valid Acc:  0.6875  at batch 433.\n",
            "Valid Loss: 1.5097146034240723    Valid Acc:  0.5625  at batch 434.\n",
            "Valid Loss: 1.3933444023132324    Valid Acc:  0.65625  at batch 435.\n",
            "Valid Loss: 1.3521989583969116    Valid Acc:  0.8125  at batch 436.\n",
            "Valid Loss: 1.3418420553207397    Valid Acc:  0.875  at batch 437.\n",
            "Valid Loss: 1.452534794807434    Valid Acc:  0.625  at batch 438.\n",
            "Valid Loss: 1.3128403425216675    Valid Acc:  0.8125  at batch 439.\n",
            "Valid Loss: 1.4191526174545288    Valid Acc:  0.6875  at batch 440.\n",
            "Valid Loss: 1.3695162534713745    Valid Acc:  0.6875  at batch 441.\n",
            "Valid Loss: 1.357033610343933    Valid Acc:  0.71875  at batch 442.\n",
            "Valid Loss: 1.3955622911453247    Valid Acc:  0.71875  at batch 443.\n",
            "Valid Loss: 1.4574010372161865    Valid Acc:  0.71875  at batch 444.\n",
            "Valid Loss: 1.3233033418655396    Valid Acc:  0.78125  at batch 445.\n",
            "Valid Loss: 1.5285677909851074    Valid Acc:  0.59375  at batch 446.\n",
            "Valid Loss: 1.4023065567016602    Valid Acc:  0.6875  at batch 447.\n",
            "Valid Loss: 1.3155156373977661    Valid Acc:  0.65625  at batch 448.\n",
            "Valid Loss: 1.5410823822021484    Valid Acc:  0.59375  at batch 449.\n",
            "Valid Loss: 1.3584794998168945    Valid Acc:  0.78125  at batch 450.\n",
            "Valid Loss: 1.4045205116271973    Valid Acc:  0.78125  at batch 451.\n",
            "Valid Loss: 1.3752541542053223    Valid Acc:  0.625  at batch 452.\n",
            "Valid Loss: 1.2541249990463257    Valid Acc:  0.8125  at batch 453.\n",
            "Valid Loss: 1.3041616678237915    Valid Acc:  0.71875  at batch 454.\n",
            "Valid Loss: 1.4855945110321045    Valid Acc:  0.59375  at batch 455.\n",
            "Valid Loss: 1.4480607509613037    Valid Acc:  0.625  at batch 456.\n",
            "Valid Loss: 1.3595718145370483    Valid Acc:  0.625  at batch 457.\n",
            "Valid Loss: 1.491944432258606    Valid Acc:  0.6875  at batch 458.\n",
            "Valid Loss: 1.4385889768600464    Valid Acc:  0.65625  at batch 459.\n",
            "Valid Loss: 1.412765622138977    Valid Acc:  0.75  at batch 460.\n",
            "Valid Loss: 1.2896027565002441    Valid Acc:  0.75  at batch 461.\n",
            "Valid Loss: 1.328809142112732    Valid Acc:  0.8125  at batch 462.\n",
            "Valid Loss: 1.232250452041626    Valid Acc:  0.78125  at batch 463.\n",
            "Valid Loss: 1.289455771446228    Valid Acc:  0.6875  at batch 464.\n",
            "Valid Loss: 1.2689776420593262    Valid Acc:  0.78125  at batch 465.\n",
            "Valid Loss: 1.4955158233642578    Valid Acc:  0.59375  at batch 466.\n",
            "Valid Loss: 1.3384392261505127    Valid Acc:  0.5625  at batch 467.\n",
            "Valid Loss: 1.3938546180725098    Valid Acc:  0.71875  at batch 468.\n",
            "Valid Loss: 1.5523262023925781    Valid Acc:  0.5  at batch 469.\n",
            "Valid Loss: 1.373874306678772    Valid Acc:  0.65625  at batch 470.\n",
            "Valid Loss: 1.4240636825561523    Valid Acc:  0.71875  at batch 471.\n",
            "Valid Loss: 1.4009130001068115    Valid Acc:  0.6875  at batch 472.\n",
            "Valid Loss: 1.297353744506836    Valid Acc:  0.8125  at batch 473.\n",
            "Valid Loss: 1.5175628662109375    Valid Acc:  0.5625  at batch 474.\n",
            "Valid Loss: 1.3026783466339111    Valid Acc:  0.78125  at batch 475.\n",
            "Valid Loss: 1.4977390766143799    Valid Acc:  0.625  at batch 476.\n",
            "Valid Loss: 1.2270015478134155    Valid Acc:  0.84375  at batch 477.\n",
            "Valid Loss: 1.5632901191711426    Valid Acc:  0.625  at batch 478.\n",
            "Valid Loss: 1.556470513343811    Valid Acc:  0.5625  at batch 479.\n",
            "Valid Loss: 1.3160532712936401    Valid Acc:  0.71875  at batch 480.\n",
            "Valid Loss: 1.5772414207458496    Valid Acc:  0.53125  at batch 481.\n",
            "Valid Loss: 1.4469796419143677    Valid Acc:  0.6875  at batch 482.\n",
            "Valid Loss: 1.325972080230713    Valid Acc:  0.75  at batch 483.\n",
            "Valid Loss: 1.4156358242034912    Valid Acc:  0.625  at batch 484.\n",
            "Valid Loss: 1.4265810251235962    Valid Acc:  0.59375  at batch 485.\n",
            "Valid Loss: 1.4750841856002808    Valid Acc:  0.625  at batch 486.\n",
            "Valid Loss: 1.4800299406051636    Valid Acc:  0.625  at batch 487.\n",
            "Valid Loss: 1.2905653715133667    Valid Acc:  0.71875  at batch 488.\n",
            "Valid Loss: 1.5084046125411987    Valid Acc:  0.6875  at batch 489.\n",
            "Valid Loss: 1.42576003074646    Valid Acc:  0.65625  at batch 490.\n",
            "Valid Loss: 1.5189871788024902    Valid Acc:  0.5625  at batch 491.\n",
            "Valid Loss: 1.5815171003341675    Valid Acc:  0.4375  at batch 492.\n",
            "Valid Loss: 1.400978684425354    Valid Acc:  0.75  at batch 493.\n",
            "Valid Loss: 1.4110256433486938    Valid Acc:  0.6875  at batch 494.\n",
            "Valid Loss: 1.534016489982605    Valid Acc:  0.625  at batch 495.\n",
            "Valid Loss: 1.356435775756836    Valid Acc:  0.71875  at batch 496.\n",
            "Valid Loss: 1.5818285942077637    Valid Acc:  0.5  at batch 497.\n",
            "Valid Loss: 1.4030742645263672    Valid Acc:  0.6875  at batch 498.\n",
            "Valid Loss: 1.2626699209213257    Valid Acc:  0.75  at batch 499.\n",
            "Valid Loss: 1.261614441871643    Valid Acc:  0.75  at batch 500.\n",
            "Valid Loss: 1.4085073471069336    Valid Acc:  0.75  at batch 501.\n",
            "Valid Loss: 1.4691566228866577    Valid Acc:  0.65625  at batch 502.\n",
            "Valid Loss: 1.52434504032135    Valid Acc:  0.625  at batch 503.\n",
            "Valid Loss: 1.4923903942108154    Valid Acc:  0.71875  at batch 504.\n",
            "Valid Loss: 1.3191310167312622    Valid Acc:  0.75  at batch 505.\n",
            "Valid Loss: 1.4363679885864258    Valid Acc:  0.65625  at batch 506.\n",
            "Valid Loss: 1.4206122159957886    Valid Acc:  0.5625  at batch 507.\n",
            "Valid Loss: 1.5492234230041504    Valid Acc:  0.5  at batch 508.\n",
            "Valid Loss: 1.6721632480621338    Valid Acc:  0.5625  at batch 509.\n",
            "Valid Loss: 1.4362441301345825    Valid Acc:  0.625  at batch 510.\n",
            "Valid Loss: 1.4299343824386597    Valid Acc:  0.75  at batch 511.\n",
            "Valid Loss: 1.2745386362075806    Valid Acc:  0.8125  at batch 512.\n",
            "Valid Loss: 1.432565450668335    Valid Acc:  0.59375  at batch 513.\n",
            "Valid Loss: 1.4648971557617188    Valid Acc:  0.625  at batch 514.\n",
            "Valid Loss: 1.3494235277175903    Valid Acc:  0.78125  at batch 515.\n",
            "Valid Loss: 1.513688564300537    Valid Acc:  0.65625  at batch 516.\n",
            "Valid Loss: 1.3729164600372314    Valid Acc:  0.6875  at batch 517.\n",
            "Valid Loss: 1.4414029121398926    Valid Acc:  0.5625  at batch 518.\n",
            "Valid Loss: 1.524376630783081    Valid Acc:  0.53125  at batch 519.\n",
            "Valid Loss: 1.4692457914352417    Valid Acc:  0.625  at batch 520.\n",
            "Valid Loss: 1.3662441968917847    Valid Acc:  0.6875  at batch 521.\n",
            "Valid Loss: 1.3817224502563477    Valid Acc:  0.71875  at batch 522.\n",
            "Valid Loss: 1.4649832248687744    Valid Acc:  0.6875  at batch 523.\n",
            "Valid Loss: 1.415959119796753    Valid Acc:  0.65625  at batch 524.\n",
            "Valid Loss: 1.4148725271224976    Valid Acc:  0.65625  at batch 525.\n",
            "Valid Loss: 1.421954870223999    Valid Acc:  0.71875  at batch 526.\n",
            "Valid Loss: 1.3812143802642822    Valid Acc:  0.71875  at batch 527.\n",
            "Valid Loss: 1.50108003616333    Valid Acc:  0.5625  at batch 528.\n",
            "Valid Loss: 1.4006191492080688    Valid Acc:  0.78125  at batch 529.\n",
            "Valid Loss: 1.4948999881744385    Valid Acc:  0.59375  at batch 530.\n",
            "Valid Loss: 1.3842183351516724    Valid Acc:  0.75  at batch 531.\n",
            "Valid Loss: 1.4957691431045532    Valid Acc:  0.625  at batch 532.\n",
            "Valid Loss: 1.4587562084197998    Valid Acc:  0.65625  at batch 533.\n",
            "Valid Loss: 1.410801887512207    Valid Acc:  0.75  at batch 534.\n",
            "Valid Loss: 1.442849040031433    Valid Acc:  0.65625  at batch 535.\n",
            "Valid Loss: 1.428085446357727    Valid Acc:  0.5625  at batch 536.\n",
            "Valid Loss: 1.2296576499938965    Valid Acc:  0.84375  at batch 537.\n",
            "Valid Loss: 1.2893366813659668    Valid Acc:  0.6875  at batch 538.\n",
            "Valid Loss: 1.4493160247802734    Valid Acc:  0.6875  at batch 539.\n",
            "Valid Loss: 1.3169866800308228    Valid Acc:  0.75  at batch 540.\n",
            "Valid Loss: 1.5047136545181274    Valid Acc:  0.59375  at batch 541.\n",
            "Valid Loss: 1.5058695077896118    Valid Acc:  0.5625  at batch 542.\n",
            "Valid Loss: 1.4518966674804688    Valid Acc:  0.625  at batch 543.\n",
            "Valid Loss: 1.5385942459106445    Valid Acc:  0.53125  at batch 544.\n",
            "Valid Loss: 1.39622163772583    Valid Acc:  0.59375  at batch 545.\n",
            "Valid Loss: 1.4614791870117188    Valid Acc:  0.75  at batch 546.\n",
            "Valid Loss: 1.4128953218460083    Valid Acc:  0.71875  at batch 547.\n",
            "Valid Loss: 1.3867993354797363    Valid Acc:  0.75  at batch 548.\n",
            "Valid Loss: 1.5004982948303223    Valid Acc:  0.6875  at batch 549.\n",
            "Valid Loss: 1.446730375289917    Valid Acc:  0.6875  at batch 550.\n",
            "Valid Loss: 1.3913395404815674    Valid Acc:  0.71875  at batch 551.\n",
            "Valid Loss: 1.2909513711929321    Valid Acc:  0.8125  at batch 552.\n",
            "Valid Loss: 1.3528293371200562    Valid Acc:  0.75  at batch 553.\n",
            "Valid Loss: 1.433058261871338    Valid Acc:  0.65625  at batch 554.\n",
            "Valid Loss: 1.3106800317764282    Valid Acc:  0.75  at batch 555.\n",
            "Valid Loss: 1.5003430843353271    Valid Acc:  0.5625  at batch 556.\n",
            "Valid Loss: 1.4964529275894165    Valid Acc:  0.53125  at batch 557.\n",
            "Valid Loss: 1.3644862174987793    Valid Acc:  0.75  at batch 558.\n",
            "Valid Loss: 1.4133570194244385    Valid Acc:  0.75  at batch 559.\n",
            "Valid Loss: 1.366073727607727    Valid Acc:  0.71875  at batch 560.\n",
            "Valid Loss: 1.4037373065948486    Valid Acc:  0.75  at batch 561.\n",
            "Valid Loss: 1.6414272785186768    Valid Acc:  0.59375  at batch 562.\n",
            "Valid Loss: 1.4958529472351074    Valid Acc:  0.6875  at batch 563.\n",
            "Valid Loss: 1.3415417671203613    Valid Acc:  0.71875  at batch 564.\n",
            "Valid Loss: 1.3294380903244019    Valid Acc:  0.84375  at batch 565.\n",
            "Valid Loss: 1.3706512451171875    Valid Acc:  0.78125  at batch 566.\n",
            "Valid Loss: 1.2983766794204712    Valid Acc:  0.71875  at batch 567.\n",
            "Valid Loss: 1.312808871269226    Valid Acc:  0.78125  at batch 568.\n",
            "Valid Loss: 1.212378978729248    Valid Acc:  0.71875  at batch 569.\n",
            "Valid Loss: 1.3363834619522095    Valid Acc:  0.65625  at batch 570.\n",
            "Valid Loss: 1.4941176176071167    Valid Acc:  0.65625  at batch 571.\n",
            "Valid Loss: 1.2288535833358765    Valid Acc:  0.8125  at batch 572.\n",
            "Valid Loss: 1.492321491241455    Valid Acc:  0.6875  at batch 573.\n",
            "Valid Loss: 1.531843900680542    Valid Acc:  0.6875  at batch 574.\n",
            "Valid Loss: 1.393693447113037    Valid Acc:  0.625  at batch 575.\n",
            "Valid Loss: 1.337461233139038    Valid Acc:  0.6875  at batch 576.\n",
            "Valid Loss: 1.3068755865097046    Valid Acc:  0.65625  at batch 577.\n",
            "Valid Loss: 1.3635995388031006    Valid Acc:  0.78125  at batch 578.\n",
            "Valid Loss: 1.2088243961334229    Valid Acc:  0.84375  at batch 579.\n",
            "Valid Loss: 1.4828609228134155    Valid Acc:  0.65625  at batch 580.\n",
            "Valid Loss: 1.4038790464401245    Valid Acc:  0.71875  at batch 581.\n",
            "Valid Loss: 1.4168864488601685    Valid Acc:  0.625  at batch 582.\n",
            "Valid Loss: 1.3714420795440674    Valid Acc:  0.75  at batch 583.\n",
            "Valid Loss: 1.3650860786437988    Valid Acc:  0.71875  at batch 584.\n",
            "Valid Loss: 1.4759522676467896    Valid Acc:  0.6875  at batch 585.\n",
            "Valid Loss: 1.5505073070526123    Valid Acc:  0.59375  at batch 586.\n",
            "Valid Loss: 1.4341117143630981    Valid Acc:  0.71875  at batch 587.\n",
            "Valid Loss: 1.487955927848816    Valid Acc:  0.625  at batch 588.\n",
            "Valid Loss: 1.2857484817504883    Valid Acc:  0.75  at batch 589.\n",
            "Valid Loss: 1.3984296321868896    Valid Acc:  0.6875  at batch 590.\n",
            "Valid Loss: 1.3635985851287842    Valid Acc:  0.71875  at batch 591.\n",
            "Valid Loss: 1.4080289602279663    Valid Acc:  0.71875  at batch 592.\n",
            "Valid Loss: 1.3571685552597046    Valid Acc:  0.6875  at batch 593.\n",
            "Valid Loss: 1.309931755065918    Valid Acc:  0.84375  at batch 594.\n",
            "Valid Loss: 1.4990489482879639    Valid Acc:  0.59375  at batch 595.\n",
            "Valid Loss: 1.3940719366073608    Valid Acc:  0.75  at batch 596.\n",
            "Valid Loss: 1.4583667516708374    Valid Acc:  0.59375  at batch 597.\n",
            "Valid Loss: 1.3471996784210205    Valid Acc:  0.6875  at batch 598.\n",
            "Valid Loss: 1.5092523097991943    Valid Acc:  0.625  at batch 599.\n",
            "Valid Loss: 1.4633368253707886    Valid Acc:  0.6875  at batch 600.\n",
            "Valid Loss: 1.3800355195999146    Valid Acc:  0.71875  at batch 601.\n",
            "Valid Loss: 1.503422498703003    Valid Acc:  0.65625  at batch 602.\n",
            "Valid Loss: 1.5450104475021362    Valid Acc:  0.625  at batch 603.\n",
            "Valid Loss: 1.3036131858825684    Valid Acc:  0.8125  at batch 604.\n",
            "Valid Loss: 1.5488783121109009    Valid Acc:  0.65625  at batch 605.\n",
            "Valid Loss: 1.354396104812622    Valid Acc:  0.6875  at batch 606.\n",
            "Valid Loss: 1.293516993522644    Valid Acc:  0.71875  at batch 607.\n",
            "Valid Loss: 1.4645527601242065    Valid Acc:  0.71875  at batch 608.\n",
            "Valid Loss: 1.3743388652801514    Valid Acc:  0.71875  at batch 609.\n",
            "Valid Loss: 1.336413025856018    Valid Acc:  0.75  at batch 610.\n",
            "Valid Loss: 1.4269261360168457    Valid Acc:  0.78125  at batch 611.\n",
            "Valid Loss: 1.5157947540283203    Valid Acc:  0.6875  at batch 612.\n",
            "Valid Loss: 1.346724271774292    Valid Acc:  0.65625  at batch 613.\n",
            "Valid Loss: 1.440622329711914    Valid Acc:  0.5625  at batch 614.\n",
            "Valid Loss: 1.3115315437316895    Valid Acc:  0.78125  at batch 615.\n",
            "Valid Loss: 1.4199274778366089    Valid Acc:  0.59375  at batch 616.\n",
            "Valid Loss: 1.5508580207824707    Valid Acc:  0.53125  at batch 617.\n",
            "Valid Loss: 1.4217866659164429    Valid Acc:  0.78125  at batch 618.\n",
            "Valid Loss: 1.4458025693893433    Valid Acc:  0.6875  at batch 619.\n",
            "Valid Loss: 1.4470422267913818    Valid Acc:  0.5625  at batch 620.\n",
            "Valid Loss: 1.3941574096679688    Valid Acc:  0.71875  at batch 621.\n",
            "Valid Loss: 1.4859912395477295    Valid Acc:  0.71875  at batch 622.\n",
            "Valid Loss: 1.3256053924560547    Valid Acc:  0.78125  at batch 623.\n",
            "Valid Loss: 1.510522723197937    Valid Acc:  0.6875  at batch 624.\n",
            "Valid Loss: 1.5397834777832031    Valid Acc:  0.5625  at batch 625.\n",
            "Valid Loss: 1.2763721942901611    Valid Acc:  0.6875  at batch 626.\n",
            "Valid Loss: 1.4336446523666382    Valid Acc:  0.71875  at batch 627.\n",
            "Valid Loss: 1.4051895141601562    Valid Acc:  0.59375  at batch 628.\n",
            "Valid Loss: 1.3779947757720947    Valid Acc:  0.6875  at batch 629.\n",
            "Valid Loss: 1.335971713066101    Valid Acc:  0.78125  at batch 630.\n",
            "Valid Loss: 1.4256882667541504    Valid Acc:  0.78125  at batch 631.\n",
            "Valid Loss: 1.5909961462020874    Valid Acc:  0.53125  at batch 632.\n",
            "Valid Loss: 1.3679163455963135    Valid Acc:  0.59375  at batch 633.\n",
            "Valid Loss: 1.4886353015899658    Valid Acc:  0.5625  at batch 634.\n",
            "Valid Loss: 1.4696935415267944    Valid Acc:  0.625  at batch 635.\n",
            "Valid Loss: 1.3591225147247314    Valid Acc:  0.625  at batch 636.\n",
            "Valid Loss: 1.3961340188980103    Valid Acc:  0.5625  at batch 637.\n",
            "Valid Loss: 1.4258077144622803    Valid Acc:  0.625  at batch 638.\n",
            "Valid Loss: 1.5245649814605713    Valid Acc:  0.625  at batch 639.\n",
            "Valid Loss: 1.3171510696411133    Valid Acc:  0.8125  at batch 640.\n",
            "Valid Loss: 1.5794306993484497    Valid Acc:  0.5625  at batch 641.\n",
            "Valid Loss: 1.4812345504760742    Valid Acc:  0.75  at batch 642.\n",
            "Valid Loss: 1.2559187412261963    Valid Acc:  0.8125  at batch 643.\n",
            "Valid Loss: 1.4306252002716064    Valid Acc:  0.6875  at batch 644.\n",
            "Valid Loss: 1.539698600769043    Valid Acc:  0.5625  at batch 645.\n",
            "Valid Loss: 1.4718096256256104    Valid Acc:  0.75  at batch 646.\n",
            "Valid Loss: 1.339566707611084    Valid Acc:  0.75  at batch 647.\n",
            "Valid Loss: 1.520189642906189    Valid Acc:  0.5625  at batch 648.\n",
            "Valid Loss: 1.6302140951156616    Valid Acc:  0.59375  at batch 649.\n",
            "Valid Loss: 1.509185552597046    Valid Acc:  0.625  at batch 650.\n",
            "Valid Loss: 1.359179973602295    Valid Acc:  0.75  at batch 651.\n",
            "Valid Loss: 1.2203187942504883    Valid Acc:  0.8125  at batch 652.\n",
            "Valid Loss: 1.4829930067062378    Valid Acc:  0.5  at batch 653.\n",
            "Valid Loss: 1.4020925760269165    Valid Acc:  0.65625  at batch 654.\n",
            "Valid Loss: 1.5328501462936401    Valid Acc:  0.65625  at batch 655.\n",
            "Valid Loss: 1.3076809644699097    Valid Acc:  0.71875  at batch 656.\n",
            "Valid Loss: 1.2257555723190308    Valid Acc:  0.78125  at batch 657.\n",
            "Valid Loss: 1.5373927354812622    Valid Acc:  0.5625  at batch 658.\n",
            "Valid Loss: 1.3911280632019043    Valid Acc:  0.6875  at batch 659.\n",
            "Valid Loss: 1.486032247543335    Valid Acc:  0.625  at batch 660.\n",
            "Valid Loss: 1.2954456806182861    Valid Acc:  0.6875  at batch 661.\n",
            "Valid Loss: 1.32960045337677    Valid Acc:  0.84375  at batch 662.\n",
            "Valid Loss: 1.3688455820083618    Valid Acc:  0.6875  at batch 663.\n",
            "Valid Loss: 1.486798644065857    Valid Acc:  0.625  at batch 664.\n",
            "Valid Loss: 1.3937914371490479    Valid Acc:  0.625  at batch 665.\n",
            "Valid Loss: 1.4452773332595825    Valid Acc:  0.6875  at batch 666.\n",
            "Valid Loss: 1.4026278257369995    Valid Acc:  0.65625  at batch 667.\n",
            "Valid Loss: 1.501049280166626    Valid Acc:  0.78125  at batch 668.\n",
            "Valid Loss: 1.3493478298187256    Valid Acc:  0.71875  at batch 669.\n",
            "Valid Loss: 1.3963549137115479    Valid Acc:  0.625  at batch 670.\n",
            "Valid Loss: 1.5662403106689453    Valid Acc:  0.4375  at batch 671.\n",
            "Valid Loss: 1.4479843378067017    Valid Acc:  0.6875  at batch 672.\n",
            "Valid Loss: 1.3860334157943726    Valid Acc:  0.6875  at batch 673.\n",
            "Valid Loss: 1.42888343334198    Valid Acc:  0.59375  at batch 674.\n",
            "Valid Loss: 1.5179598331451416    Valid Acc:  0.4375  at batch 675.\n",
            "Valid Loss: 1.4535839557647705    Valid Acc:  0.59375  at batch 676.\n",
            "Valid Loss: 1.507605791091919    Valid Acc:  0.5  at batch 677.\n",
            "Valid Loss: 1.34165358543396    Valid Acc:  0.71875  at batch 678.\n",
            "Valid Loss: 1.394307255744934    Valid Acc:  0.65625  at batch 679.\n",
            "Valid Loss: 1.33629310131073    Valid Acc:  0.84375  at batch 680.\n",
            "Valid Loss: 1.337836742401123    Valid Acc:  0.875  at batch 681.\n",
            "Valid Loss: 1.356819987297058    Valid Acc:  0.78125  at batch 682.\n",
            "Valid Loss: 1.34626305103302    Valid Acc:  0.6875  at batch 683.\n",
            "Valid Loss: 1.4439455270767212    Valid Acc:  0.625  at batch 684.\n",
            "Valid Loss: 1.3929693698883057    Valid Acc:  0.78125  at batch 685.\n",
            "Valid Loss: 1.4510602951049805    Valid Acc:  0.53125  at batch 686.\n",
            "Valid Loss: 1.3064831495285034    Valid Acc:  0.75  at batch 687.\n",
            "Valid Loss: 1.456502914428711    Valid Acc:  0.6875  at batch 688.\n",
            "Valid Loss: 1.4093607664108276    Valid Acc:  0.6875  at batch 689.\n",
            "Valid Loss: 1.4830732345581055    Valid Acc:  0.625  at batch 690.\n",
            "Valid Loss: 1.4837732315063477    Valid Acc:  0.625  at batch 691.\n",
            "Valid Loss: 1.347123622894287    Valid Acc:  0.71875  at batch 692.\n",
            "Valid Loss: 1.5186424255371094    Valid Acc:  0.4375  at batch 693.\n",
            "Valid Loss: 1.460491418838501    Valid Acc:  0.625  at batch 694.\n",
            "Valid Loss: 1.3822873830795288    Valid Acc:  0.625  at batch 695.\n",
            "Valid Loss: 1.5730929374694824    Valid Acc:  0.46875  at batch 696.\n",
            "Valid Loss: 1.576027512550354    Valid Acc:  0.59375  at batch 697.\n",
            "Valid Loss: 1.4162707328796387    Valid Acc:  0.65625  at batch 698.\n",
            "Valid Loss: 1.5543534755706787    Valid Acc:  0.5625  at batch 699.\n",
            "Valid Loss: 1.4063022136688232    Valid Acc:  0.78125  at batch 700.\n",
            "Valid Loss: 1.4974195957183838    Valid Acc:  0.59375  at batch 701.\n",
            "Valid Loss: 1.5027635097503662    Valid Acc:  0.6875  at batch 702.\n",
            "Valid Loss: 1.6647425889968872    Valid Acc:  0.53125  at batch 703.\n",
            "Valid Loss: 1.386656641960144    Valid Acc:  0.625  at batch 704.\n",
            "Valid Loss: 1.4552793502807617    Valid Acc:  0.53125  at batch 705.\n",
            "Valid Loss: 1.4147543907165527    Valid Acc:  0.71875  at batch 706.\n",
            "Valid Loss: 1.41023850440979    Valid Acc:  0.6875  at batch 707.\n",
            "Valid Loss: 1.4117909669876099    Valid Acc:  0.625  at batch 708.\n",
            "Valid Loss: 1.3519830703735352    Valid Acc:  0.78125  at batch 709.\n",
            "Valid Loss: 1.4179702997207642    Valid Acc:  0.5625  at batch 710.\n",
            "Valid Loss: 1.1336686611175537    Valid Acc:  0.9375  at batch 711.\n",
            "Valid Loss: 1.2889540195465088    Valid Acc:  0.71875  at batch 712.\n",
            "Valid Loss: 1.4937704801559448    Valid Acc:  0.5625  at batch 713.\n",
            "Valid Loss: 1.4245432615280151    Valid Acc:  0.6875  at batch 714.\n",
            "Valid Loss: 1.3064268827438354    Valid Acc:  0.78125  at batch 715.\n",
            "Valid Loss: 1.2451512813568115    Valid Acc:  0.875  at batch 716.\n",
            "Valid Loss: 1.4702671766281128    Valid Acc:  0.71875  at batch 717.\n",
            "Valid Loss: 1.3785024881362915    Valid Acc:  0.65625  at batch 718.\n",
            "Valid Loss: 1.3103996515274048    Valid Acc:  0.75  at batch 719.\n",
            "Valid Loss: 1.4520962238311768    Valid Acc:  0.59375  at batch 720.\n",
            "Valid Loss: 1.389833688735962    Valid Acc:  0.71875  at batch 721.\n",
            "Valid Loss: 1.5224109888076782    Valid Acc:  0.71875  at batch 722.\n",
            "Valid Loss: 1.4767414331436157    Valid Acc:  0.75  at batch 723.\n",
            "Valid Loss: 1.329481601715088    Valid Acc:  0.625  at batch 724.\n",
            "Valid Loss: 1.433668851852417    Valid Acc:  0.65625  at batch 725.\n",
            "Valid Loss: 1.3018195629119873    Valid Acc:  0.8125  at batch 726.\n",
            "Valid Loss: 1.2839092016220093    Valid Acc:  0.78125  at batch 727.\n",
            "Valid Loss: 1.5202970504760742    Valid Acc:  0.625  at batch 728.\n",
            "Valid Loss: 1.3696399927139282    Valid Acc:  0.625  at batch 729.\n",
            "Valid Loss: 1.4038399457931519    Valid Acc:  0.65625  at batch 730.\n",
            "Valid Loss: 1.3862507343292236    Valid Acc:  0.6875  at batch 731.\n",
            "Valid Loss: 1.5008882284164429    Valid Acc:  0.59375  at batch 732.\n",
            "Valid Loss: 1.3704124689102173    Valid Acc:  0.78125  at batch 733.\n",
            "Valid Loss: 1.4623548984527588    Valid Acc:  0.65625  at batch 734.\n",
            "Valid Loss: 1.3564010858535767    Valid Acc:  0.78125  at batch 735.\n",
            "Valid Loss: 1.4987926483154297    Valid Acc:  0.5625  at batch 736.\n",
            "Valid Loss: 1.350095510482788    Valid Acc:  0.71875  at batch 737.\n",
            "Valid Loss: 1.3273298740386963    Valid Acc:  0.6875  at batch 738.\n",
            "Valid Loss: 1.426086664199829    Valid Acc:  0.75  at batch 739.\n",
            "Valid Loss: 1.412134051322937    Valid Acc:  0.59375  at batch 740.\n",
            "Valid Loss: 1.3477325439453125    Valid Acc:  0.6875  at batch 741.\n",
            "Valid Loss: 1.3148460388183594    Valid Acc:  0.78125  at batch 742.\n",
            "Valid Loss: 1.6710866689682007    Valid Acc:  0.4375  at batch 743.\n",
            "Valid Loss: 1.2423216104507446    Valid Acc:  0.75  at batch 744.\n",
            "Valid Loss: 1.4245944023132324    Valid Acc:  0.625  at batch 745.\n",
            "Valid Loss: 1.341176152229309    Valid Acc:  0.6875  at batch 746.\n",
            "Valid Loss: 1.425513744354248    Valid Acc:  0.625  at batch 747.\n",
            "Valid Loss: 1.457967758178711    Valid Acc:  0.625  at batch 748.\n",
            "Valid Loss: 1.4114447832107544    Valid Acc:  0.6875  at batch 749.\n",
            "Valid Loss: 1.5496572256088257    Valid Acc:  0.5625  at batch 750.\n",
            "Valid Loss: 1.4299737215042114    Valid Acc:  0.6875  at batch 751.\n",
            "Valid Loss: 1.2808775901794434    Valid Acc:  0.65625  at batch 752.\n",
            "Valid Loss: 1.4706919193267822    Valid Acc:  0.625  at batch 753.\n",
            "Valid Loss: 1.3671084642410278    Valid Acc:  0.6875  at batch 754.\n",
            "Valid Loss: 1.5637407302856445    Valid Acc:  0.5625  at batch 755.\n",
            "Valid Loss: 1.5051575899124146    Valid Acc:  0.5625  at batch 756.\n",
            "Valid Loss: 1.4318653345108032    Valid Acc:  0.75  at batch 757.\n",
            "Valid Loss: 1.2698063850402832    Valid Acc:  0.71875  at batch 758.\n",
            "Valid Loss: 1.4079210758209229    Valid Acc:  0.625  at batch 759.\n",
            "Valid Loss: 1.5601134300231934    Valid Acc:  0.46875  at batch 760.\n",
            "Valid Loss: 1.647376537322998    Valid Acc:  0.53125  at batch 761.\n",
            "Valid Loss: 1.497401237487793    Valid Acc:  0.6875  at batch 762.\n",
            "Valid Loss: 1.4473316669464111    Valid Acc:  0.5625  at batch 763.\n",
            "Valid Loss: 1.3243236541748047    Valid Acc:  0.6875  at batch 764.\n",
            "Valid Loss: 1.597359299659729    Valid Acc:  0.625  at batch 765.\n",
            "Valid Loss: 1.4871127605438232    Valid Acc:  0.625  at batch 766.\n",
            "Valid Loss: 1.178225040435791    Valid Acc:  0.84375  at batch 767.\n",
            "Valid Loss: 1.44647216796875    Valid Acc:  0.53125  at batch 768.\n",
            "Valid Loss: 1.3539559841156006    Valid Acc:  0.84375  at batch 769.\n",
            "Valid Loss: 1.4391213655471802    Valid Acc:  0.75  at batch 770.\n",
            "Valid Loss: 1.4160020351409912    Valid Acc:  0.65625  at batch 771.\n",
            "Valid Loss: 1.505548357963562    Valid Acc:  0.65625  at batch 772.\n",
            "Valid Loss: 1.4214049577713013    Valid Acc:  0.625  at batch 773.\n",
            "Valid Loss: 1.4081521034240723    Valid Acc:  0.6875  at batch 774.\n",
            "Valid Loss: 1.4634082317352295    Valid Acc:  0.59375  at batch 775.\n",
            "Valid Loss: 1.2791094779968262    Valid Acc:  0.75  at batch 776.\n",
            "Valid Loss: 1.501719355583191    Valid Acc:  0.625  at batch 777.\n",
            "Valid Loss: 1.4527766704559326    Valid Acc:  0.5  at batch 778.\n",
            "Valid Loss: 1.4489803314208984    Valid Acc:  0.625  at batch 779.\n",
            "Valid Loss: 1.463655948638916    Valid Acc:  0.65625  at batch 780.\n",
            "Valid Loss: 1.3286285400390625    Valid Acc:  0.65625  at batch 781.\n",
            "Valid Loss: 1.2994729280471802    Valid Acc:  0.78125  at batch 782.\n",
            "Valid Loss: 1.439395785331726    Valid Acc:  0.78125  at batch 783.\n",
            "Valid Loss: 1.4203686714172363    Valid Acc:  0.71875  at batch 784.\n",
            "Valid Loss: 1.3735039234161377    Valid Acc:  0.71875  at batch 785.\n",
            "Valid Loss: 1.2743937969207764    Valid Acc:  0.8125  at batch 786.\n",
            "Valid Loss: 1.3919346332550049    Valid Acc:  0.6875  at batch 787.\n",
            "Valid Loss: 1.4724023342132568    Valid Acc:  0.59375  at batch 788.\n",
            "Valid Loss: 1.371251106262207    Valid Acc:  0.75  at batch 789.\n",
            "Valid Loss: 1.3202712535858154    Valid Acc:  0.75  at batch 790.\n",
            "Valid Loss: 1.4244147539138794    Valid Acc:  0.8125  at batch 791.\n",
            "Valid Loss: 1.3191967010498047    Valid Acc:  0.71875  at batch 792.\n",
            "Valid Loss: 1.4546817541122437    Valid Acc:  0.65625  at batch 793.\n",
            "Valid Loss: 1.5416789054870605    Valid Acc:  0.59375  at batch 794.\n",
            "Valid Loss: 1.6211141347885132    Valid Acc:  0.59375  at batch 795.\n",
            "Valid Loss: 1.2878799438476562    Valid Acc:  0.59375  at batch 796.\n",
            "Valid Loss: 1.333558440208435    Valid Acc:  0.90625  at batch 797.\n",
            "Valid Loss: 1.3446640968322754    Valid Acc:  0.8125  at batch 798.\n",
            "Valid Loss: 1.535380482673645    Valid Acc:  0.5625  at batch 799.\n",
            "Valid Loss: 1.3667325973510742    Valid Acc:  0.65625  at batch 800.\n",
            "Valid Loss: 1.313636064529419    Valid Acc:  0.71875  at batch 801.\n",
            "Valid Loss: 1.3149291276931763    Valid Acc:  0.75  at batch 802.\n",
            "Valid Loss: 1.368333101272583    Valid Acc:  0.65625  at batch 803.\n",
            "Valid Loss: 1.3918510675430298    Valid Acc:  0.65625  at batch 804.\n",
            "Valid Loss: 1.4394934177398682    Valid Acc:  0.6875  at batch 805.\n",
            "Valid Loss: 1.358707308769226    Valid Acc:  0.6875  at batch 806.\n",
            "Valid Loss: 1.367797613143921    Valid Acc:  0.75  at batch 807.\n",
            "Valid Loss: 1.4787509441375732    Valid Acc:  0.5625  at batch 808.\n",
            "Valid Loss: 1.5533876419067383    Valid Acc:  0.5625  at batch 809.\n",
            "Valid Loss: 1.6106288433074951    Valid Acc:  0.625  at batch 810.\n",
            "Valid Loss: 1.4533257484436035    Valid Acc:  0.65625  at batch 811.\n",
            "Valid Loss: 1.202866554260254    Valid Acc:  0.75  at batch 812.\n",
            "Valid Loss: 1.391470193862915    Valid Acc:  0.6875  at batch 813.\n",
            "Valid Loss: 1.3922815322875977    Valid Acc:  0.65625  at batch 814.\n",
            "Valid Loss: 1.4889352321624756    Valid Acc:  0.5625  at batch 815.\n",
            "Valid Loss: 1.3352737426757812    Valid Acc:  0.78125  at batch 816.\n",
            "Valid Loss: 1.4771817922592163    Valid Acc:  0.71875  at batch 817.\n",
            "Valid Loss: 1.27691650390625    Valid Acc:  0.65625  at batch 818.\n",
            "Valid Loss: 1.3634305000305176    Valid Acc:  0.6875  at batch 819.\n",
            "Valid Loss: 1.5778224468231201    Valid Acc:  0.53125  at batch 820.\n",
            "Valid Loss: 1.5991928577423096    Valid Acc:  0.5  at batch 821.\n",
            "Valid Loss: 1.3502238988876343    Valid Acc:  0.71875  at batch 822.\n",
            "Valid Loss: 1.4143335819244385    Valid Acc:  0.6875  at batch 823.\n",
            "Valid Loss: 1.3824679851531982    Valid Acc:  0.5625  at batch 824.\n",
            "Valid Loss: 1.3809123039245605    Valid Acc:  0.71875  at batch 825.\n",
            "Valid Loss: 1.292881965637207    Valid Acc:  0.75  at batch 826.\n",
            "Valid Loss: 1.3743321895599365    Valid Acc:  0.625  at batch 827.\n",
            "Valid Loss: 1.3657026290893555    Valid Acc:  0.75  at batch 828.\n",
            "Valid Loss: 1.3862251043319702    Valid Acc:  0.71875  at batch 829.\n",
            "Valid Loss: 1.5045714378356934    Valid Acc:  0.59375  at batch 830.\n",
            "Valid Loss: 1.299353837966919    Valid Acc:  0.84375  at batch 831.\n",
            "Valid Loss: 1.4625306129455566    Valid Acc:  0.6875  at batch 832.\n",
            "Valid Loss: 1.4219120740890503    Valid Acc:  0.65625  at batch 833.\n",
            "Valid Loss: 1.4019476175308228    Valid Acc:  0.6875  at batch 834.\n",
            "Valid Loss: 1.3602268695831299    Valid Acc:  0.6875  at batch 835.\n",
            "Valid Loss: 1.3919283151626587    Valid Acc:  0.84375  at batch 836.\n",
            "Valid Loss: 1.4007705450057983    Valid Acc:  0.65625  at batch 837.\n",
            "Valid Loss: 1.3565306663513184    Valid Acc:  0.59375  at batch 838.\n",
            "Valid Loss: 1.4690489768981934    Valid Acc:  0.75  at batch 839.\n",
            "Valid Loss: 1.4111329317092896    Valid Acc:  0.65625  at batch 840.\n",
            "Valid Loss: 1.4031034708023071    Valid Acc:  0.59375  at batch 841.\n",
            "Valid Loss: 1.28337824344635    Valid Acc:  0.8125  at batch 842.\n",
            "Valid Loss: 1.5512011051177979    Valid Acc:  0.59375  at batch 843.\n",
            "Valid Loss: 1.62398362159729    Valid Acc:  0.40625  at batch 844.\n",
            "Valid Loss: 1.5272488594055176    Valid Acc:  0.59375  at batch 845.\n",
            "Valid Loss: 1.3521742820739746    Valid Acc:  0.78125  at batch 846.\n",
            "Valid Loss: 1.4679510593414307    Valid Acc:  0.625  at batch 847.\n",
            "Valid Loss: 1.24204421043396    Valid Acc:  0.8125  at batch 848.\n",
            "Valid Loss: 1.4026505947113037    Valid Acc:  0.625  at batch 849.\n",
            "Valid Loss: 1.4045072793960571    Valid Acc:  0.625  at batch 850.\n",
            "Valid Loss: 1.2985254526138306    Valid Acc:  0.8125  at batch 851.\n",
            "Valid Loss: 1.5341506004333496    Valid Acc:  0.53125  at batch 852.\n",
            "Valid Loss: 1.2968701124191284    Valid Acc:  0.6875  at batch 853.\n",
            "Valid Loss: 1.359296441078186    Valid Acc:  0.6875  at batch 854.\n",
            "Valid Loss: 1.347962498664856    Valid Acc:  0.6875  at batch 855.\n",
            "Valid Loss: 1.4746655225753784    Valid Acc:  0.59375  at batch 856.\n",
            "Valid Loss: 1.5728988647460938    Valid Acc:  0.40625  at batch 857.\n",
            "Valid Loss: 1.4373146295547485    Valid Acc:  0.59375  at batch 858.\n",
            "Valid Loss: 1.5672358274459839    Valid Acc:  0.59375  at batch 859.\n",
            "Valid Loss: 1.4609073400497437    Valid Acc:  0.6875  at batch 860.\n",
            "Valid Loss: 1.4515414237976074    Valid Acc:  0.625  at batch 861.\n",
            "Valid Loss: 1.289913535118103    Valid Acc:  0.71875  at batch 862.\n",
            "Valid Loss: 1.32648503780365    Valid Acc:  0.8125  at batch 863.\n",
            "Valid Loss: 1.512755274772644    Valid Acc:  0.5625  at batch 864.\n",
            "Valid Loss: 1.3440678119659424    Valid Acc:  0.75  at batch 865.\n",
            "Valid Loss: 1.484367847442627    Valid Acc:  0.625  at batch 866.\n",
            "Valid Loss: 1.3272062540054321    Valid Acc:  0.8125  at batch 867.\n",
            "Valid Loss: 1.6444157361984253    Valid Acc:  0.5625  at batch 868.\n",
            "Valid Loss: 1.2760764360427856    Valid Acc:  0.8125  at batch 869.\n",
            "Valid Loss: 1.1275476217269897    Valid Acc:  0.8125  at batch 870.\n",
            "Valid Loss: 1.4696894884109497    Valid Acc:  0.6875  at batch 871.\n",
            "Valid Loss: 1.4530616998672485    Valid Acc:  0.59375  at batch 872.\n",
            "Valid Loss: 1.594271183013916    Valid Acc:  0.5625  at batch 873.\n",
            "Valid Loss: 1.6447526216506958    Valid Acc:  0.5625  at batch 874.\n",
            "Valid Loss: 1.4477511644363403    Valid Acc:  0.65625  at batch 875.\n",
            "Valid Loss: 1.2291252613067627    Valid Acc:  0.75  at batch 876.\n",
            "Valid Loss: 1.4320708513259888    Valid Acc:  0.625  at batch 877.\n",
            "Valid Loss: 1.3918567895889282    Valid Acc:  0.75  at batch 878.\n",
            "Valid Loss: 1.4430744647979736    Valid Acc:  0.6875  at batch 879.\n",
            "Valid Loss: 1.5160821676254272    Valid Acc:  0.75  at batch 880.\n",
            "Valid Loss: 1.492652177810669    Valid Acc:  0.6875  at batch 881.\n",
            "Valid Loss: 1.6232678890228271    Valid Acc:  0.5  at batch 882.\n",
            "Valid Loss: 1.4145886898040771    Valid Acc:  0.65625  at batch 883.\n",
            "Valid Loss: 1.36147141456604    Valid Acc:  0.8125  at batch 884.\n",
            "Valid Loss: 1.4758981466293335    Valid Acc:  0.5625  at batch 885.\n",
            "Valid Loss: 1.3715373277664185    Valid Acc:  0.75  at batch 886.\n",
            "Valid Loss: 1.3451087474822998    Valid Acc:  0.78125  at batch 887.\n",
            "Valid Loss: 1.2808926105499268    Valid Acc:  0.78125  at batch 888.\n",
            "Valid Loss: 1.3453609943389893    Valid Acc:  0.78125  at batch 889.\n",
            "Valid Loss: 1.3987116813659668    Valid Acc:  0.71875  at batch 890.\n",
            "Valid Loss: 1.3882639408111572    Valid Acc:  0.6875  at batch 891.\n",
            "Valid Loss: 1.4351935386657715    Valid Acc:  0.6875  at batch 892.\n",
            "Valid Loss: 1.3658971786499023    Valid Acc:  0.71875  at batch 893.\n",
            "Valid Loss: 1.544001579284668    Valid Acc:  0.6875  at batch 894.\n",
            "Valid Loss: 1.2764214277267456    Valid Acc:  0.78125  at batch 895.\n",
            "Valid Loss: 1.3472455739974976    Valid Acc:  0.71875  at batch 896.\n",
            "Valid Loss: 1.2948219776153564    Valid Acc:  0.75  at batch 897.\n",
            "Valid Loss: 1.4824079275131226    Valid Acc:  0.65625  at batch 898.\n",
            "Valid Loss: 1.445633053779602    Valid Acc:  0.5  at batch 899.\n",
            "Valid Loss: 1.2916229963302612    Valid Acc:  0.6875  at batch 900.\n",
            "Valid Loss: 1.5495661497116089    Valid Acc:  0.5625  at batch 901.\n",
            "Valid Loss: 1.3552311658859253    Valid Acc:  0.6875  at batch 902.\n",
            "Valid Loss: 1.353825330734253    Valid Acc:  0.75  at batch 903.\n",
            "Valid Loss: 1.2680481672286987    Valid Acc:  0.84375  at batch 904.\n",
            "Valid Loss: 1.4302458763122559    Valid Acc:  0.625  at batch 905.\n",
            "Valid Loss: 1.4407459497451782    Valid Acc:  0.6875  at batch 906.\n",
            "Valid Loss: 1.3804082870483398    Valid Acc:  0.6875  at batch 907.\n",
            "Valid Loss: 1.4951449632644653    Valid Acc:  0.625  at batch 908.\n",
            "Valid Loss: 1.3023216724395752    Valid Acc:  0.8125  at batch 909.\n",
            "Valid Loss: 1.3267637491226196    Valid Acc:  0.65625  at batch 910.\n",
            "Valid Loss: 1.5241150856018066    Valid Acc:  0.65625  at batch 911.\n",
            "Valid Loss: 1.28941810131073    Valid Acc:  0.75  at batch 912.\n",
            "Valid Loss: 1.6473312377929688    Valid Acc:  0.5  at batch 913.\n",
            "Valid Loss: 1.3907309770584106    Valid Acc:  0.6875  at batch 914.\n",
            "Valid Loss: 1.6014232635498047    Valid Acc:  0.65625  at batch 915.\n",
            "Valid Loss: 1.4762310981750488    Valid Acc:  0.5625  at batch 916.\n",
            "Valid Loss: 1.435802698135376    Valid Acc:  0.71875  at batch 917.\n",
            "Valid Loss: 1.342923641204834    Valid Acc:  0.78125  at batch 918.\n",
            "Valid Loss: 1.4540694952011108    Valid Acc:  0.6875  at batch 919.\n",
            "Valid Loss: 1.419484257698059    Valid Acc:  0.71875  at batch 920.\n",
            "Valid Loss: 1.4031249284744263    Valid Acc:  0.6875  at batch 921.\n",
            "Valid Loss: 1.312314510345459    Valid Acc:  0.75  at batch 922.\n",
            "Valid Loss: 1.4682022333145142    Valid Acc:  0.53125  at batch 923.\n",
            "Valid Loss: 1.3821296691894531    Valid Acc:  0.65625  at batch 924.\n",
            "Valid Loss: 1.532915472984314    Valid Acc:  0.46875  at batch 925.\n",
            "Valid Loss: 1.4225245714187622    Valid Acc:  0.65625  at batch 926.\n",
            "Valid Loss: 1.523750901222229    Valid Acc:  0.5625  at batch 927.\n",
            "Valid Loss: 1.4661122560501099    Valid Acc:  0.71875  at batch 928.\n",
            "Valid Loss: 1.2612253427505493    Valid Acc:  0.71875  at batch 929.\n",
            "Valid Loss: 1.5219132900238037    Valid Acc:  0.59375  at batch 930.\n",
            "Valid Loss: 1.3434959650039673    Valid Acc:  0.71875  at batch 931.\n",
            "Valid Loss: 1.4031999111175537    Valid Acc:  0.65625  at batch 932.\n",
            "Valid Loss: 1.565447211265564    Valid Acc:  0.59375  at batch 933.\n",
            "Valid Loss: 1.536134123802185    Valid Acc:  0.5625  at batch 934.\n",
            "Valid Loss: 1.4036667346954346    Valid Acc:  0.6875  at batch 935.\n",
            "Valid Loss: 1.5559251308441162    Valid Acc:  0.53125  at batch 936.\n",
            "Valid Loss: 1.5544480085372925    Valid Acc:  0.5  at batch 937.\n",
            "Valid Loss: 1.3909633159637451    Valid Acc:  0.71875  at batch 938.\n",
            "Valid Loss: 1.339097023010254    Valid Acc:  0.75  at batch 939.\n",
            "Valid Loss: 1.4206929206848145    Valid Acc:  0.59375  at batch 940.\n",
            "Valid Loss: 1.3307385444641113    Valid Acc:  0.625  at batch 941.\n",
            "Valid Loss: 1.424258828163147    Valid Acc:  0.65625  at batch 942.\n",
            "Valid Loss: 1.5241947174072266    Valid Acc:  0.59375  at batch 943.\n",
            "Valid Loss: 1.5301977396011353    Valid Acc:  0.625  at batch 944.\n",
            "Valid Loss: 1.4986281394958496    Valid Acc:  0.5  at batch 945.\n",
            "Valid Loss: 1.4371379613876343    Valid Acc:  0.625  at batch 946.\n",
            "Valid Loss: 1.5244280099868774    Valid Acc:  0.59375  at batch 947.\n",
            "Valid Loss: 1.6053071022033691    Valid Acc:  0.5625  at batch 948.\n",
            "Valid Loss: 1.374155879020691    Valid Acc:  0.75  at batch 949.\n",
            "Valid Loss: 1.3281619548797607    Valid Acc:  0.6875  at batch 950.\n",
            "Valid Loss: 1.5853701829910278    Valid Acc:  0.5  at batch 951.\n",
            "Valid Loss: 1.4811829328536987    Valid Acc:  0.5625  at batch 952.\n",
            "Valid Loss: 1.366077184677124    Valid Acc:  0.71875  at batch 953.\n",
            "Valid Loss: 1.3591887950897217    Valid Acc:  0.65625  at batch 954.\n",
            "Valid Loss: 1.6114481687545776    Valid Acc:  0.59375  at batch 955.\n",
            "Valid Loss: 1.491686224937439    Valid Acc:  0.625  at batch 956.\n",
            "Valid Loss: 1.4377727508544922    Valid Acc:  0.6875  at batch 957.\n",
            "Valid Loss: 1.4163779020309448    Valid Acc:  0.625  at batch 958.\n",
            "Valid Loss: 1.3405340909957886    Valid Acc:  0.75  at batch 959.\n",
            "Valid Loss: 1.4538880586624146    Valid Acc:  0.6875  at batch 960.\n",
            "Valid Loss: 1.5288622379302979    Valid Acc:  0.5  at batch 961.\n",
            "Valid Loss: 1.3868850469589233    Valid Acc:  0.65625  at batch 962.\n",
            "Valid Loss: 1.4344823360443115    Valid Acc:  0.78125  at batch 963.\n",
            "Valid Loss: 1.661224603652954    Valid Acc:  0.5  at batch 964.\n",
            "Valid Loss: 1.3244038820266724    Valid Acc:  0.71875  at batch 965.\n",
            "Valid Loss: 1.4387282133102417    Valid Acc:  0.6875  at batch 966.\n",
            "Valid Loss: 1.522023320198059    Valid Acc:  0.46875  at batch 967.\n",
            "Valid Loss: 1.4089181423187256    Valid Acc:  0.65625  at batch 968.\n",
            "Valid Loss: 1.3379836082458496    Valid Acc:  0.6875  at batch 969.\n",
            "Valid Loss: 1.4370150566101074    Valid Acc:  0.59375  at batch 970.\n",
            "Valid Loss: 1.5046356916427612    Valid Acc:  0.6875  at batch 971.\n",
            "Valid Loss: 1.4118348360061646    Valid Acc:  0.71875  at batch 972.\n",
            "Valid Loss: 1.4727129936218262    Valid Acc:  0.5625  at batch 973.\n",
            "Valid Loss: 1.389230489730835    Valid Acc:  0.6875  at batch 974.\n",
            "Valid Loss: 1.4399511814117432    Valid Acc:  0.6875  at batch 975.\n",
            "Valid Loss: 1.4397553205490112    Valid Acc:  0.78125  at batch 976.\n",
            "Valid Loss: 1.4224673509597778    Valid Acc:  0.71875  at batch 977.\n",
            "Valid Loss: 1.4304598569869995    Valid Acc:  0.625  at batch 978.\n",
            "Valid Loss: 1.3111138343811035    Valid Acc:  0.71875  at batch 979.\n",
            "Valid Loss: 1.400511622428894    Valid Acc:  0.71875  at batch 980.\n",
            "Valid Loss: 1.430719256401062    Valid Acc:  0.6875  at batch 981.\n",
            "Valid Loss: 1.2401310205459595    Valid Acc:  0.75  at batch 982.\n",
            "Valid Loss: 1.4418675899505615    Valid Acc:  0.6875  at batch 983.\n",
            "Valid Loss: 1.308350920677185    Valid Acc:  0.78125  at batch 984.\n",
            "Valid Loss: 1.3007745742797852    Valid Acc:  0.6875  at batch 985.\n",
            "Valid Loss: 1.3159140348434448    Valid Acc:  0.65625  at batch 986.\n",
            "Valid Loss: 1.3848472833633423    Valid Acc:  0.6875  at batch 987.\n",
            "Valid Loss: 1.3160866498947144    Valid Acc:  0.84375  at batch 988.\n",
            "Valid Loss: 1.4327601194381714    Valid Acc:  0.625  at batch 989.\n",
            "Valid Loss: 1.3908332586288452    Valid Acc:  0.75  at batch 990.\n",
            "Valid Loss: 1.4095045328140259    Valid Acc:  0.71875  at batch 991.\n",
            "Valid Loss: 1.4925556182861328    Valid Acc:  0.59375  at batch 992.\n",
            "Valid Loss: 1.4663587808609009    Valid Acc:  0.6875  at batch 993.\n",
            "Valid Loss: 1.415716528892517    Valid Acc:  0.65625  at batch 994.\n",
            "Valid Loss: 1.3449692726135254    Valid Acc:  0.75  at batch 995.\n",
            "Valid Loss: 1.4085066318511963    Valid Acc:  0.65625  at batch 996.\n",
            "Valid Loss: 1.3394478559494019    Valid Acc:  0.8125  at batch 997.\n",
            "Valid Loss: 1.5279772281646729    Valid Acc:  0.625  at batch 998.\n",
            "Valid Loss: 1.3415758609771729    Valid Acc:  0.78125  at batch 999.\n",
            "Valid Loss: 1.5024973154067993    Valid Acc:  0.5625  at batch 1000.\n",
            "Valid Loss: 1.4381370544433594    Valid Acc:  0.59375  at batch 1001.\n",
            "Valid Loss: 1.4103082418441772    Valid Acc:  0.65625  at batch 1002.\n",
            "Valid Loss: 1.4909477233886719    Valid Acc:  0.75  at batch 1003.\n",
            "Valid Loss: 1.4927783012390137    Valid Acc:  0.65625  at batch 1004.\n",
            "Valid Loss: 1.4549564123153687    Valid Acc:  0.6875  at batch 1005.\n",
            "Valid Loss: 1.3174073696136475    Valid Acc:  0.65625  at batch 1006.\n",
            "Valid Loss: 1.4311238527297974    Valid Acc:  0.65625  at batch 1007.\n",
            "Valid Loss: 1.3858020305633545    Valid Acc:  0.65625  at batch 1008.\n",
            "Valid Loss: 1.4602742195129395    Valid Acc:  0.59375  at batch 1009.\n",
            "Valid Loss: 1.312058448791504    Valid Acc:  0.6875  at batch 1010.\n",
            "Valid Loss: 1.3858932256698608    Valid Acc:  0.65625  at batch 1011.\n",
            "Valid Loss: 1.4173301458358765    Valid Acc:  0.625  at batch 1012.\n",
            "Valid Loss: 1.4295094013214111    Valid Acc:  0.71875  at batch 1013.\n",
            "Valid Loss: 1.3976165056228638    Valid Acc:  0.625  at batch 1014.\n",
            "Valid Loss: 1.4446979761123657    Valid Acc:  0.5625  at batch 1015.\n",
            "Valid Loss: 1.517610788345337    Valid Acc:  0.59375  at batch 1016.\n",
            "Valid Loss: 1.4348750114440918    Valid Acc:  0.6875  at batch 1017.\n",
            "Valid Loss: 1.5856536626815796    Valid Acc:  0.53125  at batch 1018.\n",
            "Valid Loss: 1.3055222034454346    Valid Acc:  0.8125  at batch 1019.\n",
            "Valid Loss: 1.2607828378677368    Valid Acc:  0.78125  at batch 1020.\n",
            "Valid Loss: 1.515687346458435    Valid Acc:  0.65625  at batch 1021.\n",
            "Valid Loss: 1.4023631811141968    Valid Acc:  0.59375  at batch 1022.\n",
            "Valid Loss: 1.3160046339035034    Valid Acc:  0.6875  at batch 1023.\n",
            "Valid Loss: 1.4775984287261963    Valid Acc:  0.65625  at batch 1024.\n",
            "Valid Loss: 1.5112802982330322    Valid Acc:  0.53125  at batch 1025.\n",
            "Valid Loss: 1.4567347764968872    Valid Acc:  0.625  at batch 1026.\n",
            "Valid Loss: 1.461402177810669    Valid Acc:  0.65625  at batch 1027.\n",
            "Valid Loss: 1.4122809171676636    Valid Acc:  0.65625  at batch 1028.\n",
            "Valid Loss: 1.4559776782989502    Valid Acc:  0.625  at batch 1029.\n",
            "Valid Loss: 1.464068055152893    Valid Acc:  0.6875  at batch 1030.\n",
            "Valid Loss: 1.3643274307250977    Valid Acc:  0.65625  at batch 1031.\n",
            "Valid Loss: 1.4581279754638672    Valid Acc:  0.6875  at batch 1032.\n",
            "Valid Loss: 1.429604411125183    Valid Acc:  0.75  at batch 1033.\n",
            "Valid Loss: 1.3519259691238403    Valid Acc:  0.6875  at batch 1034.\n",
            "Valid Loss: 1.3503122329711914    Valid Acc:  0.75  at batch 1035.\n",
            "Valid Loss: 1.4665261507034302    Valid Acc:  0.65625  at batch 1036.\n",
            "Valid Loss: 1.4878469705581665    Valid Acc:  0.71875  at batch 1037.\n",
            "Valid Loss: 1.503948450088501    Valid Acc:  0.53125  at batch 1038.\n",
            "Valid Loss: 1.2504078149795532    Valid Acc:  0.8125  at batch 1039.\n",
            "Valid Loss: 1.4329955577850342    Valid Acc:  0.6875  at batch 1040.\n",
            "Valid Loss: 1.3912187814712524    Valid Acc:  0.84375  at batch 1041.\n",
            "Valid Loss: 1.470064640045166    Valid Acc:  0.65625  at batch 1042.\n",
            "Valid Loss: 1.4857499599456787    Valid Acc:  0.6875  at batch 1043.\n",
            "Valid Loss: 1.4605127573013306    Valid Acc:  0.625  at batch 1044.\n",
            "Valid Loss: 1.4500435590744019    Valid Acc:  0.6875  at batch 1045.\n",
            "Valid Loss: 1.4117590188980103    Valid Acc:  0.75  at batch 1046.\n",
            "Valid Loss: 1.3847687244415283    Valid Acc:  0.6875  at batch 1047.\n",
            "Valid Loss: 1.4703282117843628    Valid Acc:  0.71875  at batch 1048.\n",
            "Valid Loss: 1.3987852334976196    Valid Acc:  0.625  at batch 1049.\n",
            "Valid Loss: 1.3567276000976562    Valid Acc:  0.71875  at batch 1050.\n",
            "Valid Loss: 1.4890315532684326    Valid Acc:  0.59375  at batch 1051.\n",
            "Valid Loss: 1.3648467063903809    Valid Acc:  0.75  at batch 1052.\n",
            "Valid Loss: 1.4316047430038452    Valid Acc:  0.625  at batch 1053.\n",
            "Valid Loss: 1.3632535934448242    Valid Acc:  0.59375  at batch 1054.\n",
            "Valid Loss: 1.3830175399780273    Valid Acc:  0.6875  at batch 1055.\n",
            "Valid Loss: 1.3029032945632935    Valid Acc:  0.8125  at batch 1056.\n",
            "Valid Loss: 1.4200645685195923    Valid Acc:  0.78125  at batch 1057.\n",
            "Valid Loss: 1.3102352619171143    Valid Acc:  0.65625  at batch 1058.\n",
            "Valid Loss: 1.4558353424072266    Valid Acc:  0.625  at batch 1059.\n",
            "Valid Loss: 1.375842571258545    Valid Acc:  0.75  at batch 1060.\n",
            "Valid Loss: 1.4145957231521606    Valid Acc:  0.65625  at batch 1061.\n",
            "Valid Loss: 1.417815089225769    Valid Acc:  0.6875  at batch 1062.\n",
            "Valid Loss: 1.437875747680664    Valid Acc:  0.625  at batch 1063.\n",
            "Valid Loss: 1.4930121898651123    Valid Acc:  0.5625  at batch 1064.\n",
            "Valid Loss: 1.3678288459777832    Valid Acc:  0.6875  at batch 1065.\n",
            "Valid Loss: 1.4560259580612183    Valid Acc:  0.5625  at batch 1066.\n",
            "Valid Loss: 1.4874025583267212    Valid Acc:  0.625  at batch 1067.\n",
            "Valid Loss: 1.4036225080490112    Valid Acc:  0.65625  at batch 1068.\n",
            "Valid Loss: 1.4243273735046387    Valid Acc:  0.65625  at batch 1069.\n",
            "Valid Loss: 1.4512012004852295    Valid Acc:  0.75  at batch 1070.\n",
            "Valid Loss: 1.5157443284988403    Valid Acc:  0.5625  at batch 1071.\n",
            "Valid Loss: 1.4540637731552124    Valid Acc:  0.59375  at batch 1072.\n",
            "Valid Loss: 1.4916000366210938    Valid Acc:  0.65625  at batch 1073.\n",
            "Valid Loss: 1.4231457710266113    Valid Acc:  0.71875  at batch 1074.\n",
            "Valid Loss: 1.420124888420105    Valid Acc:  0.6875  at batch 1075.\n",
            "Valid Loss: 1.3515129089355469    Valid Acc:  0.625  at batch 1076.\n",
            "Valid Loss: 1.2523359060287476    Valid Acc:  0.78125  at batch 1077.\n",
            "Valid Loss: 1.4627037048339844    Valid Acc:  0.59375  at batch 1078.\n",
            "Valid Loss: 1.4660300016403198    Valid Acc:  0.71875  at batch 1079.\n",
            "Valid Loss: 1.5235207080841064    Valid Acc:  0.5625  at batch 1080.\n",
            "Valid Loss: 1.544121503829956    Valid Acc:  0.65625  at batch 1081.\n",
            "Valid Loss: 1.3894314765930176    Valid Acc:  0.65625  at batch 1082.\n",
            "Valid Loss: 1.2360759973526    Valid Acc:  0.8125  at batch 1083.\n",
            "Valid Loss: 1.5270459651947021    Valid Acc:  0.59375  at batch 1084.\n",
            "Valid Loss: 1.4969980716705322    Valid Acc:  0.5625  at batch 1085.\n",
            "Valid Loss: 1.5309250354766846    Valid Acc:  0.65625  at batch 1086.\n",
            "Valid Loss: 1.3955650329589844    Valid Acc:  0.6875  at batch 1087.\n",
            "Valid Loss: 1.373607873916626    Valid Acc:  0.75  at batch 1088.\n",
            "Valid Loss: 1.4563082456588745    Valid Acc:  0.6875  at batch 1089.\n",
            "Valid Loss: 1.5781192779541016    Valid Acc:  0.53125  at batch 1090.\n",
            "Valid Loss: 1.3583033084869385    Valid Acc:  0.78125  at batch 1091.\n",
            "Valid Loss: 1.41001296043396    Valid Acc:  0.6875  at batch 1092.\n",
            "Valid Loss: 1.2395110130310059    Valid Acc:  0.71875  at batch 1093.\n",
            "Valid Loss: 1.4202487468719482    Valid Acc:  0.71875  at batch 1094.\n",
            "Valid Loss: 1.2993031740188599    Valid Acc:  0.75  at batch 1095.\n",
            "Valid Loss: 1.5053832530975342    Valid Acc:  0.6875  at batch 1096.\n",
            "Valid Loss: 1.4020864963531494    Valid Acc:  0.6875  at batch 1097.\n",
            "Valid Loss: 1.4510562419891357    Valid Acc:  0.6875  at batch 1098.\n",
            "Valid Loss: 1.4686686992645264    Valid Acc:  0.71875  at batch 1099.\n",
            "Valid Loss: 1.3559775352478027    Valid Acc:  0.71875  at batch 1100.\n",
            "Valid Loss: 1.4298807382583618    Valid Acc:  0.65625  at batch 1101.\n",
            "Valid Loss: 1.4801502227783203    Valid Acc:  0.65625  at batch 1102.\n",
            "Valid Loss: 1.3834389448165894    Valid Acc:  0.71875  at batch 1103.\n",
            "Valid Loss: 1.3790500164031982    Valid Acc:  0.6875  at batch 1104.\n",
            "Valid Loss: 1.3140870332717896    Valid Acc:  0.78125  at batch 1105.\n",
            "Valid Loss: 1.4047839641571045    Valid Acc:  0.75  at batch 1106.\n",
            "Valid Loss: 1.3796595335006714    Valid Acc:  0.625  at batch 1107.\n",
            "Valid Loss: 1.6784553527832031    Valid Acc:  0.53125  at batch 1108.\n",
            "Valid Loss: 1.5078184604644775    Valid Acc:  0.5625  at batch 1109.\n",
            "Valid Loss: 1.3347243070602417    Valid Acc:  0.78125  at batch 1110.\n",
            "Valid Loss: 1.5591422319412231    Valid Acc:  0.6875  at batch 1111.\n",
            "Valid Loss: 1.480260968208313    Valid Acc:  0.625  at batch 1112.\n",
            "Valid Loss: 1.3808013200759888    Valid Acc:  0.78125  at batch 1113.\n",
            "Valid Loss: 1.4164882898330688    Valid Acc:  0.65625  at batch 1114.\n",
            "Valid Loss: 1.6039074659347534    Valid Acc:  0.65625  at batch 1115.\n",
            "Valid Loss: 1.3389887809753418    Valid Acc:  0.71875  at batch 1116.\n",
            "Valid Loss: 1.481149673461914    Valid Acc:  0.59375  at batch 1117.\n",
            "Valid Loss: 1.306686282157898    Valid Acc:  0.8125  at batch 1118.\n",
            "Valid Loss: 1.429544448852539    Valid Acc:  0.625  at batch 1119.\n",
            "Valid Loss: 1.6112595796585083    Valid Acc:  0.46875  at batch 1120.\n",
            "Valid Loss: 1.1947225332260132    Valid Acc:  0.90625  at batch 1121.\n",
            "Valid Loss: 1.319839596748352    Valid Acc:  0.71875  at batch 1122.\n",
            "Valid Loss: 1.4428369998931885    Valid Acc:  0.65625  at batch 1123.\n",
            "Valid Loss: 1.4291958808898926    Valid Acc:  0.6875  at batch 1124.\n",
            "Valid Loss: 1.4825196266174316    Valid Acc:  0.59375  at batch 1125.\n",
            "Valid Loss: 1.4432011842727661    Valid Acc:  0.59375  at batch 1126.\n",
            "Valid Loss: 1.4982638359069824    Valid Acc:  0.59375  at batch 1127.\n",
            "Valid Loss: 1.330284833908081    Valid Acc:  0.71875  at batch 1128.\n",
            "Valid Loss: 1.277573823928833    Valid Acc:  0.8125  at batch 1129.\n",
            "Valid Loss: 1.4812601804733276    Valid Acc:  0.71875  at batch 1130.\n",
            "Valid Loss: 1.4453496932983398    Valid Acc:  0.625  at batch 1131.\n",
            "Valid Loss: 1.4418590068817139    Valid Acc:  0.65625  at batch 1132.\n",
            "Valid Loss: 1.3225764036178589    Valid Acc:  0.78125  at batch 1133.\n",
            "Valid Loss: 1.446527123451233    Valid Acc:  0.625  at batch 1134.\n",
            "Valid Loss: 1.4297583103179932    Valid Acc:  0.53125  at batch 1135.\n",
            "Valid Loss: 1.2070215940475464    Valid Acc:  0.71875  at batch 1136.\n",
            "Valid Loss: 1.337656021118164    Valid Acc:  0.78125  at batch 1137.\n",
            "Valid Loss: 1.3666867017745972    Valid Acc:  0.71875  at batch 1138.\n",
            "Valid Loss: 1.5606504678726196    Valid Acc:  0.59375  at batch 1139.\n",
            "Valid Loss: 1.373816728591919    Valid Acc:  0.65625  at batch 1140.\n",
            "Valid Loss: 1.3382833003997803    Valid Acc:  0.75  at batch 1141.\n",
            "Valid Loss: 1.437160611152649    Valid Acc:  0.625  at batch 1142.\n",
            "Valid Loss: 1.2865126132965088    Valid Acc:  0.78125  at batch 1143.\n",
            "Valid Loss: 1.4765092134475708    Valid Acc:  0.6875  at batch 1144.\n",
            "Valid Loss: 1.412843108177185    Valid Acc:  0.71875  at batch 1145.\n",
            "Valid Loss: 1.413129210472107    Valid Acc:  0.71875  at batch 1146.\n",
            "Valid Loss: 1.3824951648712158    Valid Acc:  0.6875  at batch 1147.\n",
            "Valid Loss: 1.3497370481491089    Valid Acc:  0.6875  at batch 1148.\n",
            "Valid Loss: 1.4973092079162598    Valid Acc:  0.59375  at batch 1149.\n",
            "Valid Loss: 1.4581043720245361    Valid Acc:  0.6875  at batch 1150.\n",
            "Valid Loss: 1.4943281412124634    Valid Acc:  0.6875  at batch 1151.\n",
            "Valid Loss: 1.419982671737671    Valid Acc:  0.625  at batch 1152.\n",
            "Valid Loss: 1.2580889463424683    Valid Acc:  0.75  at batch 1153.\n",
            "Valid Loss: 1.511924147605896    Valid Acc:  0.625  at batch 1154.\n",
            "Valid Loss: 1.3746918439865112    Valid Acc:  0.75  at batch 1155.\n",
            "Valid Loss: 1.5266547203063965    Valid Acc:  0.53125  at batch 1156.\n",
            "Valid Loss: 1.4295929670333862    Valid Acc:  0.65625  at batch 1157.\n",
            "Valid Loss: 1.5146310329437256    Valid Acc:  0.65625  at batch 1158.\n",
            "Valid Loss: 1.534728765487671    Valid Acc:  0.625  at batch 1159.\n",
            "Valid Loss: 1.449020266532898    Valid Acc:  0.5625  at batch 1160.\n",
            "Valid Loss: 1.6355185508728027    Valid Acc:  0.4375  at batch 1161.\n",
            "Valid Loss: 1.5025078058242798    Valid Acc:  0.625  at batch 1162.\n",
            "Valid Loss: 1.4569011926651    Valid Acc:  0.59375  at batch 1163.\n",
            "Valid Loss: 1.3287075757980347    Valid Acc:  0.6875  at batch 1164.\n",
            "Valid Loss: 1.3973610401153564    Valid Acc:  0.71875  at batch 1165.\n",
            "Valid Loss: 1.3798555135726929    Valid Acc:  0.6875  at batch 1166.\n",
            "Valid Loss: 1.4075359106063843    Valid Acc:  0.71875  at batch 1167.\n",
            "Valid Loss: 1.3746974468231201    Valid Acc:  0.6875  at batch 1168.\n",
            "Valid Loss: 1.30856192111969    Valid Acc:  0.6875  at batch 1169.\n",
            "Valid Loss: 1.4006482362747192    Valid Acc:  0.625  at batch 1170.\n",
            "Valid Loss: 1.4157967567443848    Valid Acc:  0.65625  at batch 1171.\n",
            "Valid Loss: 1.524918556213379    Valid Acc:  0.5  at batch 1172.\n",
            "Valid Loss: 1.424331545829773    Valid Acc:  0.59375  at batch 1173.\n",
            "Valid Loss: 1.2818560600280762    Valid Acc:  0.75  at batch 1174.\n",
            "Valid Loss: 1.469702959060669    Valid Acc:  0.65625  at batch 1175.\n",
            "Valid Loss: 1.38212251663208    Valid Acc:  0.625  at batch 1176.\n",
            "Valid Loss: 1.4334228038787842    Valid Acc:  0.6875  at batch 1177.\n",
            "Valid Loss: 1.5488075017929077    Valid Acc:  0.6875  at batch 1178.\n",
            "Valid Loss: 1.5320883989334106    Valid Acc:  0.59375  at batch 1179.\n",
            "Valid Loss: 1.450954794883728    Valid Acc:  0.625  at batch 1180.\n",
            "Valid Loss: 1.362583875656128    Valid Acc:  0.71875  at batch 1181.\n",
            "Valid Loss: 1.2986854314804077    Valid Acc:  0.8125  at batch 1182.\n",
            "Valid Loss: 1.4432284832000732    Valid Acc:  0.65625  at batch 1183.\n",
            "Valid Loss: 1.4105325937271118    Valid Acc:  0.71875  at batch 1184.\n",
            "Valid Loss: 1.4426676034927368    Valid Acc:  0.71875  at batch 1185.\n",
            "Valid Loss: 1.477540135383606    Valid Acc:  0.625  at batch 1186.\n",
            "Valid Loss: 1.3453309535980225    Valid Acc:  0.6875  at batch 1187.\n",
            "Valid Loss: 1.337599277496338    Valid Acc:  0.75  at batch 1188.\n",
            "Valid Loss: 1.3543468713760376    Valid Acc:  0.6875  at batch 1189.\n",
            "Valid Loss: 1.4408725500106812    Valid Acc:  0.59375  at batch 1190.\n",
            "Valid Loss: 1.423639178276062    Valid Acc:  0.71875  at batch 1191.\n",
            "Valid Loss: 1.3826894760131836    Valid Acc:  0.71875  at batch 1192.\n",
            "Valid Loss: 1.317784309387207    Valid Acc:  0.71875  at batch 1193.\n",
            "Valid Loss: 1.420079231262207    Valid Acc:  0.6875  at batch 1194.\n",
            "Valid Loss: 1.371848702430725    Valid Acc:  0.78125  at batch 1195.\n",
            "Valid Loss: 1.3006094694137573    Valid Acc:  0.78125  at batch 1196.\n",
            "Valid Loss: 1.4241095781326294    Valid Acc:  0.6875  at batch 1197.\n",
            "Valid Loss: 1.5980674028396606    Valid Acc:  0.59375  at batch 1198.\n",
            "Valid Loss: 1.411706805229187    Valid Acc:  0.625  at batch 1199.\n",
            "Valid Loss: 1.47508704662323    Valid Acc:  0.6875  at batch 1200.\n",
            "Valid Loss: 1.3285027742385864    Valid Acc:  0.8125  at batch 1201.\n",
            "Valid Loss: 1.4074063301086426    Valid Acc:  0.6875  at batch 1202.\n",
            "Valid Loss: 1.362908959388733    Valid Acc:  0.65625  at batch 1203.\n",
            "Valid Loss: 1.2719745635986328    Valid Acc:  0.71875  at batch 1204.\n",
            "Valid Loss: 1.3286396265029907    Valid Acc:  0.75  at batch 1205.\n",
            "Valid Loss: 1.437545895576477    Valid Acc:  0.6875  at batch 1206.\n",
            "Valid Loss: 1.2809009552001953    Valid Acc:  0.78125  at batch 1207.\n",
            "Valid Loss: 1.5651780366897583    Valid Acc:  0.53125  at batch 1208.\n",
            "Valid Loss: 1.2958539724349976    Valid Acc:  0.71875  at batch 1209.\n",
            "Valid Loss: 1.3014488220214844    Valid Acc:  0.78125  at batch 1210.\n",
            "Valid Loss: 1.3746705055236816    Valid Acc:  0.71875  at batch 1211.\n",
            "Valid Loss: 1.5264017581939697    Valid Acc:  0.625  at batch 1212.\n",
            "Valid Loss: 1.5370965003967285    Valid Acc:  0.5625  at batch 1213.\n",
            "Valid Loss: 1.4480810165405273    Valid Acc:  0.65625  at batch 1214.\n",
            "Valid Loss: 1.412662386894226    Valid Acc:  0.65625  at batch 1215.\n",
            "Valid Loss: 1.3396953344345093    Valid Acc:  0.6875  at batch 1216.\n",
            "Valid Loss: 1.2561242580413818    Valid Acc:  0.78125  at batch 1217.\n",
            "Valid Loss: 1.3455719947814941    Valid Acc:  0.75  at batch 1218.\n",
            "Valid Loss: 1.4698227643966675    Valid Acc:  0.65625  at batch 1219.\n",
            "Valid Loss: 1.1932004690170288    Valid Acc:  0.84375  at batch 1220.\n",
            "Valid Loss: 1.4187793731689453    Valid Acc:  0.6875  at batch 1221.\n",
            "Valid Loss: 1.513598918914795    Valid Acc:  0.75  at batch 1222.\n",
            "Valid Loss: 1.336325764656067    Valid Acc:  0.6875  at batch 1223.\n",
            "Valid Loss: 1.472901701927185    Valid Acc:  0.6875  at batch 1224.\n",
            "Valid Loss: 1.345279335975647    Valid Acc:  0.65625  at batch 1225.\n",
            "Valid Loss: 1.3239874839782715    Valid Acc:  0.75  at batch 1226.\n",
            "Valid Loss: 1.3661783933639526    Valid Acc:  0.65625  at batch 1227.\n",
            "Valid Loss: 1.2703913450241089    Valid Acc:  0.75  at batch 1228.\n",
            "Valid Loss: 1.4074872732162476    Valid Acc:  0.6875  at batch 1229.\n",
            "Valid Loss: 1.419958472251892    Valid Acc:  0.6875  at batch 1230.\n",
            "Valid Loss: 1.3924506902694702    Valid Acc:  0.625  at batch 1231.\n",
            "Valid Loss: 1.4469304084777832    Valid Acc:  0.59375  at batch 1232.\n",
            "Valid Loss: 1.3220927715301514    Valid Acc:  0.8125  at batch 1233.\n",
            "Valid Loss: 1.3146388530731201    Valid Acc:  0.75  at batch 1234.\n",
            "Valid Loss: 1.6378114223480225    Valid Acc:  0.5625  at batch 1235.\n",
            "Valid Loss: 1.4191815853118896    Valid Acc:  0.71875  at batch 1236.\n",
            "Valid Loss: 1.3267275094985962    Valid Acc:  0.8125  at batch 1237.\n",
            "Valid Loss: 1.4463624954223633    Valid Acc:  0.59375  at batch 1238.\n",
            "Valid Loss: 1.3653980493545532    Valid Acc:  0.625  at batch 1239.\n",
            "Valid Loss: 1.4428303241729736    Valid Acc:  0.5625  at batch 1240.\n",
            "Valid Loss: 1.3361175060272217    Valid Acc:  0.8125  at batch 1241.\n",
            "Valid Loss: 1.3442902565002441    Valid Acc:  0.71875  at batch 1242.\n",
            "Valid Loss: 1.526018500328064    Valid Acc:  0.5625  at batch 1243.\n",
            "Valid Loss: 1.4733206033706665    Valid Acc:  0.6875  at batch 1244.\n",
            "Valid Loss: 1.3464337587356567    Valid Acc:  0.75  at batch 1245.\n",
            "Valid Loss: 1.3269097805023193    Valid Acc:  0.75  at batch 1246.\n",
            "Valid Loss: 1.280792474746704    Valid Acc:  0.90625  at batch 1247.\n",
            "Valid Loss: 1.3252038955688477    Valid Acc:  0.75  at batch 1248.\n",
            "Valid Loss: 1.4665056467056274    Valid Acc:  0.625  at batch 1249.\n",
            "Valid Loss: 1.6104129552841187    Valid Acc:  0.53125  at batch 1250.\n",
            "Valid Loss: 1.4188023805618286    Valid Acc:  0.71875  at batch 1251.\n",
            "Valid Loss: 1.3990846872329712    Valid Acc:  0.625  at batch 1252.\n",
            "Valid Loss: 1.3555158376693726    Valid Acc:  0.71875  at batch 1253.\n",
            "Valid Loss: 1.4316892623901367    Valid Acc:  0.6875  at batch 1254.\n",
            "Valid Loss: 1.399410367012024    Valid Acc:  0.8125  at batch 1255.\n",
            "Valid Loss: 1.3183283805847168    Valid Acc:  0.65625  at batch 1256.\n",
            "Valid Loss: 1.3729100227355957    Valid Acc:  0.5625  at batch 1257.\n",
            "Valid Loss: 1.5510761737823486    Valid Acc:  0.625  at batch 1258.\n",
            "Valid Loss: 1.615126371383667    Valid Acc:  0.5  at batch 1259.\n",
            "Valid Loss: 1.3899123668670654    Valid Acc:  0.6875  at batch 1260.\n",
            "Valid Loss: 1.4249166250228882    Valid Acc:  0.65625  at batch 1261.\n",
            "Valid Loss: 1.4253277778625488    Valid Acc:  0.65625  at batch 1262.\n",
            "Valid Loss: 1.6678606271743774    Valid Acc:  0.53125  at batch 1263.\n",
            "Valid Loss: 1.4581643342971802    Valid Acc:  0.71875  at batch 1264.\n",
            "Valid Loss: 1.3393399715423584    Valid Acc:  0.65625  at batch 1265.\n",
            "Valid Loss: 1.5083688497543335    Valid Acc:  0.65625  at batch 1266.\n",
            "Valid Loss: 1.3411887884140015    Valid Acc:  0.71875  at batch 1267.\n",
            "Valid Loss: 1.2341835498809814    Valid Acc:  0.75  at batch 1268.\n",
            "Valid Loss: 1.4131025075912476    Valid Acc:  0.8125  at batch 1269.\n",
            "Valid Loss: 1.2537912130355835    Valid Acc:  0.65625  at batch 1270.\n",
            "Valid Loss: 1.407547950744629    Valid Acc:  0.65625  at batch 1271.\n",
            "Valid Loss: 1.3083828687667847    Valid Acc:  0.78125  at batch 1272.\n",
            "Valid Loss: 1.4256802797317505    Valid Acc:  0.71875  at batch 1273.\n",
            "Valid Loss: 1.3614026308059692    Valid Acc:  0.71875  at batch 1274.\n",
            "Valid Loss: 1.40916907787323    Valid Acc:  0.53125  at batch 1275.\n",
            "Valid Loss: 1.3615748882293701    Valid Acc:  0.75  at batch 1276.\n",
            "Valid Loss: 1.3838037252426147    Valid Acc:  0.59375  at batch 1277.\n",
            "Valid Loss: 1.4442946910858154    Valid Acc:  0.75  at batch 1278.\n",
            "Valid Loss: 1.4377779960632324    Valid Acc:  0.65625  at batch 1279.\n",
            "Valid Loss: 1.3990428447723389    Valid Acc:  0.8125  at batch 1280.\n",
            "Valid Loss: 1.411303997039795    Valid Acc:  0.75  at batch 1281.\n",
            "Valid Loss: 1.342882752418518    Valid Acc:  0.75  at batch 1282.\n",
            "Valid Loss: 1.4737259149551392    Valid Acc:  0.625  at batch 1283.\n",
            "Valid Loss: 1.2796108722686768    Valid Acc:  0.6875  at batch 1284.\n",
            "Valid Loss: 1.3457112312316895    Valid Acc:  0.6875  at batch 1285.\n",
            "Valid Loss: 1.5041013956069946    Valid Acc:  0.625  at batch 1286.\n",
            "Valid Loss: 1.395660638809204    Valid Acc:  0.75  at batch 1287.\n",
            "Valid Loss: 1.4221888780593872    Valid Acc:  0.625  at batch 1288.\n",
            "Valid Loss: 1.444586992263794    Valid Acc:  0.65625  at batch 1289.\n",
            "Valid Loss: 1.4212640523910522    Valid Acc:  0.65625  at batch 1290.\n",
            "Valid Loss: 1.2435171604156494    Valid Acc:  0.75  at batch 1291.\n",
            "Valid Loss: 1.4440066814422607    Valid Acc:  0.71875  at batch 1292.\n",
            "Valid Loss: 1.347074270248413    Valid Acc:  0.75  at batch 1293.\n",
            "Valid Loss: 1.3464823961257935    Valid Acc:  0.75  at batch 1294.\n",
            "Valid Loss: 1.4783751964569092    Valid Acc:  0.65625  at batch 1295.\n",
            "Valid Loss: 1.4740393161773682    Valid Acc:  0.5625  at batch 1296.\n",
            "Valid Loss: 1.4597505331039429    Valid Acc:  0.65625  at batch 1297.\n",
            "Valid Loss: 1.3632527589797974    Valid Acc:  0.75  at batch 1298.\n",
            "Valid Loss: 1.479159951210022    Valid Acc:  0.625  at batch 1299.\n",
            "Valid Loss: 1.4784436225891113    Valid Acc:  0.5  at batch 1300.\n",
            "Valid Loss: 1.3471566438674927    Valid Acc:  0.65625  at batch 1301.\n",
            "Valid Loss: 1.378488540649414    Valid Acc:  0.71875  at batch 1302.\n",
            "Valid Loss: 1.3152058124542236    Valid Acc:  0.78125  at batch 1303.\n",
            "Valid Loss: 1.4856433868408203    Valid Acc:  0.65625  at batch 1304.\n",
            "Valid Loss: 1.2834739685058594    Valid Acc:  0.75  at batch 1305.\n",
            "Valid Loss: 1.3758058547973633    Valid Acc:  0.625  at batch 1306.\n",
            "Valid Loss: 1.406739592552185    Valid Acc:  0.6875  at batch 1307.\n",
            "Valid Loss: 1.3243019580841064    Valid Acc:  0.6875  at batch 1308.\n",
            "Valid Loss: 1.3970104455947876    Valid Acc:  0.71875  at batch 1309.\n",
            "Valid Loss: 1.438259482383728    Valid Acc:  0.65625  at batch 1310.\n",
            "Valid Loss: 1.335667610168457    Valid Acc:  0.75  at batch 1311.\n",
            "Valid Loss: 1.473556637763977    Valid Acc:  0.6875  at batch 1312.\n",
            "Valid Loss: 1.4259198904037476    Valid Acc:  0.75  at batch 1313.\n",
            "Valid Loss: 1.32574462890625    Valid Acc:  0.75  at batch 1314.\n",
            "Valid Loss: 1.421634554862976    Valid Acc:  0.71875  at batch 1315.\n",
            "Valid Loss: 1.4406474828720093    Valid Acc:  0.59375  at batch 1316.\n",
            "Valid Loss: 1.3653687238693237    Valid Acc:  0.59375  at batch 1317.\n",
            "Valid Loss: 1.340783953666687    Valid Acc:  0.75  at batch 1318.\n",
            "Valid Loss: 1.3940709829330444    Valid Acc:  0.75  at batch 1319.\n",
            "Valid Loss: 1.4898091554641724    Valid Acc:  0.59375  at batch 1320.\n",
            "Valid Loss: 1.3186354637145996    Valid Acc:  0.75  at batch 1321.\n",
            "Valid Loss: 1.3990494012832642    Valid Acc:  0.6875  at batch 1322.\n",
            "Valid Loss: 1.5590797662734985    Valid Acc:  0.5625  at batch 1323.\n",
            "Valid Loss: 1.3125667572021484    Valid Acc:  0.6875  at batch 1324.\n",
            "Valid Loss: 1.5019396543502808    Valid Acc:  0.59375  at batch 1325.\n",
            "Valid Loss: 1.5545823574066162    Valid Acc:  0.5625  at batch 1326.\n",
            "Valid Loss: 1.4028493165969849    Valid Acc:  0.625  at batch 1327.\n",
            "Valid Loss: 1.3653123378753662    Valid Acc:  0.6875  at batch 1328.\n",
            "Valid Loss: 1.4634884595870972    Valid Acc:  0.65625  at batch 1329.\n",
            "Valid Loss: 1.4692593812942505    Valid Acc:  0.6875  at batch 1330.\n",
            "Valid Loss: 1.5104384422302246    Valid Acc:  0.625  at batch 1331.\n",
            "Valid Loss: 1.228666067123413    Valid Acc:  0.90625  at batch 1332.\n",
            "Valid Loss: 1.254274845123291    Valid Acc:  0.71875  at batch 1333.\n",
            "Valid Loss: 1.5318198204040527    Valid Acc:  0.625  at batch 1334.\n",
            "Valid Loss: 1.436252236366272    Valid Acc:  0.6875  at batch 1335.\n",
            "Valid Loss: 1.353635549545288    Valid Acc:  0.6875  at batch 1336.\n",
            "Valid Loss: 1.5227887630462646    Valid Acc:  0.59375  at batch 1337.\n",
            "Valid Loss: 1.482022762298584    Valid Acc:  0.59375  at batch 1338.\n",
            "Valid Loss: 1.3359688520431519    Valid Acc:  0.78125  at batch 1339.\n",
            "Valid Loss: 1.4291763305664062    Valid Acc:  0.71875  at batch 1340.\n",
            "Valid Loss: 1.401050090789795    Valid Acc:  0.75  at batch 1341.\n",
            "Valid Loss: 1.3676328659057617    Valid Acc:  0.6875  at batch 1342.\n",
            "Valid Loss: 1.4693958759307861    Valid Acc:  0.59375  at batch 1343.\n",
            "Valid Loss: 1.3827072381973267    Valid Acc:  0.6875  at batch 1344.\n",
            "Valid Loss: 1.4169114828109741    Valid Acc:  0.5625  at batch 1345.\n",
            "Valid Loss: 1.3141603469848633    Valid Acc:  0.8125  at batch 1346.\n",
            "Valid Loss: 1.4963806867599487    Valid Acc:  0.59375  at batch 1347.\n",
            "Valid Loss: 1.469019889831543    Valid Acc:  0.5625  at batch 1348.\n",
            "Valid Loss: 1.3832453489303589    Valid Acc:  0.71875  at batch 1349.\n",
            "Valid Loss: 1.4385284185409546    Valid Acc:  0.65625  at batch 1350.\n",
            "Valid Loss: 1.2538464069366455    Valid Acc:  0.71875  at batch 1351.\n",
            "Valid Loss: 1.5066367387771606    Valid Acc:  0.65625  at batch 1352.\n",
            "Valid Loss: 1.4200773239135742    Valid Acc:  0.6875  at batch 1353.\n",
            "Valid Loss: 1.4086121320724487    Valid Acc:  0.75  at batch 1354.\n",
            "Valid Loss: 1.4147052764892578    Valid Acc:  0.65625  at batch 1355.\n",
            "Valid Loss: 1.4183193445205688    Valid Acc:  0.6875  at batch 1356.\n",
            "Valid Loss: 1.4422566890716553    Valid Acc:  0.59375  at batch 1357.\n",
            "Valid Loss: 1.2081985473632812    Valid Acc:  0.84375  at batch 1358.\n",
            "Valid Loss: 1.488737940788269    Valid Acc:  0.71875  at batch 1359.\n",
            "Valid Loss: 1.2403509616851807    Valid Acc:  0.78125  at batch 1360.\n",
            "Valid Loss: 1.401496171951294    Valid Acc:  0.75  at batch 1361.\n",
            "Valid Loss: 1.5210437774658203    Valid Acc:  0.65625  at batch 1362.\n",
            "Valid Loss: 1.3967936038970947    Valid Acc:  0.625  at batch 1363.\n",
            "Valid Loss: 1.4903994798660278    Valid Acc:  0.5625  at batch 1364.\n",
            "Valid Loss: 1.300418496131897    Valid Acc:  0.71875  at batch 1365.\n",
            "Valid Loss: 1.5272185802459717    Valid Acc:  0.53125  at batch 1366.\n",
            "Valid Loss: 1.5306370258331299    Valid Acc:  0.53125  at batch 1367.\n",
            "Valid Loss: 1.5335638523101807    Valid Acc:  0.65625  at batch 1368.\n",
            "Valid Loss: 1.4068348407745361    Valid Acc:  0.6875  at batch 1369.\n",
            "Valid Loss: 1.4374620914459229    Valid Acc:  0.65625  at batch 1370.\n",
            "Valid Loss: 1.5479850769042969    Valid Acc:  0.53125  at batch 1371.\n",
            "Valid Loss: 1.4620219469070435    Valid Acc:  0.75  at batch 1372.\n",
            "Valid Loss: 1.3164535760879517    Valid Acc:  0.78125  at batch 1373.\n",
            "Valid Loss: 1.4615436792373657    Valid Acc:  0.71875  at batch 1374.\n",
            "Valid Loss: 1.3436267375946045    Valid Acc:  0.71875  at batch 1375.\n",
            "Valid Loss: 1.3315662145614624    Valid Acc:  0.65625  at batch 1376.\n",
            "Valid Loss: 1.3606518507003784    Valid Acc:  0.71875  at batch 1377.\n",
            "Valid Loss: 1.3416303396224976    Valid Acc:  0.65625  at batch 1378.\n",
            "Valid Loss: 1.3548617362976074    Valid Acc:  0.71875  at batch 1379.\n",
            "Valid Loss: 1.4776216745376587    Valid Acc:  0.53125  at batch 1380.\n",
            "Valid Loss: 1.2709721326828003    Valid Acc:  0.75  at batch 1381.\n",
            "Valid Loss: 1.496375322341919    Valid Acc:  0.53125  at batch 1382.\n",
            "Valid Loss: 1.3920602798461914    Valid Acc:  0.71875  at batch 1383.\n",
            "Valid Loss: 1.4567675590515137    Valid Acc:  0.71875  at batch 1384.\n",
            "Valid Loss: 1.5650708675384521    Valid Acc:  0.53125  at batch 1385.\n",
            "Valid Loss: 1.339629888534546    Valid Acc:  0.75  at batch 1386.\n",
            "Valid Loss: 1.3317111730575562    Valid Acc:  0.71875  at batch 1387.\n",
            "Valid Loss: 1.3505334854125977    Valid Acc:  0.78125  at batch 1388.\n",
            "Valid Loss: 1.4242572784423828    Valid Acc:  0.65625  at batch 1389.\n",
            "Valid Loss: 1.415390968322754    Valid Acc:  0.71875  at batch 1390.\n",
            "Valid Loss: 1.457044005393982    Valid Acc:  0.625  at batch 1391.\n",
            "Valid Loss: 1.4565072059631348    Valid Acc:  0.6875  at batch 1392.\n",
            "Valid Loss: 1.4135949611663818    Valid Acc:  0.71875  at batch 1393.\n",
            "Valid Loss: 1.570984125137329    Valid Acc:  0.65625  at batch 1394.\n",
            "Valid Loss: 1.4225542545318604    Valid Acc:  0.71875  at batch 1395.\n",
            "Valid Loss: 1.315598726272583    Valid Acc:  0.78125  at batch 1396.\n",
            "Valid Loss: 1.242114782333374    Valid Acc:  0.84375  at batch 1397.\n",
            "Valid Loss: 1.3921842575073242    Valid Acc:  0.71875  at batch 1398.\n",
            "Valid Loss: 1.3061654567718506    Valid Acc:  0.6875  at batch 1399.\n",
            "Valid Loss: 1.3353137969970703    Valid Acc:  0.8125  at batch 1400.\n",
            "Valid Loss: 1.4207717180252075    Valid Acc:  0.75  at batch 1401.\n",
            "Valid Loss: 1.4280235767364502    Valid Acc:  0.625  at batch 1402.\n",
            "Valid Loss: 1.4870620965957642    Valid Acc:  0.5625  at batch 1403.\n",
            "Valid Loss: 1.4761719703674316    Valid Acc:  0.5  at batch 1404.\n",
            "Valid Loss: 1.543814778327942    Valid Acc:  0.53125  at batch 1405.\n",
            "Valid Loss: 1.5300213098526    Valid Acc:  0.71875  at batch 1406.\n",
            "Valid Loss: 1.5712953805923462    Valid Acc:  0.625  at batch 1407.\n",
            "Valid Loss: 1.2273637056350708    Valid Acc:  0.9375  at batch 1408.\n",
            "Valid Loss: 1.4892840385437012    Valid Acc:  0.625  at batch 1409.\n",
            "Valid Loss: 1.4933888912200928    Valid Acc:  0.5625  at batch 1410.\n",
            "Valid Loss: 1.3941593170166016    Valid Acc:  0.71875  at batch 1411.\n",
            "Valid Loss: 1.3274163007736206    Valid Acc:  0.78125  at batch 1412.\n",
            "Valid Loss: 1.3887237310409546    Valid Acc:  0.6875  at batch 1413.\n",
            "Valid Loss: 1.5310585498809814    Valid Acc:  0.65625  at batch 1414.\n",
            "Valid Loss: 1.4899284839630127    Valid Acc:  0.75  at batch 1415.\n",
            "Valid Loss: 1.5466413497924805    Valid Acc:  0.5625  at batch 1416.\n",
            "Valid Loss: 1.4121981859207153    Valid Acc:  0.78125  at batch 1417.\n",
            "Valid Loss: 1.5145878791809082    Valid Acc:  0.625  at batch 1418.\n",
            "Valid Loss: 1.518268346786499    Valid Acc:  0.71875  at batch 1419.\n",
            "Valid Loss: 1.475160837173462    Valid Acc:  0.59375  at batch 1420.\n",
            "Valid Loss: 1.523166298866272    Valid Acc:  0.5  at batch 1421.\n",
            "Valid Loss: 1.4206829071044922    Valid Acc:  0.59375  at batch 1422.\n",
            "Valid Loss: 1.3797714710235596    Valid Acc:  0.75  at batch 1423.\n",
            "Valid Loss: 1.3183459043502808    Valid Acc:  0.78125  at batch 1424.\n",
            "Valid Loss: 1.4177687168121338    Valid Acc:  0.625  at batch 1425.\n",
            "Valid Loss: 1.3553539514541626    Valid Acc:  0.75  at batch 1426.\n",
            "Valid Loss: 1.2892974615097046    Valid Acc:  0.78125  at batch 1427.\n",
            "Valid Loss: 1.4292839765548706    Valid Acc:  0.625  at batch 1428.\n",
            "Valid Loss: 1.4796494245529175    Valid Acc:  0.5625  at batch 1429.\n",
            "Valid Loss: 1.4643913507461548    Valid Acc:  0.6875  at batch 1430.\n",
            "Valid Loss: 1.2944809198379517    Valid Acc:  0.78125  at batch 1431.\n",
            "Valid Loss: 1.3474583625793457    Valid Acc:  0.84375  at batch 1432.\n",
            "Valid Loss: 1.4397317171096802    Valid Acc:  0.65625  at batch 1433.\n",
            "Valid Loss: 1.4918482303619385    Valid Acc:  0.65625  at batch 1434.\n",
            "Valid Loss: 1.3925241231918335    Valid Acc:  0.71875  at batch 1435.\n",
            "Valid Loss: 1.2965556383132935    Valid Acc:  0.6875  at batch 1436.\n",
            "Valid Loss: 1.4131296873092651    Valid Acc:  0.6875  at batch 1437.\n",
            "Valid Loss: 1.4725580215454102    Valid Acc:  0.65625  at batch 1438.\n",
            "Valid Loss: 1.3070563077926636    Valid Acc:  0.65625  at batch 1439.\n",
            "Valid Loss: 1.570443034172058    Valid Acc:  0.53125  at batch 1440.\n",
            "Valid Loss: 1.3677812814712524    Valid Acc:  0.8125  at batch 1441.\n",
            "Valid Loss: 1.3390827178955078    Valid Acc:  0.75  at batch 1442.\n",
            "Valid Loss: 1.316258192062378    Valid Acc:  0.71875  at batch 1443.\n",
            "Valid Loss: 1.3104846477508545    Valid Acc:  0.75  at batch 1444.\n",
            "Valid Loss: 1.5232505798339844    Valid Acc:  0.625  at batch 1445.\n",
            "Valid Loss: 1.3274565935134888    Valid Acc:  0.75  at batch 1446.\n",
            "Valid Loss: 1.3103681802749634    Valid Acc:  0.8125  at batch 1447.\n",
            "Valid Loss: 1.3713458776474    Valid Acc:  0.78125  at batch 1448.\n",
            "Valid Loss: 1.4068996906280518    Valid Acc:  0.71875  at batch 1449.\n",
            "Valid Loss: 1.6070334911346436    Valid Acc:  0.5  at batch 1450.\n",
            "Valid Loss: 1.5149599313735962    Valid Acc:  0.4375  at batch 1451.\n",
            "Valid Loss: 1.3483481407165527    Valid Acc:  0.75  at batch 1452.\n",
            "Valid Loss: 1.2952029705047607    Valid Acc:  0.71875  at batch 1453.\n",
            "Valid Loss: 1.4644241333007812    Valid Acc:  0.71875  at batch 1454.\n",
            "Valid Loss: 1.3488636016845703    Valid Acc:  0.6875  at batch 1455.\n",
            "Valid Loss: 1.3902180194854736    Valid Acc:  0.65625  at batch 1456.\n",
            "Valid Loss: 1.5111507177352905    Valid Acc:  0.5625  at batch 1457.\n",
            "Valid Loss: 1.378507375717163    Valid Acc:  0.6875  at batch 1458.\n",
            "Valid Loss: 1.3968980312347412    Valid Acc:  0.6875  at batch 1459.\n",
            "Valid Loss: 1.3749972581863403    Valid Acc:  0.625  at batch 1460.\n",
            "Valid Loss: 1.5279524326324463    Valid Acc:  0.625  at batch 1461.\n",
            "Valid Loss: 1.3271695375442505    Valid Acc:  0.6875  at batch 1462.\n",
            "Valid Loss: 1.387982964515686    Valid Acc:  0.75  at batch 1463.\n",
            "Valid Loss: 1.3391430377960205    Valid Acc:  0.6875  at batch 1464.\n",
            "Valid Loss: 1.5745134353637695    Valid Acc:  0.5  at batch 1465.\n",
            "Valid Loss: 1.4493398666381836    Valid Acc:  0.6875  at batch 1466.\n",
            "Valid Loss: 1.268784761428833    Valid Acc:  0.84375  at batch 1467.\n",
            "Valid Loss: 1.398783802986145    Valid Acc:  0.6875  at batch 1468.\n",
            "Valid Loss: 1.3757529258728027    Valid Acc:  0.65625  at batch 1469.\n",
            "Valid Loss: 1.258033275604248    Valid Acc:  0.78125  at batch 1470.\n",
            "Valid Loss: 1.5423007011413574    Valid Acc:  0.625  at batch 1471.\n",
            "Valid Loss: 1.581291913986206    Valid Acc:  0.59375  at batch 1472.\n",
            "Valid Loss: 1.3271231651306152    Valid Acc:  0.71875  at batch 1473.\n",
            "Valid Loss: 1.365239143371582    Valid Acc:  0.8125  at batch 1474.\n",
            "Valid Loss: 1.6342833042144775    Valid Acc:  0.40625  at batch 1475.\n",
            "Valid Loss: 1.3889485597610474    Valid Acc:  0.75  at batch 1476.\n",
            "Valid Loss: 1.3108872175216675    Valid Acc:  0.625  at batch 1477.\n",
            "Valid Loss: 1.4001399278640747    Valid Acc:  0.78125  at batch 1478.\n",
            "Valid Loss: 1.294258713722229    Valid Acc:  0.8125  at batch 1479.\n",
            "Valid Loss: 1.3728786706924438    Valid Acc:  0.71875  at batch 1480.\n",
            "Valid Loss: 1.3542672395706177    Valid Acc:  0.71875  at batch 1481.\n",
            "Valid Loss: 1.426982045173645    Valid Acc:  0.8125  at batch 1482.\n",
            "Valid Loss: 1.3399499654769897    Valid Acc:  0.78125  at batch 1483.\n",
            "Valid Loss: 1.433596134185791    Valid Acc:  0.71875  at batch 1484.\n",
            "Valid Loss: 1.4574707746505737    Valid Acc:  0.6875  at batch 1485.\n",
            "Valid Loss: 1.319862961769104    Valid Acc:  0.71875  at batch 1486.\n",
            "Valid Loss: 1.4098318815231323    Valid Acc:  0.6875  at batch 1487.\n",
            "Valid Loss: 1.3500456809997559    Valid Acc:  0.8125  at batch 1488.\n",
            "Valid Loss: 1.470832347869873    Valid Acc:  0.59375  at batch 1489.\n",
            "Valid Loss: 1.4522700309753418    Valid Acc:  0.59375  at batch 1490.\n",
            "Valid Loss: 1.3488222360610962    Valid Acc:  0.6875  at batch 1491.\n",
            "Valid Loss: 1.5064181089401245    Valid Acc:  0.65625  at batch 1492.\n",
            "Valid Loss: 1.4870514869689941    Valid Acc:  0.5  at batch 1493.\n",
            "Valid Loss: 1.366193413734436    Valid Acc:  0.71875  at batch 1494.\n",
            "Valid Loss: 1.3552387952804565    Valid Acc:  0.71875  at batch 1495.\n",
            "Valid Loss: 1.4881592988967896    Valid Acc:  0.625  at batch 1496.\n",
            "Valid Loss: 1.5370843410491943    Valid Acc:  0.59375  at batch 1497.\n",
            "Valid Loss: 1.3231103420257568    Valid Acc:  0.71875  at batch 1498.\n",
            "Valid Loss: 1.5149697065353394    Valid Acc:  0.75  at batch 1499.\n",
            "Valid Loss: 1.361169457435608    Valid Acc:  0.6875  at batch 1500.\n",
            "Valid Loss: 1.3515242338180542    Valid Acc:  0.71875  at batch 1501.\n",
            "Valid Loss: 1.4562634229660034    Valid Acc:  0.625  at batch 1502.\n",
            "Valid Loss: 1.4156241416931152    Valid Acc:  0.6875  at batch 1503.\n",
            "Valid Loss: 1.5223253965377808    Valid Acc:  0.59375  at batch 1504.\n",
            "Valid Loss: 1.516357660293579    Valid Acc:  0.59375  at batch 1505.\n",
            "Valid Loss: 1.1862231492996216    Valid Acc:  0.90625  at batch 1506.\n",
            "Valid Loss: 1.4940871000289917    Valid Acc:  0.5625  at batch 1507.\n",
            "Valid Loss: 1.3590118885040283    Valid Acc:  0.75  at batch 1508.\n",
            "Valid Loss: 1.453843593597412    Valid Acc:  0.65625  at batch 1509.\n",
            "Valid Loss: 1.4290231466293335    Valid Acc:  0.65625  at batch 1510.\n",
            "Valid Loss: 1.509096384048462    Valid Acc:  0.625  at batch 1511.\n",
            "Valid Loss: 1.4993129968643188    Valid Acc:  0.625  at batch 1512.\n",
            "Valid Loss: 1.4513537883758545    Valid Acc:  0.65625  at batch 1513.\n",
            "Valid Loss: 1.2428176403045654    Valid Acc:  0.8125  at batch 1514.\n",
            "Valid Loss: 1.3178209066390991    Valid Acc:  0.75  at batch 1515.\n",
            "Valid Loss: 1.3755791187286377    Valid Acc:  0.71875  at batch 1516.\n",
            "Valid Loss: 1.406919240951538    Valid Acc:  0.71875  at batch 1517.\n",
            "Valid Loss: 1.2836928367614746    Valid Acc:  0.75  at batch 1518.\n",
            "Valid Loss: 1.5327709913253784    Valid Acc:  0.5  at batch 1519.\n",
            "Valid Loss: 1.3703978061676025    Valid Acc:  0.65625  at batch 1520.\n",
            "Valid Loss: 1.485044240951538    Valid Acc:  0.5625  at batch 1521.\n",
            "Valid Loss: 1.3603743314743042    Valid Acc:  0.71875  at batch 1522.\n",
            "Valid Loss: 1.3907924890518188    Valid Acc:  0.71875  at batch 1523.\n",
            "Valid Loss: 1.4578018188476562    Valid Acc:  0.65625  at batch 1524.\n",
            "Valid Loss: 1.4712904691696167    Valid Acc:  0.6875  at batch 1525.\n",
            "Valid Loss: 1.4071654081344604    Valid Acc:  0.75  at batch 1526.\n",
            "Valid Loss: 1.2717154026031494    Valid Acc:  0.75  at batch 1527.\n",
            "Valid Loss: 1.5224478244781494    Valid Acc:  0.5  at batch 1528.\n",
            "Valid Loss: 1.4377377033233643    Valid Acc:  0.5625  at batch 1529.\n",
            "Valid Loss: 1.3384356498718262    Valid Acc:  0.65625  at batch 1530.\n",
            "Valid Loss: 1.408571720123291    Valid Acc:  0.65625  at batch 1531.\n",
            "Valid Loss: 1.3407286405563354    Valid Acc:  0.75  at batch 1532.\n",
            "Valid Loss: 1.3242993354797363    Valid Acc:  0.71875  at batch 1533.\n",
            "Valid Loss: 1.4431860446929932    Valid Acc:  0.65625  at batch 1534.\n",
            "Valid Loss: 1.3704270124435425    Valid Acc:  0.84375  at batch 1535.\n",
            "Valid Loss: 1.4008522033691406    Valid Acc:  0.78125  at batch 1536.\n",
            "Valid Loss: 1.2699202299118042    Valid Acc:  0.75  at batch 1537.\n",
            "Valid Loss: 1.4152758121490479    Valid Acc:  0.65625  at batch 1538.\n",
            "Valid Loss: 1.3389755487442017    Valid Acc:  0.75  at batch 1539.\n",
            "Valid Loss: 1.4650315046310425    Valid Acc:  0.6875  at batch 1540.\n",
            "Valid Loss: 1.2938371896743774    Valid Acc:  0.75  at batch 1541.\n",
            "Valid Loss: 1.543013095855713    Valid Acc:  0.5625  at batch 1542.\n",
            "Valid Loss: 1.320880651473999    Valid Acc:  0.6875  at batch 1543.\n",
            "Valid Loss: 1.569344162940979    Valid Acc:  0.625  at batch 1544.\n",
            "Valid Loss: 1.2282459735870361    Valid Acc:  0.78125  at batch 1545.\n",
            "Valid Loss: 1.4597978591918945    Valid Acc:  0.65625  at batch 1546.\n",
            "Valid Loss: 1.4112955331802368    Valid Acc:  0.6875  at batch 1547.\n",
            "Valid Loss: 1.378201961517334    Valid Acc:  0.71875  at batch 1548.\n",
            "Valid Loss: 1.2537277936935425    Valid Acc:  0.75  at batch 1549.\n",
            "Valid Loss: 1.4451669454574585    Valid Acc:  0.625  at batch 1550.\n",
            "Valid Loss: 1.49601411819458    Valid Acc:  0.5625  at batch 1551.\n",
            "Valid Loss: 1.3670352697372437    Valid Acc:  0.6875  at batch 1552.\n",
            "Valid Loss: 1.351747751235962    Valid Acc:  0.71875  at batch 1553.\n",
            "Valid Loss: 1.2705687284469604    Valid Acc:  0.71875  at batch 1554.\n",
            "Valid Loss: 1.367854356765747    Valid Acc:  0.6875  at batch 1555.\n",
            "Valid Loss: 1.3151774406433105    Valid Acc:  0.8125  at batch 1556.\n",
            "Valid Loss: 1.3383502960205078    Valid Acc:  0.71875  at batch 1557.\n",
            "Valid Loss: 1.3960012197494507    Valid Acc:  0.71875  at batch 1558.\n",
            "Valid Loss: 1.3983262777328491    Valid Acc:  0.59375  at batch 1559.\n",
            "Valid Loss: 1.331936001777649    Valid Acc:  0.6875  at batch 1560.\n",
            "Valid Loss: 1.4401211738586426    Valid Acc:  0.65625  at batch 1561.\n",
            "Valid Loss: 1.2536371946334839    Valid Acc:  0.6875  at batch 1562.\n",
            "\n",
            "\n",
            "EPOCH 2\n",
            "\n",
            "\n",
            "Train Loss: 1.3402366638183594    Train Acc:  0.765625  at batch 0.\n",
            "Train Loss: 1.374703288078308    Train Acc:  0.671875  at batch 1.\n",
            "Train Loss: 1.3917820453643799    Train Acc:  0.671875  at batch 2.\n",
            "Train Loss: 1.3798534870147705    Train Acc:  0.65625  at batch 3.\n",
            "Train Loss: 1.5258457660675049    Train Acc:  0.65625  at batch 4.\n",
            "Train Loss: 1.3774824142456055    Train Acc:  0.734375  at batch 5.\n",
            "Train Loss: 1.3608527183532715    Train Acc:  0.734375  at batch 6.\n",
            "Train Loss: 1.4753596782684326    Train Acc:  0.625  at batch 7.\n",
            "Train Loss: 1.300083875656128    Train Acc:  0.6875  at batch 8.\n",
            "Train Loss: 1.5142056941986084    Train Acc:  0.71875  at batch 9.\n",
            "Train Loss: 1.497139573097229    Train Acc:  0.671875  at batch 10.\n",
            "Train Loss: 1.4207874536514282    Train Acc:  0.71875  at batch 11.\n",
            "Train Loss: 1.4216794967651367    Train Acc:  0.703125  at batch 12.\n",
            "Train Loss: 1.3538613319396973    Train Acc:  0.703125  at batch 13.\n",
            "Train Loss: 1.3699623346328735    Train Acc:  0.6875  at batch 14.\n",
            "Train Loss: 1.3052994012832642    Train Acc:  0.71875  at batch 15.\n",
            "Train Loss: 1.522696852684021    Train Acc:  0.5625  at batch 16.\n",
            "Train Loss: 1.4423662424087524    Train Acc:  0.703125  at batch 17.\n",
            "Train Loss: 1.4039829969406128    Train Acc:  0.71875  at batch 18.\n",
            "Train Loss: 1.3924875259399414    Train Acc:  0.8125  at batch 19.\n",
            "Train Loss: 1.3171377182006836    Train Acc:  0.765625  at batch 20.\n",
            "Train Loss: 1.2872891426086426    Train Acc:  0.78125  at batch 21.\n",
            "Train Loss: 1.3617804050445557    Train Acc:  0.6875  at batch 22.\n",
            "Train Loss: 1.3415980339050293    Train Acc:  0.734375  at batch 23.\n",
            "Train Loss: 1.5173494815826416    Train Acc:  0.65625  at batch 24.\n",
            "Train Loss: 1.3112725019454956    Train Acc:  0.796875  at batch 25.\n",
            "Train Loss: 1.4401549100875854    Train Acc:  0.671875  at batch 26.\n",
            "Train Loss: 1.3841230869293213    Train Acc:  0.75  at batch 27.\n",
            "Train Loss: 1.3240915536880493    Train Acc:  0.796875  at batch 28.\n",
            "Train Loss: 1.3979241847991943    Train Acc:  0.765625  at batch 29.\n",
            "Train Loss: 1.3270100355148315    Train Acc:  0.8125  at batch 30.\n",
            "Train Loss: 1.2787585258483887    Train Acc:  0.8125  at batch 31.\n",
            "Train Loss: 1.340510606765747    Train Acc:  0.796875  at batch 32.\n",
            "Train Loss: 1.359872817993164    Train Acc:  0.6875  at batch 33.\n",
            "Train Loss: 1.4555749893188477    Train Acc:  0.65625  at batch 34.\n",
            "Train Loss: 1.3830604553222656    Train Acc:  0.6875  at batch 35.\n",
            "Train Loss: 1.3596415519714355    Train Acc:  0.75  at batch 36.\n",
            "Train Loss: 1.4673103094100952    Train Acc:  0.578125  at batch 37.\n",
            "Train Loss: 1.261830449104309    Train Acc:  0.765625  at batch 38.\n",
            "Train Loss: 1.5287631750106812    Train Acc:  0.71875  at batch 39.\n",
            "Train Loss: 1.3535022735595703    Train Acc:  0.6875  at batch 40.\n",
            "Train Loss: 1.416471242904663    Train Acc:  0.59375  at batch 41.\n",
            "Train Loss: 1.2437031269073486    Train Acc:  0.828125  at batch 42.\n",
            "Train Loss: 1.4779574871063232    Train Acc:  0.609375  at batch 43.\n",
            "Train Loss: 1.397255778312683    Train Acc:  0.671875  at batch 44.\n",
            "Train Loss: 1.4291789531707764    Train Acc:  0.640625  at batch 45.\n",
            "Train Loss: 1.3215090036392212    Train Acc:  0.78125  at batch 46.\n",
            "Train Loss: 1.3607521057128906    Train Acc:  0.6875  at batch 47.\n",
            "Train Loss: 1.3941142559051514    Train Acc:  0.734375  at batch 48.\n",
            "Train Loss: 1.4202895164489746    Train Acc:  0.71875  at batch 49.\n",
            "Train Loss: 1.4381918907165527    Train Acc:  0.6875  at batch 50.\n",
            "Train Loss: 1.3751413822174072    Train Acc:  0.734375  at batch 51.\n",
            "Train Loss: 1.403212070465088    Train Acc:  0.703125  at batch 52.\n",
            "Train Loss: 1.3745100498199463    Train Acc:  0.6875  at batch 53.\n",
            "Train Loss: 1.353286862373352    Train Acc:  0.734375  at batch 54.\n",
            "Train Loss: 1.4307284355163574    Train Acc:  0.703125  at batch 55.\n",
            "Train Loss: 1.3965133428573608    Train Acc:  0.734375  at batch 56.\n",
            "Train Loss: 1.3868436813354492    Train Acc:  0.734375  at batch 57.\n",
            "Train Loss: 1.3331677913665771    Train Acc:  0.828125  at batch 58.\n",
            "Train Loss: 1.516319990158081    Train Acc:  0.671875  at batch 59.\n",
            "Train Loss: 1.2450382709503174    Train Acc:  0.8125  at batch 60.\n",
            "Train Loss: 1.2383756637573242    Train Acc:  0.796875  at batch 61.\n",
            "Train Loss: 1.425797462463379    Train Acc:  0.703125  at batch 62.\n",
            "Train Loss: 1.3271008729934692    Train Acc:  0.828125  at batch 63.\n",
            "Train Loss: 1.3429449796676636    Train Acc:  0.75  at batch 64.\n",
            "Train Loss: 1.4791172742843628    Train Acc:  0.671875  at batch 65.\n",
            "Train Loss: 1.303871989250183    Train Acc:  0.78125  at batch 66.\n",
            "Train Loss: 1.280069351196289    Train Acc:  0.796875  at batch 67.\n",
            "Train Loss: 1.2703455686569214    Train Acc:  0.78125  at batch 68.\n",
            "Train Loss: 1.2535964250564575    Train Acc:  0.8125  at batch 69.\n",
            "Train Loss: 1.327071189880371    Train Acc:  0.671875  at batch 70.\n",
            "Train Loss: 1.2373676300048828    Train Acc:  0.765625  at batch 71.\n",
            "Train Loss: 1.3547275066375732    Train Acc:  0.671875  at batch 72.\n",
            "Train Loss: 1.3926700353622437    Train Acc:  0.65625  at batch 73.\n",
            "Train Loss: 1.3410427570343018    Train Acc:  0.75  at batch 74.\n",
            "Train Loss: 1.454432487487793    Train Acc:  0.671875  at batch 75.\n",
            "Train Loss: 1.3771107196807861    Train Acc:  0.609375  at batch 76.\n",
            "Train Loss: 1.3363416194915771    Train Acc:  0.6875  at batch 77.\n",
            "Train Loss: 1.3805296421051025    Train Acc:  0.671875  at batch 78.\n",
            "Train Loss: 1.40981125831604    Train Acc:  0.6875  at batch 79.\n",
            "Train Loss: 1.3532861471176147    Train Acc:  0.75  at batch 80.\n",
            "Train Loss: 1.3625285625457764    Train Acc:  0.71875  at batch 81.\n",
            "Train Loss: 1.3760048151016235    Train Acc:  0.75  at batch 82.\n",
            "Train Loss: 1.3304851055145264    Train Acc:  0.71875  at batch 83.\n",
            "Train Loss: 1.3766287565231323    Train Acc:  0.640625  at batch 84.\n",
            "Train Loss: 1.3452541828155518    Train Acc:  0.78125  at batch 85.\n",
            "Train Loss: 1.3539320230484009    Train Acc:  0.78125  at batch 86.\n",
            "Train Loss: 1.429747223854065    Train Acc:  0.625  at batch 87.\n",
            "Train Loss: 1.3252928256988525    Train Acc:  0.75  at batch 88.\n",
            "Train Loss: 1.2939255237579346    Train Acc:  0.78125  at batch 89.\n",
            "Train Loss: 1.3590134382247925    Train Acc:  0.6875  at batch 90.\n",
            "Train Loss: 1.3486876487731934    Train Acc:  0.734375  at batch 91.\n",
            "Train Loss: 1.3672020435333252    Train Acc:  0.734375  at batch 92.\n",
            "Train Loss: 1.3658360242843628    Train Acc:  0.703125  at batch 93.\n",
            "Train Loss: 1.372708797454834    Train Acc:  0.71875  at batch 94.\n",
            "Train Loss: 1.3186010122299194    Train Acc:  0.765625  at batch 95.\n",
            "Train Loss: 1.3716633319854736    Train Acc:  0.71875  at batch 96.\n",
            "Train Loss: 1.3609176874160767    Train Acc:  0.734375  at batch 97.\n",
            "Train Loss: 1.40681791305542    Train Acc:  0.65625  at batch 98.\n",
            "Train Loss: 1.4062626361846924    Train Acc:  0.703125  at batch 99.\n",
            "Train Loss: 1.4295316934585571    Train Acc:  0.734375  at batch 100.\n",
            "Train Loss: 1.3440302610397339    Train Acc:  0.75  at batch 101.\n",
            "Train Loss: 1.4083409309387207    Train Acc:  0.625  at batch 102.\n",
            "Train Loss: 1.419828176498413    Train Acc:  0.71875  at batch 103.\n",
            "Train Loss: 1.4125126600265503    Train Acc:  0.78125  at batch 104.\n",
            "Train Loss: 1.3934143781661987    Train Acc:  0.78125  at batch 105.\n",
            "Train Loss: 1.3558599948883057    Train Acc:  0.734375  at batch 106.\n",
            "Train Loss: 1.4042280912399292    Train Acc:  0.75  at batch 107.\n",
            "Train Loss: 1.257404088973999    Train Acc:  0.796875  at batch 108.\n",
            "Train Loss: 1.3350021839141846    Train Acc:  0.734375  at batch 109.\n",
            "Train Loss: 1.30686354637146    Train Acc:  0.75  at batch 110.\n",
            "Train Loss: 1.2385722398757935    Train Acc:  0.8125  at batch 111.\n",
            "Train Loss: 1.3732333183288574    Train Acc:  0.765625  at batch 112.\n",
            "Train Loss: 1.3589011430740356    Train Acc:  0.703125  at batch 113.\n",
            "Train Loss: 1.3351390361785889    Train Acc:  0.703125  at batch 114.\n",
            "Train Loss: 1.2643077373504639    Train Acc:  0.734375  at batch 115.\n",
            "Train Loss: 1.359555721282959    Train Acc:  0.75  at batch 116.\n",
            "Train Loss: 1.3332210779190063    Train Acc:  0.765625  at batch 117.\n",
            "Train Loss: 1.3191062211990356    Train Acc:  0.703125  at batch 118.\n",
            "Train Loss: 1.2571983337402344    Train Acc:  0.8125  at batch 119.\n",
            "Train Loss: 1.2937368154525757    Train Acc:  0.78125  at batch 120.\n",
            "Train Loss: 1.3493573665618896    Train Acc:  0.71875  at batch 121.\n",
            "Train Loss: 1.1385866403579712    Train Acc:  0.84375  at batch 122.\n",
            "Train Loss: 1.337403416633606    Train Acc:  0.65625  at batch 123.\n",
            "Train Loss: 1.3345739841461182    Train Acc:  0.75  at batch 124.\n",
            "Train Loss: 1.3174197673797607    Train Acc:  0.71875  at batch 125.\n",
            "Train Loss: 1.2488350868225098    Train Acc:  0.828125  at batch 126.\n",
            "Train Loss: 1.3034852743148804    Train Acc:  0.734375  at batch 127.\n",
            "Train Loss: 1.4408063888549805    Train Acc:  0.734375  at batch 128.\n",
            "Train Loss: 1.2509534358978271    Train Acc:  0.828125  at batch 129.\n",
            "Train Loss: 1.219307780265808    Train Acc:  0.78125  at batch 130.\n",
            "Train Loss: 1.2974220514297485    Train Acc:  0.765625  at batch 131.\n",
            "Train Loss: 1.3076263666152954    Train Acc:  0.75  at batch 132.\n",
            "Train Loss: 1.2451802492141724    Train Acc:  0.8125  at batch 133.\n",
            "Train Loss: 1.281018614768982    Train Acc:  0.734375  at batch 134.\n",
            "Train Loss: 1.2854859828948975    Train Acc:  0.75  at batch 135.\n",
            "Train Loss: 1.306826114654541    Train Acc:  0.71875  at batch 136.\n",
            "Train Loss: 1.3401744365692139    Train Acc:  0.6875  at batch 137.\n",
            "Train Loss: 1.3083741664886475    Train Acc:  0.765625  at batch 138.\n",
            "Train Loss: 1.2592604160308838    Train Acc:  0.765625  at batch 139.\n",
            "Train Loss: 1.2803592681884766    Train Acc:  0.796875  at batch 140.\n",
            "Train Loss: 1.3288925886154175    Train Acc:  0.671875  at batch 141.\n",
            "Train Loss: 1.3460588455200195    Train Acc:  0.671875  at batch 142.\n",
            "Train Loss: 1.3695582151412964    Train Acc:  0.796875  at batch 143.\n",
            "Train Loss: 1.2568864822387695    Train Acc:  0.765625  at batch 144.\n",
            "Train Loss: 1.3486213684082031    Train Acc:  0.71875  at batch 145.\n",
            "Train Loss: 1.277039885520935    Train Acc:  0.71875  at batch 146.\n",
            "Train Loss: 1.3116856813430786    Train Acc:  0.6875  at batch 147.\n",
            "Train Loss: 1.3151248693466187    Train Acc:  0.78125  at batch 148.\n",
            "Train Loss: 1.289967656135559    Train Acc:  0.75  at batch 149.\n",
            "Train Loss: 1.3313019275665283    Train Acc:  0.703125  at batch 150.\n",
            "Train Loss: 1.2802520990371704    Train Acc:  0.75  at batch 151.\n",
            "Train Loss: 1.3269530534744263    Train Acc:  0.75  at batch 152.\n",
            "Train Loss: 1.2991266250610352    Train Acc:  0.71875  at batch 153.\n",
            "Train Loss: 1.3061970472335815    Train Acc:  0.75  at batch 154.\n",
            "Train Loss: 1.4361538887023926    Train Acc:  0.515625  at batch 155.\n",
            "Train Loss: 1.2885346412658691    Train Acc:  0.8125  at batch 156.\n",
            "Train Loss: 1.3136643171310425    Train Acc:  0.75  at batch 157.\n",
            "Train Loss: 1.2649974822998047    Train Acc:  0.84375  at batch 158.\n",
            "Train Loss: 1.2800242900848389    Train Acc:  0.78125  at batch 159.\n",
            "Train Loss: 1.258496880531311    Train Acc:  0.828125  at batch 160.\n",
            "Train Loss: 1.2809312343597412    Train Acc:  0.8125  at batch 161.\n",
            "Train Loss: 1.2138465642929077    Train Acc:  0.765625  at batch 162.\n",
            "Train Loss: 1.3275954723358154    Train Acc:  0.71875  at batch 163.\n",
            "Train Loss: 1.2850016355514526    Train Acc:  0.796875  at batch 164.\n",
            "Train Loss: 1.3256418704986572    Train Acc:  0.6875  at batch 165.\n",
            "Train Loss: 1.3094301223754883    Train Acc:  0.71875  at batch 166.\n",
            "Train Loss: 1.2963963747024536    Train Acc:  0.796875  at batch 167.\n",
            "Train Loss: 1.1497334241867065    Train Acc:  0.765625  at batch 168.\n",
            "Train Loss: 1.241825819015503    Train Acc:  0.71875  at batch 169.\n",
            "Train Loss: 1.2328091859817505    Train Acc:  0.8125  at batch 170.\n",
            "Train Loss: 1.2686952352523804    Train Acc:  0.609375  at batch 171.\n",
            "Train Loss: 1.2494341135025024    Train Acc:  0.765625  at batch 172.\n",
            "Train Loss: 1.1689552068710327    Train Acc:  0.75  at batch 173.\n",
            "Train Loss: 1.345917820930481    Train Acc:  0.640625  at batch 174.\n",
            "Train Loss: 1.242906928062439    Train Acc:  0.71875  at batch 175.\n",
            "Train Loss: 1.3654909133911133    Train Acc:  0.734375  at batch 176.\n",
            "Train Loss: 1.180418610572815    Train Acc:  0.78125  at batch 177.\n",
            "Train Loss: 1.3532882928848267    Train Acc:  0.703125  at batch 178.\n",
            "Train Loss: 1.3677852153778076    Train Acc:  0.75  at batch 179.\n",
            "Train Loss: 1.311299443244934    Train Acc:  0.65625  at batch 180.\n",
            "Train Loss: 1.171051025390625    Train Acc:  0.796875  at batch 181.\n",
            "Train Loss: 1.3234471082687378    Train Acc:  0.734375  at batch 182.\n",
            "Train Loss: 1.2478132247924805    Train Acc:  0.703125  at batch 183.\n",
            "Train Loss: 1.2900660037994385    Train Acc:  0.78125  at batch 184.\n",
            "Train Loss: 1.2826443910598755    Train Acc:  0.71875  at batch 185.\n",
            "Train Loss: 1.2899363040924072    Train Acc:  0.71875  at batch 186.\n",
            "Train Loss: 1.3609520196914673    Train Acc:  0.6875  at batch 187.\n",
            "Train Loss: 1.1869217157363892    Train Acc:  0.78125  at batch 188.\n",
            "Train Loss: 1.3197449445724487    Train Acc:  0.734375  at batch 189.\n",
            "Train Loss: 1.296059489250183    Train Acc:  0.75  at batch 190.\n",
            "Train Loss: 1.3504817485809326    Train Acc:  0.671875  at batch 191.\n",
            "Train Loss: 1.3164231777191162    Train Acc:  0.671875  at batch 192.\n",
            "Train Loss: 1.253761887550354    Train Acc:  0.75  at batch 193.\n",
            "Train Loss: 1.1942241191864014    Train Acc:  0.796875  at batch 194.\n",
            "Train Loss: 1.2810240983963013    Train Acc:  0.78125  at batch 195.\n",
            "Train Loss: 1.2089847326278687    Train Acc:  0.78125  at batch 196.\n",
            "Train Loss: 1.3740350008010864    Train Acc:  0.640625  at batch 197.\n",
            "Train Loss: 1.3323017358779907    Train Acc:  0.6875  at batch 198.\n",
            "Train Loss: 1.2996461391448975    Train Acc:  0.75  at batch 199.\n",
            "Train Loss: 1.305075764656067    Train Acc:  0.78125  at batch 200.\n",
            "Train Loss: 1.1286847591400146    Train Acc:  0.828125  at batch 201.\n",
            "Train Loss: 1.2397263050079346    Train Acc:  0.78125  at batch 202.\n",
            "Train Loss: 1.1817998886108398    Train Acc:  0.84375  at batch 203.\n",
            "Train Loss: 1.16341233253479    Train Acc:  0.859375  at batch 204.\n",
            "Train Loss: 1.2231500148773193    Train Acc:  0.78125  at batch 205.\n",
            "Train Loss: 1.2669352293014526    Train Acc:  0.765625  at batch 206.\n",
            "Train Loss: 1.1942378282546997    Train Acc:  0.765625  at batch 207.\n",
            "Train Loss: 1.2343817949295044    Train Acc:  0.765625  at batch 208.\n",
            "Train Loss: 1.4227008819580078    Train Acc:  0.640625  at batch 209.\n",
            "Train Loss: 1.258142113685608    Train Acc:  0.75  at batch 210.\n",
            "Train Loss: 1.1981256008148193    Train Acc:  0.796875  at batch 211.\n",
            "Train Loss: 1.2201985120773315    Train Acc:  0.78125  at batch 212.\n",
            "Train Loss: 1.3333607912063599    Train Acc:  0.75  at batch 213.\n",
            "Train Loss: 1.2384620904922485    Train Acc:  0.796875  at batch 214.\n",
            "Train Loss: 1.2167551517486572    Train Acc:  0.796875  at batch 215.\n",
            "Train Loss: 1.2534167766571045    Train Acc:  0.78125  at batch 216.\n",
            "Train Loss: 1.2427376508712769    Train Acc:  0.734375  at batch 217.\n",
            "Train Loss: 1.2855044603347778    Train Acc:  0.71875  at batch 218.\n",
            "Train Loss: 1.2539808750152588    Train Acc:  0.78125  at batch 219.\n",
            "Train Loss: 1.1793735027313232    Train Acc:  0.765625  at batch 220.\n",
            "Train Loss: 1.2786614894866943    Train Acc:  0.734375  at batch 221.\n",
            "Train Loss: 1.1718056201934814    Train Acc:  0.75  at batch 222.\n",
            "Train Loss: 1.3363460302352905    Train Acc:  0.71875  at batch 223.\n",
            "Train Loss: 1.194501280784607    Train Acc:  0.75  at batch 224.\n",
            "Train Loss: 1.2563750743865967    Train Acc:  0.6875  at batch 225.\n",
            "Train Loss: 1.194929838180542    Train Acc:  0.734375  at batch 226.\n",
            "Train Loss: 1.1797425746917725    Train Acc:  0.859375  at batch 227.\n",
            "Train Loss: 1.246732234954834    Train Acc:  0.703125  at batch 228.\n",
            "Train Loss: 1.2584149837493896    Train Acc:  0.734375  at batch 229.\n",
            "Train Loss: 1.4120575189590454    Train Acc:  0.609375  at batch 230.\n",
            "Train Loss: 1.201291799545288    Train Acc:  0.78125  at batch 231.\n",
            "Train Loss: 1.205335021018982    Train Acc:  0.71875  at batch 232.\n",
            "Train Loss: 1.3922933340072632    Train Acc:  0.765625  at batch 233.\n",
            "Train Loss: 1.133771538734436    Train Acc:  0.828125  at batch 234.\n",
            "Train Loss: 1.3303494453430176    Train Acc:  0.734375  at batch 235.\n",
            "Train Loss: 1.2759034633636475    Train Acc:  0.796875  at batch 236.\n",
            "Train Loss: 1.2362170219421387    Train Acc:  0.734375  at batch 237.\n",
            "Train Loss: 1.2527647018432617    Train Acc:  0.78125  at batch 238.\n",
            "Train Loss: 1.1936581134796143    Train Acc:  0.796875  at batch 239.\n",
            "Train Loss: 1.2838619947433472    Train Acc:  0.703125  at batch 240.\n",
            "Train Loss: 1.3532894849777222    Train Acc:  0.671875  at batch 241.\n",
            "Train Loss: 1.1404693126678467    Train Acc:  0.8125  at batch 242.\n",
            "Train Loss: 1.2138899564743042    Train Acc:  0.8125  at batch 243.\n",
            "Train Loss: 1.2673535346984863    Train Acc:  0.78125  at batch 244.\n",
            "Train Loss: 1.2106536626815796    Train Acc:  0.8125  at batch 245.\n",
            "Train Loss: 1.3350526094436646    Train Acc:  0.671875  at batch 246.\n",
            "Train Loss: 1.2101703882217407    Train Acc:  0.8125  at batch 247.\n",
            "Train Loss: 1.2369612455368042    Train Acc:  0.796875  at batch 248.\n",
            "Train Loss: 1.1646987199783325    Train Acc:  0.78125  at batch 249.\n",
            "Train Loss: 1.1625995635986328    Train Acc:  0.8125  at batch 250.\n",
            "Train Loss: 1.208566427230835    Train Acc:  0.8125  at batch 251.\n",
            "Train Loss: 1.2538177967071533    Train Acc:  0.71875  at batch 252.\n",
            "Train Loss: 1.2750214338302612    Train Acc:  0.796875  at batch 253.\n",
            "Train Loss: 1.2433351278305054    Train Acc:  0.796875  at batch 254.\n",
            "Train Loss: 1.1985371112823486    Train Acc:  0.796875  at batch 255.\n",
            "Train Loss: 1.1725127696990967    Train Acc:  0.71875  at batch 256.\n",
            "Train Loss: 1.2506072521209717    Train Acc:  0.6875  at batch 257.\n",
            "Train Loss: 1.3254929780960083    Train Acc:  0.78125  at batch 258.\n",
            "Train Loss: 1.2359189987182617    Train Acc:  0.734375  at batch 259.\n",
            "Train Loss: 1.2986198663711548    Train Acc:  0.671875  at batch 260.\n",
            "Train Loss: 1.3092985153198242    Train Acc:  0.6875  at batch 261.\n",
            "Train Loss: 1.2517194747924805    Train Acc:  0.71875  at batch 262.\n",
            "Train Loss: 1.240731120109558    Train Acc:  0.6875  at batch 263.\n",
            "Train Loss: 1.207380771636963    Train Acc:  0.84375  at batch 264.\n",
            "Train Loss: 1.2295364141464233    Train Acc:  0.765625  at batch 265.\n",
            "Train Loss: 1.3181638717651367    Train Acc:  0.75  at batch 266.\n",
            "Train Loss: 1.3696482181549072    Train Acc:  0.6875  at batch 267.\n",
            "Train Loss: 1.3624067306518555    Train Acc:  0.671875  at batch 268.\n",
            "Train Loss: 1.2348997592926025    Train Acc:  0.765625  at batch 269.\n",
            "Train Loss: 1.2215454578399658    Train Acc:  0.84375  at batch 270.\n",
            "Train Loss: 1.2228777408599854    Train Acc:  0.78125  at batch 271.\n",
            "Train Loss: 1.2051163911819458    Train Acc:  0.78125  at batch 272.\n",
            "Train Loss: 1.2400450706481934    Train Acc:  0.796875  at batch 273.\n",
            "Train Loss: 1.3611727952957153    Train Acc:  0.703125  at batch 274.\n",
            "Train Loss: 1.2110631465911865    Train Acc:  0.75  at batch 275.\n",
            "Train Loss: 1.1412221193313599    Train Acc:  0.765625  at batch 276.\n",
            "Train Loss: 1.1374454498291016    Train Acc:  0.75  at batch 277.\n",
            "Train Loss: 1.2367656230926514    Train Acc:  0.828125  at batch 278.\n",
            "Train Loss: 1.3273502588272095    Train Acc:  0.71875  at batch 279.\n",
            "Train Loss: 1.3128235340118408    Train Acc:  0.734375  at batch 280.\n",
            "Train Loss: 1.2004117965698242    Train Acc:  0.765625  at batch 281.\n",
            "Train Loss: 1.2036808729171753    Train Acc:  0.71875  at batch 282.\n",
            "Train Loss: 1.148510456085205    Train Acc:  0.84375  at batch 283.\n",
            "Train Loss: 1.1632858514785767    Train Acc:  0.8125  at batch 284.\n",
            "Train Loss: 1.144202709197998    Train Acc:  0.859375  at batch 285.\n",
            "Train Loss: 1.2458490133285522    Train Acc:  0.734375  at batch 286.\n",
            "Train Loss: 1.016951560974121    Train Acc:  0.78125  at batch 287.\n",
            "Train Loss: 1.1228973865509033    Train Acc:  0.796875  at batch 288.\n",
            "Train Loss: 1.245364785194397    Train Acc:  0.75  at batch 289.\n",
            "Train Loss: 1.4575233459472656    Train Acc:  0.6875  at batch 290.\n",
            "Train Loss: 1.208031415939331    Train Acc:  0.71875  at batch 291.\n",
            "Train Loss: 1.207374930381775    Train Acc:  0.765625  at batch 292.\n",
            "Train Loss: 1.1438148021697998    Train Acc:  0.8125  at batch 293.\n",
            "Train Loss: 1.2827874422073364    Train Acc:  0.734375  at batch 294.\n",
            "Train Loss: 1.193589687347412    Train Acc:  0.703125  at batch 295.\n",
            "Train Loss: 1.2129558324813843    Train Acc:  0.796875  at batch 296.\n",
            "Train Loss: 1.3287509679794312    Train Acc:  0.6875  at batch 297.\n",
            "Train Loss: 1.2518870830535889    Train Acc:  0.75  at batch 298.\n",
            "Train Loss: 1.286699652671814    Train Acc:  0.671875  at batch 299.\n",
            "Train Loss: 1.1592155694961548    Train Acc:  0.859375  at batch 300.\n",
            "Train Loss: 1.2344226837158203    Train Acc:  0.75  at batch 301.\n",
            "Train Loss: 1.1990487575531006    Train Acc:  0.796875  at batch 302.\n",
            "Train Loss: 1.205781102180481    Train Acc:  0.75  at batch 303.\n",
            "Train Loss: 1.2953495979309082    Train Acc:  0.6875  at batch 304.\n",
            "Train Loss: 1.167338252067566    Train Acc:  0.796875  at batch 305.\n",
            "Train Loss: 1.1690541505813599    Train Acc:  0.765625  at batch 306.\n",
            "Train Loss: 1.2538371086120605    Train Acc:  0.75  at batch 307.\n",
            "Train Loss: 1.1890857219696045    Train Acc:  0.8125  at batch 308.\n",
            "Train Loss: 1.217159390449524    Train Acc:  0.734375  at batch 309.\n",
            "Train Loss: 1.200829267501831    Train Acc:  0.765625  at batch 310.\n",
            "Train Loss: 1.287329077720642    Train Acc:  0.75  at batch 311.\n",
            "Train Loss: 1.340394377708435    Train Acc:  0.6875  at batch 312.\n",
            "Train Loss: 1.3249247074127197    Train Acc:  0.796875  at batch 313.\n",
            "Train Loss: 1.1543415784835815    Train Acc:  0.796875  at batch 314.\n",
            "Train Loss: 1.1608439683914185    Train Acc:  0.828125  at batch 315.\n",
            "Train Loss: 1.2162836790084839    Train Acc:  0.71875  at batch 316.\n",
            "Train Loss: 1.0890389680862427    Train Acc:  0.84375  at batch 317.\n",
            "Train Loss: 1.2847176790237427    Train Acc:  0.71875  at batch 318.\n",
            "Train Loss: 1.1961753368377686    Train Acc:  0.734375  at batch 319.\n",
            "Train Loss: 1.1388052701950073    Train Acc:  0.8125  at batch 320.\n",
            "Train Loss: 1.3403006792068481    Train Acc:  0.71875  at batch 321.\n",
            "Train Loss: 1.165366768836975    Train Acc:  0.78125  at batch 322.\n",
            "Train Loss: 1.1296347379684448    Train Acc:  0.8125  at batch 323.\n",
            "Train Loss: 1.1776149272918701    Train Acc:  0.78125  at batch 324.\n",
            "Train Loss: 1.1968390941619873    Train Acc:  0.78125  at batch 325.\n",
            "Train Loss: 1.2207117080688477    Train Acc:  0.703125  at batch 326.\n",
            "Train Loss: 1.1040916442871094    Train Acc:  0.75  at batch 327.\n",
            "Train Loss: 1.1801363229751587    Train Acc:  0.75  at batch 328.\n",
            "Train Loss: 1.1916056871414185    Train Acc:  0.8125  at batch 329.\n",
            "Train Loss: 1.108991026878357    Train Acc:  0.796875  at batch 330.\n",
            "Train Loss: 1.161341905593872    Train Acc:  0.796875  at batch 331.\n",
            "Train Loss: 1.106756329536438    Train Acc:  0.765625  at batch 332.\n",
            "Train Loss: 1.1827770471572876    Train Acc:  0.859375  at batch 333.\n",
            "Train Loss: 1.144608974456787    Train Acc:  0.765625  at batch 334.\n",
            "Train Loss: 1.14548659324646    Train Acc:  0.78125  at batch 335.\n",
            "Train Loss: 1.255784511566162    Train Acc:  0.640625  at batch 336.\n",
            "Train Loss: 1.1472244262695312    Train Acc:  0.828125  at batch 337.\n",
            "Train Loss: 1.1647309064865112    Train Acc:  0.765625  at batch 338.\n",
            "Train Loss: 1.1974760293960571    Train Acc:  0.8125  at batch 339.\n",
            "Train Loss: 1.006019115447998    Train Acc:  0.890625  at batch 340.\n",
            "Train Loss: 1.1708669662475586    Train Acc:  0.828125  at batch 341.\n",
            "Train Loss: 1.1973806619644165    Train Acc:  0.765625  at batch 342.\n",
            "Train Loss: 1.1407159566879272    Train Acc:  0.765625  at batch 343.\n",
            "Train Loss: 1.1513984203338623    Train Acc:  0.71875  at batch 344.\n",
            "Train Loss: 1.171972632408142    Train Acc:  0.8125  at batch 345.\n",
            "Train Loss: 1.3492828607559204    Train Acc:  0.65625  at batch 346.\n",
            "Train Loss: 1.2632757425308228    Train Acc:  0.703125  at batch 347.\n",
            "Train Loss: 1.2442402839660645    Train Acc:  0.796875  at batch 348.\n",
            "Train Loss: 1.0719363689422607    Train Acc:  0.84375  at batch 349.\n",
            "Train Loss: 1.3019258975982666    Train Acc:  0.6875  at batch 350.\n",
            "Train Loss: 1.1984046697616577    Train Acc:  0.71875  at batch 351.\n",
            "Train Loss: 1.287338137626648    Train Acc:  0.703125  at batch 352.\n",
            "Train Loss: 1.1422382593154907    Train Acc:  0.78125  at batch 353.\n",
            "Train Loss: 1.040932297706604    Train Acc:  0.875  at batch 354.\n",
            "Train Loss: 1.1380541324615479    Train Acc:  0.75  at batch 355.\n",
            "Train Loss: 1.2089183330535889    Train Acc:  0.703125  at batch 356.\n",
            "Train Loss: 1.22395920753479    Train Acc:  0.78125  at batch 357.\n",
            "Train Loss: 1.210428237915039    Train Acc:  0.765625  at batch 358.\n",
            "Train Loss: 1.1458323001861572    Train Acc:  0.8125  at batch 359.\n",
            "Train Loss: 1.1700184345245361    Train Acc:  0.8125  at batch 360.\n",
            "Train Loss: 1.3126296997070312    Train Acc:  0.703125  at batch 361.\n",
            "Train Loss: 1.2367770671844482    Train Acc:  0.78125  at batch 362.\n",
            "Train Loss: 1.2861748933792114    Train Acc:  0.8125  at batch 363.\n",
            "Train Loss: 1.102047085762024    Train Acc:  0.78125  at batch 364.\n",
            "Train Loss: 1.2183185815811157    Train Acc:  0.734375  at batch 365.\n",
            "Train Loss: 1.168136715888977    Train Acc:  0.75  at batch 366.\n",
            "Train Loss: 1.1848467588424683    Train Acc:  0.796875  at batch 367.\n",
            "Train Loss: 1.3565388917922974    Train Acc:  0.703125  at batch 368.\n",
            "Train Loss: 1.2450917959213257    Train Acc:  0.78125  at batch 369.\n",
            "Train Loss: 1.3205033540725708    Train Acc:  0.6875  at batch 370.\n",
            "Train Loss: 1.1066702604293823    Train Acc:  0.828125  at batch 371.\n",
            "Train Loss: 1.1830451488494873    Train Acc:  0.765625  at batch 372.\n",
            "Train Loss: 1.0849082469940186    Train Acc:  0.734375  at batch 373.\n",
            "Train Loss: 1.1901075839996338    Train Acc:  0.71875  at batch 374.\n",
            "Train Loss: 1.1845084428787231    Train Acc:  0.734375  at batch 375.\n",
            "Train Loss: 1.2095000743865967    Train Acc:  0.703125  at batch 376.\n",
            "Train Loss: 1.132920742034912    Train Acc:  0.84375  at batch 377.\n",
            "Train Loss: 1.1684577465057373    Train Acc:  0.796875  at batch 378.\n",
            "Train Loss: 1.0336247682571411    Train Acc:  0.828125  at batch 379.\n",
            "Train Loss: 1.262916088104248    Train Acc:  0.71875  at batch 380.\n",
            "Train Loss: 1.1658374071121216    Train Acc:  0.796875  at batch 381.\n",
            "Train Loss: 1.3062483072280884    Train Acc:  0.71875  at batch 382.\n",
            "Train Loss: 1.154297947883606    Train Acc:  0.703125  at batch 383.\n",
            "Train Loss: 1.0734165906906128    Train Acc:  0.875  at batch 384.\n",
            "Train Loss: 1.2422486543655396    Train Acc:  0.71875  at batch 385.\n",
            "Train Loss: 1.12087082862854    Train Acc:  0.78125  at batch 386.\n",
            "Train Loss: 1.1120936870574951    Train Acc:  0.859375  at batch 387.\n",
            "Train Loss: 1.2815173864364624    Train Acc:  0.78125  at batch 388.\n",
            "Train Loss: 1.180924892425537    Train Acc:  0.734375  at batch 389.\n",
            "Train Loss: 1.1985235214233398    Train Acc:  0.75  at batch 390.\n",
            "Train Loss: 1.158165454864502    Train Acc:  0.84375  at batch 391.\n",
            "Train Loss: 1.1844000816345215    Train Acc:  0.75  at batch 392.\n",
            "Train Loss: 1.127721905708313    Train Acc:  0.796875  at batch 393.\n",
            "Train Loss: 1.1271681785583496    Train Acc:  0.8125  at batch 394.\n",
            "Train Loss: 1.019276738166809    Train Acc:  0.8125  at batch 395.\n",
            "Train Loss: 1.0795494318008423    Train Acc:  0.8125  at batch 396.\n",
            "Train Loss: 1.184982180595398    Train Acc:  0.75  at batch 397.\n",
            "Train Loss: 1.291019320487976    Train Acc:  0.65625  at batch 398.\n",
            "Train Loss: 1.1023201942443848    Train Acc:  0.828125  at batch 399.\n",
            "Train Loss: 1.1962342262268066    Train Acc:  0.796875  at batch 400.\n",
            "Train Loss: 1.1297392845153809    Train Acc:  0.765625  at batch 401.\n",
            "Train Loss: 1.2448620796203613    Train Acc:  0.703125  at batch 402.\n",
            "Train Loss: 1.1468273401260376    Train Acc:  0.84375  at batch 403.\n",
            "Train Loss: 1.2258461713790894    Train Acc:  0.734375  at batch 404.\n",
            "Train Loss: 1.110764980316162    Train Acc:  0.859375  at batch 405.\n",
            "Train Loss: 1.1528311967849731    Train Acc:  0.8125  at batch 406.\n",
            "Train Loss: 1.2342811822891235    Train Acc:  0.71875  at batch 407.\n",
            "Train Loss: 1.1300911903381348    Train Acc:  0.828125  at batch 408.\n",
            "Train Loss: 1.2070330381393433    Train Acc:  0.796875  at batch 409.\n",
            "Train Loss: 1.2681516408920288    Train Acc:  0.71875  at batch 410.\n",
            "Train Loss: 1.1475223302841187    Train Acc:  0.828125  at batch 411.\n",
            "Train Loss: 1.1752214431762695    Train Acc:  0.734375  at batch 412.\n",
            "Train Loss: 1.1231228113174438    Train Acc:  0.828125  at batch 413.\n",
            "Train Loss: 1.0985890626907349    Train Acc:  0.78125  at batch 414.\n",
            "Train Loss: 1.1508617401123047    Train Acc:  0.765625  at batch 415.\n",
            "Train Loss: 1.0718023777008057    Train Acc:  0.828125  at batch 416.\n",
            "Train Loss: 1.2245558500289917    Train Acc:  0.734375  at batch 417.\n",
            "Train Loss: 1.1746503114700317    Train Acc:  0.734375  at batch 418.\n",
            "Train Loss: 1.2157477140426636    Train Acc:  0.75  at batch 419.\n",
            "Train Loss: 1.2345244884490967    Train Acc:  0.75  at batch 420.\n",
            "Train Loss: 1.2040196657180786    Train Acc:  0.78125  at batch 421.\n",
            "Train Loss: 1.0425388813018799    Train Acc:  0.84375  at batch 422.\n",
            "Train Loss: 1.2120293378829956    Train Acc:  0.71875  at batch 423.\n",
            "Train Loss: 1.1692991256713867    Train Acc:  0.765625  at batch 424.\n",
            "Train Loss: 1.096644401550293    Train Acc:  0.84375  at batch 425.\n",
            "Train Loss: 1.2223118543624878    Train Acc:  0.8125  at batch 426.\n",
            "Train Loss: 1.005793809890747    Train Acc:  0.859375  at batch 427.\n",
            "Train Loss: 1.135001540184021    Train Acc:  0.796875  at batch 428.\n",
            "Train Loss: 1.0438650846481323    Train Acc:  0.8125  at batch 429.\n",
            "Train Loss: 1.1807094812393188    Train Acc:  0.78125  at batch 430.\n",
            "Train Loss: 1.0909510850906372    Train Acc:  0.8125  at batch 431.\n",
            "Train Loss: 1.1615259647369385    Train Acc:  0.734375  at batch 432.\n",
            "Train Loss: 1.1972365379333496    Train Acc:  0.734375  at batch 433.\n",
            "Train Loss: 1.0027637481689453    Train Acc:  0.84375  at batch 434.\n",
            "Train Loss: 1.1808857917785645    Train Acc:  0.75  at batch 435.\n",
            "Train Loss: 1.0828028917312622    Train Acc:  0.75  at batch 436.\n",
            "Train Loss: 1.2216746807098389    Train Acc:  0.734375  at batch 437.\n",
            "Train Loss: 1.1363635063171387    Train Acc:  0.765625  at batch 438.\n",
            "Train Loss: 1.1730024814605713    Train Acc:  0.78125  at batch 439.\n",
            "Train Loss: 1.2932384014129639    Train Acc:  0.640625  at batch 440.\n",
            "Train Loss: 1.2160611152648926    Train Acc:  0.75  at batch 441.\n",
            "Train Loss: 1.180931568145752    Train Acc:  0.703125  at batch 442.\n",
            "Train Loss: 1.0559459924697876    Train Acc:  0.84375  at batch 443.\n",
            "Train Loss: 1.0683201551437378    Train Acc:  0.828125  at batch 444.\n",
            "Train Loss: 1.1336861848831177    Train Acc:  0.796875  at batch 445.\n",
            "Train Loss: 1.1148732900619507    Train Acc:  0.78125  at batch 446.\n",
            "Train Loss: 1.168225646018982    Train Acc:  0.734375  at batch 447.\n",
            "Train Loss: 1.1706581115722656    Train Acc:  0.8125  at batch 448.\n",
            "Train Loss: 1.1940298080444336    Train Acc:  0.734375  at batch 449.\n",
            "Train Loss: 1.1172524690628052    Train Acc:  0.75  at batch 450.\n",
            "Train Loss: 1.2007893323898315    Train Acc:  0.71875  at batch 451.\n",
            "Train Loss: 1.0000430345535278    Train Acc:  0.84375  at batch 452.\n",
            "Train Loss: 1.1962031126022339    Train Acc:  0.78125  at batch 453.\n",
            "Train Loss: 1.150977373123169    Train Acc:  0.796875  at batch 454.\n",
            "Train Loss: 1.0065343379974365    Train Acc:  0.859375  at batch 455.\n",
            "Train Loss: 1.1819493770599365    Train Acc:  0.734375  at batch 456.\n",
            "Train Loss: 1.071954369544983    Train Acc:  0.828125  at batch 457.\n",
            "Train Loss: 1.0607773065567017    Train Acc:  0.75  at batch 458.\n",
            "Train Loss: 1.2572871446609497    Train Acc:  0.6875  at batch 459.\n",
            "Train Loss: 1.152769923210144    Train Acc:  0.78125  at batch 460.\n",
            "Train Loss: 1.1396747827529907    Train Acc:  0.78125  at batch 461.\n",
            "Train Loss: 1.0978946685791016    Train Acc:  0.796875  at batch 462.\n",
            "Train Loss: 1.1498758792877197    Train Acc:  0.734375  at batch 463.\n",
            "Train Loss: 1.1274405717849731    Train Acc:  0.796875  at batch 464.\n",
            "Train Loss: 1.1815340518951416    Train Acc:  0.734375  at batch 465.\n",
            "Train Loss: 1.1161516904830933    Train Acc:  0.796875  at batch 466.\n",
            "Train Loss: 1.134404182434082    Train Acc:  0.796875  at batch 467.\n",
            "Train Loss: 1.1271169185638428    Train Acc:  0.765625  at batch 468.\n",
            "Train Loss: 1.0747313499450684    Train Acc:  0.84375  at batch 469.\n",
            "Train Loss: 1.0634571313858032    Train Acc:  0.75  at batch 470.\n",
            "Train Loss: 1.047726035118103    Train Acc:  0.84375  at batch 471.\n",
            "Train Loss: 1.084092617034912    Train Acc:  0.78125  at batch 472.\n",
            "Train Loss: 1.227987289428711    Train Acc:  0.734375  at batch 473.\n",
            "Train Loss: 1.101373314857483    Train Acc:  0.828125  at batch 474.\n",
            "Train Loss: 1.105514407157898    Train Acc:  0.78125  at batch 475.\n",
            "Train Loss: 1.1905826330184937    Train Acc:  0.765625  at batch 476.\n",
            "Train Loss: 1.095982551574707    Train Acc:  0.796875  at batch 477.\n",
            "Train Loss: 1.0801328420639038    Train Acc:  0.78125  at batch 478.\n",
            "Train Loss: 1.2064110040664673    Train Acc:  0.6875  at batch 479.\n",
            "Train Loss: 1.1910593509674072    Train Acc:  0.671875  at batch 480.\n",
            "Train Loss: 0.9851227402687073    Train Acc:  0.84375  at batch 481.\n",
            "Train Loss: 1.1375558376312256    Train Acc:  0.75  at batch 482.\n",
            "Train Loss: 1.2505860328674316    Train Acc:  0.703125  at batch 483.\n",
            "Train Loss: 1.1333197355270386    Train Acc:  0.765625  at batch 484.\n",
            "Train Loss: 1.121180534362793    Train Acc:  0.734375  at batch 485.\n",
            "Train Loss: 1.1154239177703857    Train Acc:  0.75  at batch 486.\n",
            "Train Loss: 1.096307635307312    Train Acc:  0.78125  at batch 487.\n",
            "Train Loss: 1.190908670425415    Train Acc:  0.765625  at batch 488.\n",
            "Train Loss: 1.1576895713806152    Train Acc:  0.78125  at batch 489.\n",
            "Train Loss: 1.068764567375183    Train Acc:  0.828125  at batch 490.\n",
            "Train Loss: 1.1218491792678833    Train Acc:  0.71875  at batch 491.\n",
            "Train Loss: 1.1894793510437012    Train Acc:  0.75  at batch 492.\n",
            "Train Loss: 1.0925298929214478    Train Acc:  0.765625  at batch 493.\n",
            "Train Loss: 1.0527729988098145    Train Acc:  0.796875  at batch 494.\n",
            "Train Loss: 0.9697363376617432    Train Acc:  0.84375  at batch 495.\n",
            "Train Loss: 1.1467336416244507    Train Acc:  0.78125  at batch 496.\n",
            "Train Loss: 1.133146047592163    Train Acc:  0.765625  at batch 497.\n",
            "Train Loss: 1.063767910003662    Train Acc:  0.734375  at batch 498.\n",
            "Train Loss: 1.0682222843170166    Train Acc:  0.828125  at batch 499.\n",
            "Train Loss: 1.0603628158569336    Train Acc:  0.828125  at batch 500.\n",
            "Train Loss: 1.123375415802002    Train Acc:  0.75  at batch 501.\n",
            "Train Loss: 1.1265076398849487    Train Acc:  0.71875  at batch 502.\n",
            "Train Loss: 1.082413673400879    Train Acc:  0.75  at batch 503.\n",
            "Train Loss: 1.150341510772705    Train Acc:  0.71875  at batch 504.\n",
            "Train Loss: 1.0693697929382324    Train Acc:  0.8125  at batch 505.\n",
            "Train Loss: 1.0131990909576416    Train Acc:  0.828125  at batch 506.\n",
            "Train Loss: 1.1644811630249023    Train Acc:  0.734375  at batch 507.\n",
            "Train Loss: 1.219477653503418    Train Acc:  0.734375  at batch 508.\n",
            "Train Loss: 1.0944666862487793    Train Acc:  0.859375  at batch 509.\n",
            "Train Loss: 1.072176218032837    Train Acc:  0.78125  at batch 510.\n",
            "Train Loss: 1.191971778869629    Train Acc:  0.703125  at batch 511.\n",
            "Train Loss: 1.047022819519043    Train Acc:  0.796875  at batch 512.\n",
            "Train Loss: 1.1081163883209229    Train Acc:  0.78125  at batch 513.\n",
            "Train Loss: 1.1510748863220215    Train Acc:  0.78125  at batch 514.\n",
            "Train Loss: 1.1053308248519897    Train Acc:  0.78125  at batch 515.\n",
            "Train Loss: 1.1037211418151855    Train Acc:  0.765625  at batch 516.\n",
            "Train Loss: 1.1400642395019531    Train Acc:  0.796875  at batch 517.\n",
            "Train Loss: 1.1481409072875977    Train Acc:  0.78125  at batch 518.\n",
            "Train Loss: 1.0296828746795654    Train Acc:  0.765625  at batch 519.\n",
            "Train Loss: 1.0702826976776123    Train Acc:  0.75  at batch 520.\n",
            "Train Loss: 1.1365389823913574    Train Acc:  0.765625  at batch 521.\n",
            "Train Loss: 0.9946041703224182    Train Acc:  0.859375  at batch 522.\n",
            "Train Loss: 1.1329439878463745    Train Acc:  0.765625  at batch 523.\n",
            "Train Loss: 1.1433038711547852    Train Acc:  0.75  at batch 524.\n",
            "Train Loss: 1.1607686281204224    Train Acc:  0.75  at batch 525.\n",
            "Train Loss: 1.2313706874847412    Train Acc:  0.71875  at batch 526.\n",
            "Train Loss: 1.1782548427581787    Train Acc:  0.78125  at batch 527.\n",
            "Train Loss: 1.183282494544983    Train Acc:  0.6875  at batch 528.\n",
            "Train Loss: 1.195528507232666    Train Acc:  0.765625  at batch 529.\n",
            "Train Loss: 1.1337432861328125    Train Acc:  0.765625  at batch 530.\n",
            "Train Loss: 1.055486798286438    Train Acc:  0.859375  at batch 531.\n",
            "Train Loss: 1.0992026329040527    Train Acc:  0.765625  at batch 532.\n",
            "Train Loss: 1.2623114585876465    Train Acc:  0.6875  at batch 533.\n",
            "Train Loss: 1.1264877319335938    Train Acc:  0.71875  at batch 534.\n",
            "Train Loss: 1.0731229782104492    Train Acc:  0.828125  at batch 535.\n",
            "Train Loss: 1.0875109434127808    Train Acc:  0.765625  at batch 536.\n",
            "Train Loss: 1.188559889793396    Train Acc:  0.78125  at batch 537.\n",
            "Train Loss: 1.0102009773254395    Train Acc:  0.859375  at batch 538.\n",
            "Train Loss: 1.0495675802230835    Train Acc:  0.8125  at batch 539.\n",
            "Train Loss: 1.084175705909729    Train Acc:  0.828125  at batch 540.\n",
            "Train Loss: 1.0922050476074219    Train Acc:  0.765625  at batch 541.\n",
            "Train Loss: 1.0180585384368896    Train Acc:  0.796875  at batch 542.\n",
            "Train Loss: 1.0862308740615845    Train Acc:  0.78125  at batch 543.\n",
            "Train Loss: 1.1398580074310303    Train Acc:  0.71875  at batch 544.\n",
            "Train Loss: 1.0635535717010498    Train Acc:  0.78125  at batch 545.\n",
            "Train Loss: 1.0684088468551636    Train Acc:  0.859375  at batch 546.\n",
            "Train Loss: 0.9781166911125183    Train Acc:  0.890625  at batch 547.\n",
            "Train Loss: 1.0600764751434326    Train Acc:  0.765625  at batch 548.\n",
            "Train Loss: 1.0966769456863403    Train Acc:  0.828125  at batch 549.\n",
            "Train Loss: 1.120767593383789    Train Acc:  0.71875  at batch 550.\n",
            "Train Loss: 1.23544442653656    Train Acc:  0.6875  at batch 551.\n",
            "Train Loss: 1.0775851011276245    Train Acc:  0.796875  at batch 552.\n",
            "Train Loss: 1.0900846719741821    Train Acc:  0.75  at batch 553.\n",
            "Train Loss: 1.1600919961929321    Train Acc:  0.796875  at batch 554.\n",
            "Train Loss: 1.2096433639526367    Train Acc:  0.71875  at batch 555.\n",
            "Train Loss: 0.9943416714668274    Train Acc:  0.84375  at batch 556.\n",
            "Train Loss: 1.0333409309387207    Train Acc:  0.84375  at batch 557.\n",
            "Train Loss: 1.0677804946899414    Train Acc:  0.8125  at batch 558.\n",
            "Train Loss: 1.0928648710250854    Train Acc:  0.765625  at batch 559.\n",
            "Train Loss: 1.1258914470672607    Train Acc:  0.8125  at batch 560.\n",
            "Train Loss: 1.1648722887039185    Train Acc:  0.71875  at batch 561.\n",
            "Train Loss: 1.1352070569992065    Train Acc:  0.71875  at batch 562.\n",
            "Train Loss: 1.169185757637024    Train Acc:  0.78125  at batch 563.\n",
            "Train Loss: 1.1138490438461304    Train Acc:  0.796875  at batch 564.\n",
            "Train Loss: 1.127029538154602    Train Acc:  0.765625  at batch 565.\n",
            "Train Loss: 1.0710291862487793    Train Acc:  0.796875  at batch 566.\n",
            "Train Loss: 1.0245710611343384    Train Acc:  0.828125  at batch 567.\n",
            "Train Loss: 1.0434839725494385    Train Acc:  0.828125  at batch 568.\n",
            "Train Loss: 1.0263850688934326    Train Acc:  0.828125  at batch 569.\n",
            "Train Loss: 1.073580026626587    Train Acc:  0.78125  at batch 570.\n",
            "Train Loss: 1.1698253154754639    Train Acc:  0.734375  at batch 571.\n",
            "Train Loss: 1.068433403968811    Train Acc:  0.734375  at batch 572.\n",
            "Train Loss: 1.1327569484710693    Train Acc:  0.71875  at batch 573.\n",
            "Train Loss: 1.160142421722412    Train Acc:  0.78125  at batch 574.\n",
            "Train Loss: 0.9949893951416016    Train Acc:  0.84375  at batch 575.\n",
            "Train Loss: 0.9695013761520386    Train Acc:  0.859375  at batch 576.\n",
            "Train Loss: 1.1777039766311646    Train Acc:  0.71875  at batch 577.\n",
            "Train Loss: 1.0757803916931152    Train Acc:  0.8125  at batch 578.\n",
            "Train Loss: 1.0607713460922241    Train Acc:  0.78125  at batch 579.\n",
            "Train Loss: 1.1430248022079468    Train Acc:  0.75  at batch 580.\n",
            "Train Loss: 1.0993659496307373    Train Acc:  0.765625  at batch 581.\n",
            "Train Loss: 1.1146191358566284    Train Acc:  0.78125  at batch 582.\n",
            "Train Loss: 0.9886478781700134    Train Acc:  0.796875  at batch 583.\n",
            "Train Loss: 1.137760877609253    Train Acc:  0.78125  at batch 584.\n",
            "Train Loss: 1.1134172677993774    Train Acc:  0.78125  at batch 585.\n",
            "Train Loss: 1.0269049406051636    Train Acc:  0.796875  at batch 586.\n",
            "Train Loss: 1.0261616706848145    Train Acc:  0.78125  at batch 587.\n",
            "Train Loss: 0.9502496123313904    Train Acc:  0.90625  at batch 588.\n",
            "Train Loss: 1.1119511127471924    Train Acc:  0.78125  at batch 589.\n",
            "Train Loss: 1.2713841199874878    Train Acc:  0.671875  at batch 590.\n",
            "Train Loss: 0.9956349730491638    Train Acc:  0.859375  at batch 591.\n",
            "Train Loss: 1.0767804384231567    Train Acc:  0.78125  at batch 592.\n",
            "Train Loss: 1.189637541770935    Train Acc:  0.6875  at batch 593.\n",
            "Train Loss: 1.263543963432312    Train Acc:  0.671875  at batch 594.\n",
            "Train Loss: 1.074569582939148    Train Acc:  0.78125  at batch 595.\n",
            "Train Loss: 1.1266735792160034    Train Acc:  0.796875  at batch 596.\n",
            "Train Loss: 1.0311874151229858    Train Acc:  0.765625  at batch 597.\n",
            "Train Loss: 0.9756097793579102    Train Acc:  0.890625  at batch 598.\n",
            "Train Loss: 1.0479463338851929    Train Acc:  0.828125  at batch 599.\n",
            "Train Loss: 1.1219353675842285    Train Acc:  0.796875  at batch 600.\n",
            "Train Loss: 1.0784060955047607    Train Acc:  0.796875  at batch 601.\n",
            "Train Loss: 1.0859042406082153    Train Acc:  0.75  at batch 602.\n",
            "Train Loss: 0.9942712187767029    Train Acc:  0.875  at batch 603.\n",
            "Train Loss: 0.9312305450439453    Train Acc:  0.875  at batch 604.\n",
            "Train Loss: 1.0829664468765259    Train Acc:  0.78125  at batch 605.\n",
            "Train Loss: 1.1854289770126343    Train Acc:  0.75  at batch 606.\n",
            "Train Loss: 1.0785534381866455    Train Acc:  0.796875  at batch 607.\n",
            "Train Loss: 1.1233463287353516    Train Acc:  0.6875  at batch 608.\n",
            "Train Loss: 1.0386912822723389    Train Acc:  0.796875  at batch 609.\n",
            "Train Loss: 0.9727513790130615    Train Acc:  0.78125  at batch 610.\n",
            "Train Loss: 1.0393801927566528    Train Acc:  0.734375  at batch 611.\n",
            "Train Loss: 1.1730637550354004    Train Acc:  0.734375  at batch 612.\n",
            "Train Loss: 1.1119935512542725    Train Acc:  0.78125  at batch 613.\n",
            "Train Loss: 0.9124743342399597    Train Acc:  0.859375  at batch 614.\n",
            "Train Loss: 0.9940486550331116    Train Acc:  0.828125  at batch 615.\n",
            "Train Loss: 0.9754775166511536    Train Acc:  0.859375  at batch 616.\n",
            "Train Loss: 1.1034133434295654    Train Acc:  0.8125  at batch 617.\n",
            "Train Loss: 0.8970580697059631    Train Acc:  0.84375  at batch 618.\n",
            "Train Loss: 1.0672614574432373    Train Acc:  0.796875  at batch 619.\n",
            "Train Loss: 1.0942800045013428    Train Acc:  0.8125  at batch 620.\n",
            "Train Loss: 1.0176910161972046    Train Acc:  0.8125  at batch 621.\n",
            "Train Loss: 0.9437318444252014    Train Acc:  0.78125  at batch 622.\n",
            "Train Loss: 1.03473699092865    Train Acc:  0.8125  at batch 623.\n",
            "Train Loss: 1.1031445264816284    Train Acc:  0.78125  at batch 624.\n",
            "Train Loss: 1.0168285369873047    Train Acc:  0.8125  at batch 625.\n",
            "Train Loss: 1.0260854959487915    Train Acc:  0.8125  at batch 626.\n",
            "Train Loss: 1.0142343044281006    Train Acc:  0.859375  at batch 627.\n",
            "Train Loss: 1.1561921834945679    Train Acc:  0.734375  at batch 628.\n",
            "Train Loss: 0.9991921186447144    Train Acc:  0.796875  at batch 629.\n",
            "Train Loss: 1.0476292371749878    Train Acc:  0.734375  at batch 630.\n",
            "Train Loss: 1.10393226146698    Train Acc:  0.796875  at batch 631.\n",
            "Train Loss: 1.0169494152069092    Train Acc:  0.796875  at batch 632.\n",
            "Train Loss: 1.113777756690979    Train Acc:  0.796875  at batch 633.\n",
            "Train Loss: 1.022335410118103    Train Acc:  0.796875  at batch 634.\n",
            "Train Loss: 1.0265902280807495    Train Acc:  0.828125  at batch 635.\n",
            "Train Loss: 1.1818245649337769    Train Acc:  0.734375  at batch 636.\n",
            "Train Loss: 1.1317881345748901    Train Acc:  0.8125  at batch 637.\n",
            "Train Loss: 1.1326048374176025    Train Acc:  0.75  at batch 638.\n",
            "Train Loss: 1.0563560724258423    Train Acc:  0.828125  at batch 639.\n",
            "Train Loss: 1.069422721862793    Train Acc:  0.828125  at batch 640.\n",
            "Train Loss: 1.1179094314575195    Train Acc:  0.734375  at batch 641.\n",
            "Train Loss: 0.9975739121437073    Train Acc:  0.84375  at batch 642.\n",
            "Train Loss: 1.0595567226409912    Train Acc:  0.8125  at batch 643.\n",
            "Train Loss: 1.025494933128357    Train Acc:  0.828125  at batch 644.\n",
            "Train Loss: 1.0487515926361084    Train Acc:  0.75  at batch 645.\n",
            "Train Loss: 1.07809317111969    Train Acc:  0.8125  at batch 646.\n",
            "Train Loss: 1.0782432556152344    Train Acc:  0.84375  at batch 647.\n",
            "Train Loss: 0.9856983423233032    Train Acc:  0.75  at batch 648.\n",
            "Train Loss: 1.0586601495742798    Train Acc:  0.8125  at batch 649.\n",
            "Train Loss: 1.002120852470398    Train Acc:  0.8125  at batch 650.\n",
            "Train Loss: 1.168923020362854    Train Acc:  0.6875  at batch 651.\n",
            "Train Loss: 1.1054741144180298    Train Acc:  0.765625  at batch 652.\n",
            "Train Loss: 1.1451445817947388    Train Acc:  0.703125  at batch 653.\n",
            "Train Loss: 1.049370527267456    Train Acc:  0.796875  at batch 654.\n",
            "Train Loss: 0.9985339045524597    Train Acc:  0.765625  at batch 655.\n",
            "Train Loss: 1.1116582155227661    Train Acc:  0.765625  at batch 656.\n",
            "Train Loss: 1.0164729356765747    Train Acc:  0.796875  at batch 657.\n",
            "Train Loss: 1.0680855512619019    Train Acc:  0.6875  at batch 658.\n",
            "Train Loss: 0.9836060404777527    Train Acc:  0.78125  at batch 659.\n",
            "Train Loss: 1.095859408378601    Train Acc:  0.796875  at batch 660.\n",
            "Train Loss: 1.0520639419555664    Train Acc:  0.8125  at batch 661.\n",
            "Train Loss: 1.0837682485580444    Train Acc:  0.78125  at batch 662.\n",
            "Train Loss: 1.0191138982772827    Train Acc:  0.78125  at batch 663.\n",
            "Train Loss: 0.9392163157463074    Train Acc:  0.84375  at batch 664.\n",
            "Train Loss: 1.004181146621704    Train Acc:  0.71875  at batch 665.\n",
            "Train Loss: 0.9194298386573792    Train Acc:  0.78125  at batch 666.\n",
            "Train Loss: 1.0580636262893677    Train Acc:  0.84375  at batch 667.\n",
            "Train Loss: 1.1035243272781372    Train Acc:  0.75  at batch 668.\n",
            "Train Loss: 1.0660449266433716    Train Acc:  0.734375  at batch 669.\n",
            "Train Loss: 0.9772284626960754    Train Acc:  0.859375  at batch 670.\n",
            "Train Loss: 1.0647327899932861    Train Acc:  0.71875  at batch 671.\n",
            "Train Loss: 1.108258605003357    Train Acc:  0.75  at batch 672.\n",
            "Train Loss: 1.1347298622131348    Train Acc:  0.75  at batch 673.\n",
            "Train Loss: 1.0864678621292114    Train Acc:  0.765625  at batch 674.\n",
            "Train Loss: 0.9868043065071106    Train Acc:  0.84375  at batch 675.\n",
            "Train Loss: 0.9009715914726257    Train Acc:  0.796875  at batch 676.\n",
            "Train Loss: 0.9871760606765747    Train Acc:  0.8125  at batch 677.\n",
            "Train Loss: 1.0060224533081055    Train Acc:  0.8125  at batch 678.\n",
            "Train Loss: 1.065949559211731    Train Acc:  0.8125  at batch 679.\n",
            "Train Loss: 1.046726942062378    Train Acc:  0.765625  at batch 680.\n",
            "Train Loss: 0.9929845929145813    Train Acc:  0.78125  at batch 681.\n",
            "Train Loss: 1.0859653949737549    Train Acc:  0.734375  at batch 682.\n",
            "Train Loss: 1.011993408203125    Train Acc:  0.78125  at batch 683.\n",
            "Train Loss: 0.9783645868301392    Train Acc:  0.828125  at batch 684.\n",
            "Train Loss: 1.0858185291290283    Train Acc:  0.890625  at batch 685.\n",
            "Train Loss: 1.0992177724838257    Train Acc:  0.75  at batch 686.\n",
            "Train Loss: 1.052512288093567    Train Acc:  0.765625  at batch 687.\n",
            "Train Loss: 1.1631720066070557    Train Acc:  0.734375  at batch 688.\n",
            "Train Loss: 0.9345338940620422    Train Acc:  0.90625  at batch 689.\n",
            "Train Loss: 1.0602048635482788    Train Acc:  0.828125  at batch 690.\n",
            "Train Loss: 0.972136378288269    Train Acc:  0.84375  at batch 691.\n",
            "Train Loss: 1.0848045349121094    Train Acc:  0.78125  at batch 692.\n",
            "Train Loss: 1.1727906465530396    Train Acc:  0.6875  at batch 693.\n",
            "Train Loss: 1.1664949655532837    Train Acc:  0.75  at batch 694.\n",
            "Train Loss: 1.108432650566101    Train Acc:  0.734375  at batch 695.\n",
            "Train Loss: 1.0013988018035889    Train Acc:  0.84375  at batch 696.\n",
            "Train Loss: 0.9870128631591797    Train Acc:  0.765625  at batch 697.\n",
            "Train Loss: 1.0291128158569336    Train Acc:  0.765625  at batch 698.\n",
            "Train Loss: 1.0633623600006104    Train Acc:  0.734375  at batch 699.\n",
            "Train Loss: 1.0955108404159546    Train Acc:  0.703125  at batch 700.\n",
            "Train Loss: 0.9880632758140564    Train Acc:  0.8125  at batch 701.\n",
            "Train Loss: 1.2061843872070312    Train Acc:  0.6875  at batch 702.\n",
            "Train Loss: 1.0395581722259521    Train Acc:  0.828125  at batch 703.\n",
            "Train Loss: 1.0187809467315674    Train Acc:  0.75  at batch 704.\n",
            "Train Loss: 1.0229620933532715    Train Acc:  0.765625  at batch 705.\n",
            "Train Loss: 1.1798738241195679    Train Acc:  0.6875  at batch 706.\n",
            "Train Loss: 1.0377082824707031    Train Acc:  0.734375  at batch 707.\n",
            "Train Loss: 1.0852335691452026    Train Acc:  0.828125  at batch 708.\n",
            "Train Loss: 0.968569278717041    Train Acc:  0.90625  at batch 709.\n",
            "Train Loss: 0.9914624094963074    Train Acc:  0.8125  at batch 710.\n",
            "Train Loss: 1.0408293008804321    Train Acc:  0.75  at batch 711.\n",
            "Train Loss: 1.1282985210418701    Train Acc:  0.734375  at batch 712.\n",
            "Train Loss: 1.0743210315704346    Train Acc:  0.765625  at batch 713.\n",
            "Train Loss: 1.001348853111267    Train Acc:  0.8125  at batch 714.\n",
            "Train Loss: 1.0435878038406372    Train Acc:  0.765625  at batch 715.\n",
            "Train Loss: 1.0404189825057983    Train Acc:  0.765625  at batch 716.\n",
            "Train Loss: 1.0316927433013916    Train Acc:  0.796875  at batch 717.\n",
            "Train Loss: 1.1127723455429077    Train Acc:  0.796875  at batch 718.\n",
            "Train Loss: 1.0250660181045532    Train Acc:  0.8125  at batch 719.\n",
            "Train Loss: 1.0979934930801392    Train Acc:  0.75  at batch 720.\n",
            "Train Loss: 1.035386323928833    Train Acc:  0.796875  at batch 721.\n",
            "Train Loss: 1.1444703340530396    Train Acc:  0.75  at batch 722.\n",
            "Train Loss: 0.9679072499275208    Train Acc:  0.890625  at batch 723.\n",
            "Train Loss: 1.069936752319336    Train Acc:  0.84375  at batch 724.\n",
            "Train Loss: 1.0054932832717896    Train Acc:  0.8125  at batch 725.\n",
            "Train Loss: 0.8862901329994202    Train Acc:  0.875  at batch 726.\n",
            "Train Loss: 1.0799461603164673    Train Acc:  0.828125  at batch 727.\n",
            "Train Loss: 1.067410945892334    Train Acc:  0.71875  at batch 728.\n",
            "Train Loss: 0.9938290119171143    Train Acc:  0.859375  at batch 729.\n",
            "Train Loss: 1.0101839303970337    Train Acc:  0.859375  at batch 730.\n",
            "Train Loss: 0.9996013045310974    Train Acc:  0.8125  at batch 731.\n",
            "Train Loss: 0.974805474281311    Train Acc:  0.828125  at batch 732.\n",
            "Train Loss: 1.0603092908859253    Train Acc:  0.71875  at batch 733.\n",
            "Train Loss: 1.0139156579971313    Train Acc:  0.765625  at batch 734.\n",
            "Train Loss: 0.8498859405517578    Train Acc:  0.875  at batch 735.\n",
            "Train Loss: 0.9227849245071411    Train Acc:  0.8125  at batch 736.\n",
            "Train Loss: 1.0709261894226074    Train Acc:  0.828125  at batch 737.\n",
            "Train Loss: 1.1324214935302734    Train Acc:  0.71875  at batch 738.\n",
            "Train Loss: 1.080422043800354    Train Acc:  0.8125  at batch 739.\n",
            "Train Loss: 1.1225919723510742    Train Acc:  0.703125  at batch 740.\n",
            "Train Loss: 1.0962469577789307    Train Acc:  0.734375  at batch 741.\n",
            "Train Loss: 0.9408107995986938    Train Acc:  0.84375  at batch 742.\n",
            "Train Loss: 1.0829048156738281    Train Acc:  0.75  at batch 743.\n",
            "Train Loss: 1.0103540420532227    Train Acc:  0.828125  at batch 744.\n",
            "Train Loss: 1.119391679763794    Train Acc:  0.765625  at batch 745.\n",
            "Train Loss: 0.8941799998283386    Train Acc:  0.84375  at batch 746.\n",
            "Train Loss: 1.0293912887573242    Train Acc:  0.75  at batch 747.\n",
            "Train Loss: 1.0363507270812988    Train Acc:  0.890625  at batch 748.\n",
            "Train Loss: 0.9767848253250122    Train Acc:  0.90625  at batch 749.\n",
            "Train Loss: 0.9967659711837769    Train Acc:  0.796875  at batch 750.\n",
            "Train Loss: 0.9674324989318848    Train Acc:  0.859375  at batch 751.\n",
            "Train Loss: 1.053427815437317    Train Acc:  0.8125  at batch 752.\n",
            "Train Loss: 1.0066806077957153    Train Acc:  0.78125  at batch 753.\n",
            "Train Loss: 0.9940429925918579    Train Acc:  0.8125  at batch 754.\n",
            "Train Loss: 0.9616557359695435    Train Acc:  0.828125  at batch 755.\n",
            "Train Loss: 0.8268278241157532    Train Acc:  0.9375  at batch 756.\n",
            "Train Loss: 1.0326018333435059    Train Acc:  0.734375  at batch 757.\n",
            "Train Loss: 1.1202538013458252    Train Acc:  0.78125  at batch 758.\n",
            "Train Loss: 0.9831830859184265    Train Acc:  0.75  at batch 759.\n",
            "Train Loss: 1.090013861656189    Train Acc:  0.796875  at batch 760.\n",
            "Train Loss: 1.0195380449295044    Train Acc:  0.8125  at batch 761.\n",
            "Train Loss: 1.0852068662643433    Train Acc:  0.765625  at batch 762.\n",
            "Train Loss: 1.0617036819458008    Train Acc:  0.703125  at batch 763.\n",
            "Train Loss: 0.9634969830513    Train Acc:  0.875  at batch 764.\n",
            "Train Loss: 0.957558274269104    Train Acc:  0.828125  at batch 765.\n",
            "Train Loss: 0.9728158712387085    Train Acc:  0.8125  at batch 766.\n",
            "Train Loss: 1.0373104810714722    Train Acc:  0.6875  at batch 767.\n",
            "Train Loss: 0.9634782075881958    Train Acc:  0.859375  at batch 768.\n",
            "Train Loss: 1.1301034688949585    Train Acc:  0.6875  at batch 769.\n",
            "Train Loss: 0.9427679777145386    Train Acc:  0.84375  at batch 770.\n",
            "Train Loss: 1.0216835737228394    Train Acc:  0.796875  at batch 771.\n",
            "Train Loss: 1.0590354204177856    Train Acc:  0.765625  at batch 772.\n",
            "Train Loss: 1.0681618452072144    Train Acc:  0.765625  at batch 773.\n",
            "Train Loss: 0.9523931741714478    Train Acc:  0.78125  at batch 774.\n",
            "Train Loss: 1.0840668678283691    Train Acc:  0.828125  at batch 775.\n",
            "Train Loss: 1.0953408479690552    Train Acc:  0.78125  at batch 776.\n",
            "Train Loss: 0.9477782249450684    Train Acc:  0.796875  at batch 777.\n",
            "Train Loss: 1.053113341331482    Train Acc:  0.765625  at batch 778.\n",
            "Train Loss: 0.9663983583450317    Train Acc:  0.859375  at batch 779.\n",
            "Train Loss: 1.1002134084701538    Train Acc:  0.71875  at batch 780.\n",
            "Train Loss: 1.1870031356811523    Train Acc:  0.6875  at batch 781.\n",
            "Valid Loss: 0.9409219622612    Valid Acc:  0.78125  at batch 0.\n",
            "Valid Loss: 0.9181614518165588    Valid Acc:  0.8125  at batch 1.\n",
            "Valid Loss: 0.9361740350723267    Valid Acc:  0.84375  at batch 2.\n",
            "Valid Loss: 1.0939841270446777    Valid Acc:  0.75  at batch 3.\n",
            "Valid Loss: 0.9970929026603699    Valid Acc:  0.8125  at batch 4.\n",
            "Valid Loss: 1.2144123315811157    Valid Acc:  0.6875  at batch 5.\n",
            "Valid Loss: 1.0086230039596558    Valid Acc:  0.8125  at batch 6.\n",
            "Valid Loss: 1.094415545463562    Valid Acc:  0.6875  at batch 7.\n",
            "Valid Loss: 0.9099321961402893    Valid Acc:  0.875  at batch 8.\n",
            "Valid Loss: 0.8579544425010681    Valid Acc:  0.875  at batch 9.\n",
            "Valid Loss: 1.0933363437652588    Valid Acc:  0.75  at batch 10.\n",
            "Valid Loss: 1.0674139261245728    Valid Acc:  0.75  at batch 11.\n",
            "Valid Loss: 1.0616555213928223    Valid Acc:  0.78125  at batch 12.\n",
            "Valid Loss: 0.998271644115448    Valid Acc:  0.71875  at batch 13.\n",
            "Valid Loss: 0.8670815825462341    Valid Acc:  0.8125  at batch 14.\n",
            "Valid Loss: 1.03380286693573    Valid Acc:  0.75  at batch 15.\n",
            "Valid Loss: 0.9980113506317139    Valid Acc:  0.71875  at batch 16.\n",
            "Valid Loss: 1.0331590175628662    Valid Acc:  0.75  at batch 17.\n",
            "Valid Loss: 1.1288249492645264    Valid Acc:  0.78125  at batch 18.\n",
            "Valid Loss: 1.0564603805541992    Valid Acc:  0.78125  at batch 19.\n",
            "Valid Loss: 0.9957568049430847    Valid Acc:  0.875  at batch 20.\n",
            "Valid Loss: 1.0206308364868164    Valid Acc:  0.78125  at batch 21.\n",
            "Valid Loss: 0.8977526426315308    Valid Acc:  0.84375  at batch 22.\n",
            "Valid Loss: 0.916269063949585    Valid Acc:  0.875  at batch 23.\n",
            "Valid Loss: 0.9889488220214844    Valid Acc:  0.71875  at batch 24.\n",
            "Valid Loss: 0.8999770879745483    Valid Acc:  0.84375  at batch 25.\n",
            "Valid Loss: 1.1021183729171753    Valid Acc:  0.78125  at batch 26.\n",
            "Valid Loss: 1.0328177213668823    Valid Acc:  0.75  at batch 27.\n",
            "Valid Loss: 0.8228458762168884    Valid Acc:  0.84375  at batch 28.\n",
            "Valid Loss: 0.9679133296012878    Valid Acc:  0.6875  at batch 29.\n",
            "Valid Loss: 0.9794214367866516    Valid Acc:  0.84375  at batch 30.\n",
            "Valid Loss: 0.8850085139274597    Valid Acc:  0.875  at batch 31.\n",
            "Valid Loss: 0.8840109705924988    Valid Acc:  0.84375  at batch 32.\n",
            "Valid Loss: 0.9633325934410095    Valid Acc:  0.8125  at batch 33.\n",
            "Valid Loss: 1.059613585472107    Valid Acc:  0.71875  at batch 34.\n",
            "Valid Loss: 0.8829234838485718    Valid Acc:  0.75  at batch 35.\n",
            "Valid Loss: 1.1665115356445312    Valid Acc:  0.78125  at batch 36.\n",
            "Valid Loss: 1.1560789346694946    Valid Acc:  0.75  at batch 37.\n",
            "Valid Loss: 1.0957567691802979    Valid Acc:  0.6875  at batch 38.\n",
            "Valid Loss: 0.872810423374176    Valid Acc:  0.84375  at batch 39.\n",
            "Valid Loss: 0.9141690731048584    Valid Acc:  0.84375  at batch 40.\n",
            "Valid Loss: 0.9963636994361877    Valid Acc:  0.875  at batch 41.\n",
            "Valid Loss: 0.8625027537345886    Valid Acc:  0.8125  at batch 42.\n",
            "Valid Loss: 0.9053575396537781    Valid Acc:  0.78125  at batch 43.\n",
            "Valid Loss: 1.0553964376449585    Valid Acc:  0.71875  at batch 44.\n",
            "Valid Loss: 0.9769933223724365    Valid Acc:  0.84375  at batch 45.\n",
            "Valid Loss: 0.8532840013504028    Valid Acc:  0.90625  at batch 46.\n",
            "Valid Loss: 1.0836718082427979    Valid Acc:  0.875  at batch 47.\n",
            "Valid Loss: 1.018730640411377    Valid Acc:  0.71875  at batch 48.\n",
            "Valid Loss: 0.9678540825843811    Valid Acc:  0.78125  at batch 49.\n",
            "Valid Loss: 0.8750622868537903    Valid Acc:  0.875  at batch 50.\n",
            "Valid Loss: 1.0298212766647339    Valid Acc:  0.78125  at batch 51.\n",
            "Valid Loss: 0.9289836883544922    Valid Acc:  0.84375  at batch 52.\n",
            "Valid Loss: 0.9361862540245056    Valid Acc:  0.78125  at batch 53.\n",
            "Valid Loss: 1.2111049890518188    Valid Acc:  0.71875  at batch 54.\n",
            "Valid Loss: 1.0326803922653198    Valid Acc:  0.71875  at batch 55.\n",
            "Valid Loss: 1.0744866132736206    Valid Acc:  0.78125  at batch 56.\n",
            "Valid Loss: 0.8552803993225098    Valid Acc:  0.8125  at batch 57.\n",
            "Valid Loss: 0.956526517868042    Valid Acc:  0.71875  at batch 58.\n",
            "Valid Loss: 0.9157040119171143    Valid Acc:  0.875  at batch 59.\n",
            "Valid Loss: 0.9321470856666565    Valid Acc:  0.875  at batch 60.\n",
            "Valid Loss: 1.0343703031539917    Valid Acc:  0.8125  at batch 61.\n",
            "Valid Loss: 0.8994098901748657    Valid Acc:  0.84375  at batch 62.\n",
            "Valid Loss: 0.9600784182548523    Valid Acc:  0.6875  at batch 63.\n",
            "Valid Loss: 1.1161952018737793    Valid Acc:  0.71875  at batch 64.\n",
            "Valid Loss: 0.9706262350082397    Valid Acc:  0.8125  at batch 65.\n",
            "Valid Loss: 0.9102593064308167    Valid Acc:  0.78125  at batch 66.\n",
            "Valid Loss: 0.9716274738311768    Valid Acc:  0.71875  at batch 67.\n",
            "Valid Loss: 1.1259135007858276    Valid Acc:  0.71875  at batch 68.\n",
            "Valid Loss: 0.7676672339439392    Valid Acc:  0.96875  at batch 69.\n",
            "Valid Loss: 1.0715277194976807    Valid Acc:  0.78125  at batch 70.\n",
            "Valid Loss: 0.8376854658126831    Valid Acc:  0.84375  at batch 71.\n",
            "Valid Loss: 0.9370383024215698    Valid Acc:  0.78125  at batch 72.\n",
            "Valid Loss: 0.7994654178619385    Valid Acc:  0.84375  at batch 73.\n",
            "Valid Loss: 1.2054587602615356    Valid Acc:  0.65625  at batch 74.\n",
            "Valid Loss: 0.762295663356781    Valid Acc:  0.84375  at batch 75.\n",
            "Valid Loss: 0.8220621943473816    Valid Acc:  0.96875  at batch 76.\n",
            "Valid Loss: 0.9938691854476929    Valid Acc:  0.875  at batch 77.\n",
            "Valid Loss: 1.0308127403259277    Valid Acc:  0.8125  at batch 78.\n",
            "Valid Loss: 1.053662657737732    Valid Acc:  0.71875  at batch 79.\n",
            "Valid Loss: 1.0528833866119385    Valid Acc:  0.75  at batch 80.\n",
            "Valid Loss: 1.2031638622283936    Valid Acc:  0.75  at batch 81.\n",
            "Valid Loss: 1.1263220310211182    Valid Acc:  0.75  at batch 82.\n",
            "Valid Loss: 0.9882824420928955    Valid Acc:  0.84375  at batch 83.\n",
            "Valid Loss: 1.0490412712097168    Valid Acc:  0.875  at batch 84.\n",
            "Valid Loss: 1.0427881479263306    Valid Acc:  0.75  at batch 85.\n",
            "Valid Loss: 0.9650105237960815    Valid Acc:  0.8125  at batch 86.\n",
            "Valid Loss: 0.9500612020492554    Valid Acc:  0.75  at batch 87.\n",
            "Valid Loss: 0.9960394501686096    Valid Acc:  0.75  at batch 88.\n",
            "Valid Loss: 0.9864325523376465    Valid Acc:  0.875  at batch 89.\n",
            "Valid Loss: 1.068235158920288    Valid Acc:  0.75  at batch 90.\n",
            "Valid Loss: 1.126012921333313    Valid Acc:  0.6875  at batch 91.\n",
            "Valid Loss: 0.96999192237854    Valid Acc:  0.84375  at batch 92.\n",
            "Valid Loss: 1.1698453426361084    Valid Acc:  0.71875  at batch 93.\n",
            "Valid Loss: 0.9351890087127686    Valid Acc:  0.8125  at batch 94.\n",
            "Valid Loss: 1.0209120512008667    Valid Acc:  0.71875  at batch 95.\n",
            "Valid Loss: 1.014062762260437    Valid Acc:  0.78125  at batch 96.\n",
            "Valid Loss: 0.8646333813667297    Valid Acc:  0.90625  at batch 97.\n",
            "Valid Loss: 1.069579005241394    Valid Acc:  0.6875  at batch 98.\n",
            "Valid Loss: 1.1948754787445068    Valid Acc:  0.71875  at batch 99.\n",
            "Valid Loss: 0.8337759375572205    Valid Acc:  0.875  at batch 100.\n",
            "Valid Loss: 0.9903281927108765    Valid Acc:  0.78125  at batch 101.\n",
            "Valid Loss: 0.8412020206451416    Valid Acc:  0.84375  at batch 102.\n",
            "Valid Loss: 1.0647387504577637    Valid Acc:  0.8125  at batch 103.\n",
            "Valid Loss: 1.113122582435608    Valid Acc:  0.71875  at batch 104.\n",
            "Valid Loss: 1.0035818815231323    Valid Acc:  0.84375  at batch 105.\n",
            "Valid Loss: 0.899199366569519    Valid Acc:  0.78125  at batch 106.\n",
            "Valid Loss: 1.090409755706787    Valid Acc:  0.65625  at batch 107.\n",
            "Valid Loss: 1.1772584915161133    Valid Acc:  0.75  at batch 108.\n",
            "Valid Loss: 1.165524959564209    Valid Acc:  0.78125  at batch 109.\n",
            "Valid Loss: 0.9126381874084473    Valid Acc:  0.8125  at batch 110.\n",
            "Valid Loss: 0.8251726031303406    Valid Acc:  0.875  at batch 111.\n",
            "Valid Loss: 1.0089821815490723    Valid Acc:  0.875  at batch 112.\n",
            "Valid Loss: 1.1435693502426147    Valid Acc:  0.78125  at batch 113.\n",
            "Valid Loss: 0.8647748231887817    Valid Acc:  0.84375  at batch 114.\n",
            "Valid Loss: 0.9030718803405762    Valid Acc:  0.8125  at batch 115.\n",
            "Valid Loss: 1.099755883216858    Valid Acc:  0.71875  at batch 116.\n",
            "Valid Loss: 1.0591011047363281    Valid Acc:  0.84375  at batch 117.\n",
            "Valid Loss: 0.9744366407394409    Valid Acc:  0.71875  at batch 118.\n",
            "Valid Loss: 1.0629148483276367    Valid Acc:  0.84375  at batch 119.\n",
            "Valid Loss: 0.8801140189170837    Valid Acc:  0.8125  at batch 120.\n",
            "Valid Loss: 1.036858320236206    Valid Acc:  0.8125  at batch 121.\n",
            "Valid Loss: 0.9395941495895386    Valid Acc:  0.8125  at batch 122.\n",
            "Valid Loss: 1.1468567848205566    Valid Acc:  0.71875  at batch 123.\n",
            "Valid Loss: 1.158023715019226    Valid Acc:  0.71875  at batch 124.\n",
            "Valid Loss: 0.9379274845123291    Valid Acc:  0.84375  at batch 125.\n",
            "Valid Loss: 0.879892885684967    Valid Acc:  0.84375  at batch 126.\n",
            "Valid Loss: 1.0501713752746582    Valid Acc:  0.84375  at batch 127.\n",
            "Valid Loss: 0.9189313650131226    Valid Acc:  0.84375  at batch 128.\n",
            "Valid Loss: 0.9027459025382996    Valid Acc:  0.84375  at batch 129.\n",
            "Valid Loss: 1.025534987449646    Valid Acc:  0.78125  at batch 130.\n",
            "Valid Loss: 0.9788317680358887    Valid Acc:  0.75  at batch 131.\n",
            "Valid Loss: 0.9832351803779602    Valid Acc:  0.75  at batch 132.\n",
            "Valid Loss: 0.9872519373893738    Valid Acc:  0.84375  at batch 133.\n",
            "Valid Loss: 1.0355876684188843    Valid Acc:  0.8125  at batch 134.\n",
            "Valid Loss: 0.9238536357879639    Valid Acc:  0.84375  at batch 135.\n",
            "Valid Loss: 0.9985819458961487    Valid Acc:  0.8125  at batch 136.\n",
            "Valid Loss: 0.9368194937705994    Valid Acc:  0.8125  at batch 137.\n",
            "Valid Loss: 1.2418334484100342    Valid Acc:  0.5625  at batch 138.\n",
            "Valid Loss: 1.0514365434646606    Valid Acc:  0.75  at batch 139.\n",
            "Valid Loss: 1.183664083480835    Valid Acc:  0.65625  at batch 140.\n",
            "Valid Loss: 0.9809189438819885    Valid Acc:  0.75  at batch 141.\n",
            "Valid Loss: 0.9747274518013    Valid Acc:  0.875  at batch 142.\n",
            "Valid Loss: 1.0442497730255127    Valid Acc:  0.75  at batch 143.\n",
            "Valid Loss: 0.9314294457435608    Valid Acc:  0.8125  at batch 144.\n",
            "Valid Loss: 0.9230632185935974    Valid Acc:  0.84375  at batch 145.\n",
            "Valid Loss: 1.1173207759857178    Valid Acc:  0.71875  at batch 146.\n",
            "Valid Loss: 1.2416425943374634    Valid Acc:  0.78125  at batch 147.\n",
            "Valid Loss: 1.0534731149673462    Valid Acc:  0.84375  at batch 148.\n",
            "Valid Loss: 1.247388482093811    Valid Acc:  0.625  at batch 149.\n",
            "Valid Loss: 0.9073989987373352    Valid Acc:  0.8125  at batch 150.\n",
            "Valid Loss: 0.8766583204269409    Valid Acc:  0.84375  at batch 151.\n",
            "Valid Loss: 1.0908281803131104    Valid Acc:  0.78125  at batch 152.\n",
            "Valid Loss: 1.0948853492736816    Valid Acc:  0.75  at batch 153.\n",
            "Valid Loss: 0.8546194434165955    Valid Acc:  0.8125  at batch 154.\n",
            "Valid Loss: 1.0780810117721558    Valid Acc:  0.625  at batch 155.\n",
            "Valid Loss: 0.9888773560523987    Valid Acc:  0.8125  at batch 156.\n",
            "Valid Loss: 1.001563310623169    Valid Acc:  0.75  at batch 157.\n",
            "Valid Loss: 1.0340360403060913    Valid Acc:  0.75  at batch 158.\n",
            "Valid Loss: 1.0912202596664429    Valid Acc:  0.78125  at batch 159.\n",
            "Valid Loss: 0.9522429704666138    Valid Acc:  0.90625  at batch 160.\n",
            "Valid Loss: 0.9008575081825256    Valid Acc:  0.8125  at batch 161.\n",
            "Valid Loss: 1.2403733730316162    Valid Acc:  0.625  at batch 162.\n",
            "Valid Loss: 0.8701142072677612    Valid Acc:  0.8125  at batch 163.\n",
            "Valid Loss: 1.1270437240600586    Valid Acc:  0.8125  at batch 164.\n",
            "Valid Loss: 0.9540289044380188    Valid Acc:  0.84375  at batch 165.\n",
            "Valid Loss: 1.0294808149337769    Valid Acc:  0.78125  at batch 166.\n",
            "Valid Loss: 1.0489188432693481    Valid Acc:  0.71875  at batch 167.\n",
            "Valid Loss: 0.9506690502166748    Valid Acc:  0.84375  at batch 168.\n",
            "Valid Loss: 1.0187839269638062    Valid Acc:  0.84375  at batch 169.\n",
            "Valid Loss: 0.9282925724983215    Valid Acc:  0.84375  at batch 170.\n",
            "Valid Loss: 0.896807074546814    Valid Acc:  0.84375  at batch 171.\n",
            "Valid Loss: 0.9099844098091125    Valid Acc:  0.84375  at batch 172.\n",
            "Valid Loss: 1.0497881174087524    Valid Acc:  0.78125  at batch 173.\n",
            "Valid Loss: 1.2119523286819458    Valid Acc:  0.65625  at batch 174.\n",
            "Valid Loss: 1.112108826637268    Valid Acc:  0.65625  at batch 175.\n",
            "Valid Loss: 1.2586613893508911    Valid Acc:  0.65625  at batch 176.\n",
            "Valid Loss: 0.8213970065116882    Valid Acc:  0.875  at batch 177.\n",
            "Valid Loss: 1.0698682069778442    Valid Acc:  0.75  at batch 178.\n",
            "Valid Loss: 1.2408682107925415    Valid Acc:  0.6875  at batch 179.\n",
            "Valid Loss: 0.9938115477561951    Valid Acc:  0.75  at batch 180.\n",
            "Valid Loss: 0.9722352027893066    Valid Acc:  0.78125  at batch 181.\n",
            "Valid Loss: 1.0806561708450317    Valid Acc:  0.78125  at batch 182.\n",
            "Valid Loss: 0.9045003056526184    Valid Acc:  0.8125  at batch 183.\n",
            "Valid Loss: 0.9594013094902039    Valid Acc:  0.75  at batch 184.\n",
            "Valid Loss: 1.01542067527771    Valid Acc:  0.78125  at batch 185.\n",
            "Valid Loss: 0.9499712586402893    Valid Acc:  0.75  at batch 186.\n",
            "Valid Loss: 0.9015563130378723    Valid Acc:  0.84375  at batch 187.\n",
            "Valid Loss: 0.9341406226158142    Valid Acc:  0.78125  at batch 188.\n",
            "Valid Loss: 1.15920090675354    Valid Acc:  0.65625  at batch 189.\n",
            "Valid Loss: 0.9901120662689209    Valid Acc:  0.84375  at batch 190.\n",
            "Valid Loss: 1.1914997100830078    Valid Acc:  0.65625  at batch 191.\n",
            "Valid Loss: 0.9773418307304382    Valid Acc:  0.75  at batch 192.\n",
            "Valid Loss: 0.8611559867858887    Valid Acc:  0.875  at batch 193.\n",
            "Valid Loss: 1.0720582008361816    Valid Acc:  0.75  at batch 194.\n",
            "Valid Loss: 0.9048544764518738    Valid Acc:  0.8125  at batch 195.\n",
            "Valid Loss: 1.0342299938201904    Valid Acc:  0.75  at batch 196.\n",
            "Valid Loss: 1.3175569772720337    Valid Acc:  0.59375  at batch 197.\n",
            "Valid Loss: 1.2477190494537354    Valid Acc:  0.625  at batch 198.\n",
            "Valid Loss: 1.0148919820785522    Valid Acc:  0.75  at batch 199.\n",
            "Valid Loss: 0.9130316376686096    Valid Acc:  0.875  at batch 200.\n",
            "Valid Loss: 1.137277364730835    Valid Acc:  0.71875  at batch 201.\n",
            "Valid Loss: 0.8422465920448303    Valid Acc:  0.84375  at batch 202.\n",
            "Valid Loss: 1.193921685218811    Valid Acc:  0.78125  at batch 203.\n",
            "Valid Loss: 1.0923885107040405    Valid Acc:  0.8125  at batch 204.\n",
            "Valid Loss: 1.032750129699707    Valid Acc:  0.71875  at batch 205.\n",
            "Valid Loss: 1.0074493885040283    Valid Acc:  0.78125  at batch 206.\n",
            "Valid Loss: 1.0293872356414795    Valid Acc:  0.78125  at batch 207.\n",
            "Valid Loss: 1.1542915105819702    Valid Acc:  0.78125  at batch 208.\n",
            "Valid Loss: 1.0027555227279663    Valid Acc:  0.8125  at batch 209.\n",
            "Valid Loss: 0.9315558671951294    Valid Acc:  0.875  at batch 210.\n",
            "Valid Loss: 1.0737656354904175    Valid Acc:  0.8125  at batch 211.\n",
            "Valid Loss: 0.844925045967102    Valid Acc:  0.8125  at batch 212.\n",
            "Valid Loss: 1.0275098085403442    Valid Acc:  0.8125  at batch 213.\n",
            "Valid Loss: 0.8358924388885498    Valid Acc:  0.84375  at batch 214.\n",
            "Valid Loss: 1.05101478099823    Valid Acc:  0.8125  at batch 215.\n",
            "Valid Loss: 0.9704726338386536    Valid Acc:  0.8125  at batch 216.\n",
            "Valid Loss: 0.7607249021530151    Valid Acc:  0.90625  at batch 217.\n",
            "Valid Loss: 1.1021822690963745    Valid Acc:  0.6875  at batch 218.\n",
            "Valid Loss: 1.1708844900131226    Valid Acc:  0.71875  at batch 219.\n",
            "Valid Loss: 0.9826195240020752    Valid Acc:  0.84375  at batch 220.\n",
            "Valid Loss: 1.0862663984298706    Valid Acc:  0.6875  at batch 221.\n",
            "Valid Loss: 0.9968541264533997    Valid Acc:  0.65625  at batch 222.\n",
            "Valid Loss: 1.0446052551269531    Valid Acc:  0.65625  at batch 223.\n",
            "Valid Loss: 0.9593417644500732    Valid Acc:  0.8125  at batch 224.\n",
            "Valid Loss: 0.8634454011917114    Valid Acc:  0.84375  at batch 225.\n",
            "Valid Loss: 1.1073541641235352    Valid Acc:  0.71875  at batch 226.\n",
            "Valid Loss: 1.0029467344284058    Valid Acc:  0.78125  at batch 227.\n",
            "Valid Loss: 1.000915765762329    Valid Acc:  0.78125  at batch 228.\n",
            "Valid Loss: 1.0023136138916016    Valid Acc:  0.71875  at batch 229.\n",
            "Valid Loss: 1.0165596008300781    Valid Acc:  0.8125  at batch 230.\n",
            "Valid Loss: 1.0525914430618286    Valid Acc:  0.6875  at batch 231.\n",
            "Valid Loss: 1.0197370052337646    Valid Acc:  0.8125  at batch 232.\n",
            "Valid Loss: 0.8637939095497131    Valid Acc:  0.96875  at batch 233.\n",
            "Valid Loss: 0.976908802986145    Valid Acc:  0.8125  at batch 234.\n",
            "Valid Loss: 0.9643535614013672    Valid Acc:  0.78125  at batch 235.\n",
            "Valid Loss: 0.9458664655685425    Valid Acc:  0.78125  at batch 236.\n",
            "Valid Loss: 0.8658093214035034    Valid Acc:  0.875  at batch 237.\n",
            "Valid Loss: 0.9056384563446045    Valid Acc:  0.875  at batch 238.\n",
            "Valid Loss: 0.9381718635559082    Valid Acc:  0.78125  at batch 239.\n",
            "Valid Loss: 0.9692968130111694    Valid Acc:  0.8125  at batch 240.\n",
            "Valid Loss: 0.8645897507667542    Valid Acc:  0.8125  at batch 241.\n",
            "Valid Loss: 1.2125569581985474    Valid Acc:  0.78125  at batch 242.\n",
            "Valid Loss: 0.9179734587669373    Valid Acc:  0.8125  at batch 243.\n",
            "Valid Loss: 1.0082366466522217    Valid Acc:  0.875  at batch 244.\n",
            "Valid Loss: 1.0907771587371826    Valid Acc:  0.71875  at batch 245.\n",
            "Valid Loss: 1.0025726556777954    Valid Acc:  0.84375  at batch 246.\n",
            "Valid Loss: 1.020028829574585    Valid Acc:  0.78125  at batch 247.\n",
            "Valid Loss: 1.102744221687317    Valid Acc:  0.75  at batch 248.\n",
            "Valid Loss: 1.0778337717056274    Valid Acc:  0.78125  at batch 249.\n",
            "Valid Loss: 0.9787306189537048    Valid Acc:  0.8125  at batch 250.\n",
            "Valid Loss: 0.9570322036743164    Valid Acc:  0.84375  at batch 251.\n",
            "Valid Loss: 0.8215934634208679    Valid Acc:  0.84375  at batch 252.\n",
            "Valid Loss: 1.0298155546188354    Valid Acc:  0.75  at batch 253.\n",
            "Valid Loss: 1.038453221321106    Valid Acc:  0.6875  at batch 254.\n",
            "Valid Loss: 1.0602288246154785    Valid Acc:  0.71875  at batch 255.\n",
            "Valid Loss: 0.9394230246543884    Valid Acc:  0.84375  at batch 256.\n",
            "Valid Loss: 1.0502943992614746    Valid Acc:  0.78125  at batch 257.\n",
            "Valid Loss: 0.9685538411140442    Valid Acc:  0.8125  at batch 258.\n",
            "Valid Loss: 0.9104884266853333    Valid Acc:  0.84375  at batch 259.\n",
            "Valid Loss: 0.9231309294700623    Valid Acc:  0.875  at batch 260.\n",
            "Valid Loss: 0.8391891121864319    Valid Acc:  0.84375  at batch 261.\n",
            "Valid Loss: 1.008699655532837    Valid Acc:  0.6875  at batch 262.\n",
            "Valid Loss: 0.9462366104125977    Valid Acc:  0.75  at batch 263.\n",
            "Valid Loss: 1.1300501823425293    Valid Acc:  0.71875  at batch 264.\n",
            "Valid Loss: 0.948981523513794    Valid Acc:  0.75  at batch 265.\n",
            "Valid Loss: 1.2462228536605835    Valid Acc:  0.6875  at batch 266.\n",
            "Valid Loss: 0.8054380416870117    Valid Acc:  0.875  at batch 267.\n",
            "Valid Loss: 0.8742260932922363    Valid Acc:  0.8125  at batch 268.\n",
            "Valid Loss: 0.8478454351425171    Valid Acc:  0.78125  at batch 269.\n",
            "Valid Loss: 0.9838475584983826    Valid Acc:  0.84375  at batch 270.\n",
            "Valid Loss: 1.0621362924575806    Valid Acc:  0.78125  at batch 271.\n",
            "Valid Loss: 0.9308882355690002    Valid Acc:  0.9375  at batch 272.\n",
            "Valid Loss: 0.8527647852897644    Valid Acc:  0.8125  at batch 273.\n",
            "Valid Loss: 0.9741056561470032    Valid Acc:  0.78125  at batch 274.\n",
            "Valid Loss: 0.9544446468353271    Valid Acc:  0.875  at batch 275.\n",
            "Valid Loss: 1.0916162729263306    Valid Acc:  0.8125  at batch 276.\n",
            "Valid Loss: 0.8745782971382141    Valid Acc:  0.8125  at batch 277.\n",
            "Valid Loss: 0.9774878025054932    Valid Acc:  0.78125  at batch 278.\n",
            "Valid Loss: 0.9459349513053894    Valid Acc:  0.875  at batch 279.\n",
            "Valid Loss: 0.8377439975738525    Valid Acc:  0.84375  at batch 280.\n",
            "Valid Loss: 0.96074378490448    Valid Acc:  0.78125  at batch 281.\n",
            "Valid Loss: 1.0733972787857056    Valid Acc:  0.8125  at batch 282.\n",
            "Valid Loss: 1.0761698484420776    Valid Acc:  0.875  at batch 283.\n",
            "Valid Loss: 1.1454583406448364    Valid Acc:  0.8125  at batch 284.\n",
            "Valid Loss: 0.9481264352798462    Valid Acc:  0.875  at batch 285.\n",
            "Valid Loss: 1.0939778089523315    Valid Acc:  0.78125  at batch 286.\n",
            "Valid Loss: 0.8994507193565369    Valid Acc:  0.78125  at batch 287.\n",
            "Valid Loss: 0.9543745517730713    Valid Acc:  0.71875  at batch 288.\n",
            "Valid Loss: 1.1701796054840088    Valid Acc:  0.71875  at batch 289.\n",
            "Valid Loss: 0.9959889054298401    Valid Acc:  0.78125  at batch 290.\n",
            "Valid Loss: 0.9466260671615601    Valid Acc:  0.78125  at batch 291.\n",
            "Valid Loss: 1.0186896324157715    Valid Acc:  0.8125  at batch 292.\n",
            "Valid Loss: 1.1255898475646973    Valid Acc:  0.6875  at batch 293.\n",
            "Valid Loss: 1.0303789377212524    Valid Acc:  0.8125  at batch 294.\n",
            "Valid Loss: 0.9578266739845276    Valid Acc:  0.875  at batch 295.\n",
            "Valid Loss: 0.8678539395332336    Valid Acc:  0.8125  at batch 296.\n",
            "Valid Loss: 0.8344733715057373    Valid Acc:  0.8125  at batch 297.\n",
            "Valid Loss: 1.0197603702545166    Valid Acc:  0.84375  at batch 298.\n",
            "Valid Loss: 0.910112738609314    Valid Acc:  0.90625  at batch 299.\n",
            "Valid Loss: 0.9796484708786011    Valid Acc:  0.78125  at batch 300.\n",
            "Valid Loss: 0.9975498914718628    Valid Acc:  0.8125  at batch 301.\n",
            "Valid Loss: 1.0880848169326782    Valid Acc:  0.75  at batch 302.\n",
            "Valid Loss: 1.0452924966812134    Valid Acc:  0.90625  at batch 303.\n",
            "Valid Loss: 1.0236172676086426    Valid Acc:  0.6875  at batch 304.\n",
            "Valid Loss: 0.9253110885620117    Valid Acc:  0.84375  at batch 305.\n",
            "Valid Loss: 1.0130232572555542    Valid Acc:  0.8125  at batch 306.\n",
            "Valid Loss: 1.0122848749160767    Valid Acc:  0.71875  at batch 307.\n",
            "Valid Loss: 0.8864367604255676    Valid Acc:  0.875  at batch 308.\n",
            "Valid Loss: 0.963060200214386    Valid Acc:  0.9375  at batch 309.\n",
            "Valid Loss: 0.9412486553192139    Valid Acc:  0.8125  at batch 310.\n",
            "Valid Loss: 1.0440115928649902    Valid Acc:  0.78125  at batch 311.\n",
            "Valid Loss: 0.9433866143226624    Valid Acc:  0.8125  at batch 312.\n",
            "Valid Loss: 1.0662975311279297    Valid Acc:  0.6875  at batch 313.\n",
            "Valid Loss: 1.028105616569519    Valid Acc:  0.71875  at batch 314.\n",
            "Valid Loss: 1.0177927017211914    Valid Acc:  0.90625  at batch 315.\n",
            "Valid Loss: 0.9279109239578247    Valid Acc:  0.75  at batch 316.\n",
            "Valid Loss: 0.9280655384063721    Valid Acc:  0.8125  at batch 317.\n",
            "Valid Loss: 0.970888614654541    Valid Acc:  0.75  at batch 318.\n",
            "Valid Loss: 0.9533951282501221    Valid Acc:  0.8125  at batch 319.\n",
            "Valid Loss: 0.9096564650535583    Valid Acc:  0.84375  at batch 320.\n",
            "Valid Loss: 1.0763906240463257    Valid Acc:  0.875  at batch 321.\n",
            "Valid Loss: 0.8906227350234985    Valid Acc:  0.78125  at batch 322.\n",
            "Valid Loss: 1.0211209058761597    Valid Acc:  0.71875  at batch 323.\n",
            "Valid Loss: 1.0008840560913086    Valid Acc:  0.78125  at batch 324.\n",
            "Valid Loss: 0.9105462431907654    Valid Acc:  0.84375  at batch 325.\n",
            "Valid Loss: 0.8339861035346985    Valid Acc:  0.9375  at batch 326.\n",
            "Valid Loss: 1.125883936882019    Valid Acc:  0.625  at batch 327.\n",
            "Valid Loss: 0.9398698210716248    Valid Acc:  0.875  at batch 328.\n",
            "Valid Loss: 1.1692266464233398    Valid Acc:  0.8125  at batch 329.\n",
            "Valid Loss: 0.9412174224853516    Valid Acc:  0.78125  at batch 330.\n",
            "Valid Loss: 0.9081434607505798    Valid Acc:  0.875  at batch 331.\n",
            "Valid Loss: 1.0294432640075684    Valid Acc:  0.78125  at batch 332.\n",
            "Valid Loss: 1.2018688917160034    Valid Acc:  0.625  at batch 333.\n",
            "Valid Loss: 0.9118617177009583    Valid Acc:  0.90625  at batch 334.\n",
            "Valid Loss: 0.9035675525665283    Valid Acc:  0.875  at batch 335.\n",
            "Valid Loss: 0.8667025566101074    Valid Acc:  0.875  at batch 336.\n",
            "Valid Loss: 0.9465870261192322    Valid Acc:  0.8125  at batch 337.\n",
            "Valid Loss: 0.918931245803833    Valid Acc:  0.8125  at batch 338.\n",
            "Valid Loss: 0.931181013584137    Valid Acc:  0.875  at batch 339.\n",
            "Valid Loss: 1.1490243673324585    Valid Acc:  0.65625  at batch 340.\n",
            "Valid Loss: 1.1024631261825562    Valid Acc:  0.71875  at batch 341.\n",
            "Valid Loss: 0.9065250754356384    Valid Acc:  0.84375  at batch 342.\n",
            "Valid Loss: 0.9167002439498901    Valid Acc:  0.8125  at batch 343.\n",
            "Valid Loss: 0.9081433415412903    Valid Acc:  0.9375  at batch 344.\n",
            "Valid Loss: 1.0025265216827393    Valid Acc:  0.84375  at batch 345.\n",
            "Valid Loss: 1.0552786588668823    Valid Acc:  0.78125  at batch 346.\n",
            "Valid Loss: 0.9004963636398315    Valid Acc:  0.84375  at batch 347.\n",
            "Valid Loss: 0.8462609052658081    Valid Acc:  0.84375  at batch 348.\n",
            "Valid Loss: 1.0414636135101318    Valid Acc:  0.84375  at batch 349.\n",
            "Valid Loss: 0.996789813041687    Valid Acc:  0.8125  at batch 350.\n",
            "Valid Loss: 0.967569887638092    Valid Acc:  0.90625  at batch 351.\n",
            "Valid Loss: 1.3717161417007446    Valid Acc:  0.53125  at batch 352.\n",
            "Valid Loss: 0.9399370551109314    Valid Acc:  0.875  at batch 353.\n",
            "Valid Loss: 1.011418342590332    Valid Acc:  0.78125  at batch 354.\n",
            "Valid Loss: 0.824173092842102    Valid Acc:  0.875  at batch 355.\n",
            "Valid Loss: 1.0111240148544312    Valid Acc:  0.8125  at batch 356.\n",
            "Valid Loss: 1.0413686037063599    Valid Acc:  0.8125  at batch 357.\n",
            "Valid Loss: 0.9735270142555237    Valid Acc:  0.8125  at batch 358.\n",
            "Valid Loss: 0.9888619780540466    Valid Acc:  0.84375  at batch 359.\n",
            "Valid Loss: 0.977566659450531    Valid Acc:  0.6875  at batch 360.\n",
            "Valid Loss: 1.1156136989593506    Valid Acc:  0.65625  at batch 361.\n",
            "Valid Loss: 0.8635919094085693    Valid Acc:  0.90625  at batch 362.\n",
            "Valid Loss: 0.954709529876709    Valid Acc:  0.71875  at batch 363.\n",
            "Valid Loss: 1.278517246246338    Valid Acc:  0.65625  at batch 364.\n",
            "Valid Loss: 0.9200937747955322    Valid Acc:  0.8125  at batch 365.\n",
            "Valid Loss: 1.0908832550048828    Valid Acc:  0.78125  at batch 366.\n",
            "Valid Loss: 1.031254768371582    Valid Acc:  0.71875  at batch 367.\n",
            "Valid Loss: 0.9854699969291687    Valid Acc:  0.78125  at batch 368.\n",
            "Valid Loss: 0.8234591484069824    Valid Acc:  0.875  at batch 369.\n",
            "Valid Loss: 0.9338220357894897    Valid Acc:  0.8125  at batch 370.\n",
            "Valid Loss: 0.9967603087425232    Valid Acc:  0.75  at batch 371.\n",
            "Valid Loss: 0.7598687410354614    Valid Acc:  0.875  at batch 372.\n",
            "Valid Loss: 0.9251875877380371    Valid Acc:  0.8125  at batch 373.\n",
            "Valid Loss: 0.8870211839675903    Valid Acc:  0.84375  at batch 374.\n",
            "Valid Loss: 1.0602494478225708    Valid Acc:  0.6875  at batch 375.\n",
            "Valid Loss: 0.9766156673431396    Valid Acc:  0.78125  at batch 376.\n",
            "Valid Loss: 1.0031474828720093    Valid Acc:  0.78125  at batch 377.\n",
            "Valid Loss: 0.9718307852745056    Valid Acc:  0.78125  at batch 378.\n",
            "Valid Loss: 1.1363502740859985    Valid Acc:  0.71875  at batch 379.\n",
            "Valid Loss: 1.0188567638397217    Valid Acc:  0.84375  at batch 380.\n",
            "Valid Loss: 1.1792787313461304    Valid Acc:  0.8125  at batch 381.\n",
            "Valid Loss: 0.9600296020507812    Valid Acc:  0.90625  at batch 382.\n",
            "Valid Loss: 0.961857259273529    Valid Acc:  0.78125  at batch 383.\n",
            "Valid Loss: 0.9025678038597107    Valid Acc:  0.875  at batch 384.\n",
            "Valid Loss: 1.1532872915267944    Valid Acc:  0.78125  at batch 385.\n",
            "Valid Loss: 0.9532325267791748    Valid Acc:  0.8125  at batch 386.\n",
            "Valid Loss: 0.9538283348083496    Valid Acc:  0.75  at batch 387.\n",
            "Valid Loss: 1.170882225036621    Valid Acc:  0.6875  at batch 388.\n",
            "Valid Loss: 1.0119686126708984    Valid Acc:  0.71875  at batch 389.\n",
            "Valid Loss: 1.083314299583435    Valid Acc:  0.75  at batch 390.\n",
            "Valid Loss: 0.9649912118911743    Valid Acc:  0.8125  at batch 391.\n",
            "Valid Loss: 0.9746289253234863    Valid Acc:  0.875  at batch 392.\n",
            "Valid Loss: 1.1224161386489868    Valid Acc:  0.6875  at batch 393.\n",
            "Valid Loss: 0.9511930346488953    Valid Acc:  0.84375  at batch 394.\n",
            "Valid Loss: 1.1435191631317139    Valid Acc:  0.71875  at batch 395.\n",
            "Valid Loss: 0.8585769534111023    Valid Acc:  0.875  at batch 396.\n",
            "Valid Loss: 1.1441164016723633    Valid Acc:  0.75  at batch 397.\n",
            "Valid Loss: 0.8849881887435913    Valid Acc:  0.84375  at batch 398.\n",
            "Valid Loss: 0.9419216513633728    Valid Acc:  0.875  at batch 399.\n",
            "Valid Loss: 1.120712161064148    Valid Acc:  0.71875  at batch 400.\n",
            "Valid Loss: 1.0827360153198242    Valid Acc:  0.71875  at batch 401.\n",
            "Valid Loss: 1.0756568908691406    Valid Acc:  0.6875  at batch 402.\n",
            "Valid Loss: 1.086022973060608    Valid Acc:  0.75  at batch 403.\n",
            "Valid Loss: 1.138353943824768    Valid Acc:  0.71875  at batch 404.\n",
            "Valid Loss: 0.9116011261940002    Valid Acc:  0.78125  at batch 405.\n",
            "Valid Loss: 1.197522759437561    Valid Acc:  0.8125  at batch 406.\n",
            "Valid Loss: 1.0243061780929565    Valid Acc:  0.71875  at batch 407.\n",
            "Valid Loss: 0.8786237835884094    Valid Acc:  0.875  at batch 408.\n",
            "Valid Loss: 0.98166823387146    Valid Acc:  0.84375  at batch 409.\n",
            "Valid Loss: 0.9969746470451355    Valid Acc:  0.8125  at batch 410.\n",
            "Valid Loss: 0.9140329957008362    Valid Acc:  0.84375  at batch 411.\n",
            "Valid Loss: 0.9879467487335205    Valid Acc:  0.78125  at batch 412.\n",
            "Valid Loss: 1.0474923849105835    Valid Acc:  0.8125  at batch 413.\n",
            "Valid Loss: 0.9315235614776611    Valid Acc:  0.84375  at batch 414.\n",
            "Valid Loss: 0.9832334518432617    Valid Acc:  0.875  at batch 415.\n",
            "Valid Loss: 1.1040358543395996    Valid Acc:  0.6875  at batch 416.\n",
            "Valid Loss: 0.9246671199798584    Valid Acc:  0.8125  at batch 417.\n",
            "Valid Loss: 0.986286997795105    Valid Acc:  0.84375  at batch 418.\n",
            "Valid Loss: 0.8861292004585266    Valid Acc:  0.84375  at batch 419.\n",
            "Valid Loss: 0.8852956891059875    Valid Acc:  0.8125  at batch 420.\n",
            "Valid Loss: 0.9708506464958191    Valid Acc:  0.75  at batch 421.\n",
            "Valid Loss: 1.0303149223327637    Valid Acc:  0.71875  at batch 422.\n",
            "Valid Loss: 1.0412505865097046    Valid Acc:  0.78125  at batch 423.\n",
            "Valid Loss: 0.8593958616256714    Valid Acc:  0.84375  at batch 424.\n",
            "Valid Loss: 0.9057960510253906    Valid Acc:  0.78125  at batch 425.\n",
            "Valid Loss: 1.0497554540634155    Valid Acc:  0.71875  at batch 426.\n",
            "Valid Loss: 0.9747317433357239    Valid Acc:  0.71875  at batch 427.\n",
            "Valid Loss: 1.1409186124801636    Valid Acc:  0.6875  at batch 428.\n",
            "Valid Loss: 0.8114562034606934    Valid Acc:  0.875  at batch 429.\n",
            "Valid Loss: 1.1186860799789429    Valid Acc:  0.65625  at batch 430.\n",
            "Valid Loss: 1.0004597902297974    Valid Acc:  0.78125  at batch 431.\n",
            "Valid Loss: 0.9787819981575012    Valid Acc:  0.78125  at batch 432.\n",
            "Valid Loss: 0.9153664112091064    Valid Acc:  0.78125  at batch 433.\n",
            "Valid Loss: 1.1321004629135132    Valid Acc:  0.71875  at batch 434.\n",
            "Valid Loss: 0.9419106841087341    Valid Acc:  0.875  at batch 435.\n",
            "Valid Loss: 0.9263021349906921    Valid Acc:  0.875  at batch 436.\n",
            "Valid Loss: 0.9315425157546997    Valid Acc:  0.8125  at batch 437.\n",
            "Valid Loss: 1.0347707271575928    Valid Acc:  0.71875  at batch 438.\n",
            "Valid Loss: 0.8667019605636597    Valid Acc:  0.875  at batch 439.\n",
            "Valid Loss: 1.0069502592086792    Valid Acc:  0.78125  at batch 440.\n",
            "Valid Loss: 1.0034596920013428    Valid Acc:  0.75  at batch 441.\n",
            "Valid Loss: 0.9881196022033691    Valid Acc:  0.875  at batch 442.\n",
            "Valid Loss: 1.0310710668563843    Valid Acc:  0.6875  at batch 443.\n",
            "Valid Loss: 1.0648677349090576    Valid Acc:  0.75  at batch 444.\n",
            "Valid Loss: 0.8558819890022278    Valid Acc:  0.875  at batch 445.\n",
            "Valid Loss: 1.103018879890442    Valid Acc:  0.71875  at batch 446.\n",
            "Valid Loss: 0.9581237435340881    Valid Acc:  0.8125  at batch 447.\n",
            "Valid Loss: 0.9243071675300598    Valid Acc:  0.84375  at batch 448.\n",
            "Valid Loss: 1.1519169807434082    Valid Acc:  0.71875  at batch 449.\n",
            "Valid Loss: 0.9582428932189941    Valid Acc:  0.8125  at batch 450.\n",
            "Valid Loss: 0.9741013050079346    Valid Acc:  0.84375  at batch 451.\n",
            "Valid Loss: 1.0134156942367554    Valid Acc:  0.78125  at batch 452.\n",
            "Valid Loss: 0.8485304117202759    Valid Acc:  0.90625  at batch 453.\n",
            "Valid Loss: 0.8944725394248962    Valid Acc:  0.8125  at batch 454.\n",
            "Valid Loss: 1.0412105321884155    Valid Acc:  0.8125  at batch 455.\n",
            "Valid Loss: 0.9325675368309021    Valid Acc:  0.90625  at batch 456.\n",
            "Valid Loss: 1.0143100023269653    Valid Acc:  0.75  at batch 457.\n",
            "Valid Loss: 1.0647938251495361    Valid Acc:  0.8125  at batch 458.\n",
            "Valid Loss: 1.0778762102127075    Valid Acc:  0.8125  at batch 459.\n",
            "Valid Loss: 1.041369080543518    Valid Acc:  0.71875  at batch 460.\n",
            "Valid Loss: 0.8800327181816101    Valid Acc:  0.8125  at batch 461.\n",
            "Valid Loss: 0.8872733116149902    Valid Acc:  0.90625  at batch 462.\n",
            "Valid Loss: 0.7570764422416687    Valid Acc:  0.875  at batch 463.\n",
            "Valid Loss: 0.895993173122406    Valid Acc:  0.8125  at batch 464.\n",
            "Valid Loss: 0.8310641646385193    Valid Acc:  0.90625  at batch 465.\n",
            "Valid Loss: 1.017075538635254    Valid Acc:  0.84375  at batch 466.\n",
            "Valid Loss: 0.9746865630149841    Valid Acc:  0.71875  at batch 467.\n",
            "Valid Loss: 1.0041651725769043    Valid Acc:  0.6875  at batch 468.\n",
            "Valid Loss: 1.2132529020309448    Valid Acc:  0.625  at batch 469.\n",
            "Valid Loss: 0.9409946203231812    Valid Acc:  0.8125  at batch 470.\n",
            "Valid Loss: 1.0153752565383911    Valid Acc:  0.78125  at batch 471.\n",
            "Valid Loss: 0.9283986687660217    Valid Acc:  0.84375  at batch 472.\n",
            "Valid Loss: 0.9089807868003845    Valid Acc:  0.8125  at batch 473.\n",
            "Valid Loss: 1.1094691753387451    Valid Acc:  0.78125  at batch 474.\n",
            "Valid Loss: 0.9258002042770386    Valid Acc:  0.90625  at batch 475.\n",
            "Valid Loss: 1.091557264328003    Valid Acc:  0.71875  at batch 476.\n",
            "Valid Loss: 0.8087282776832581    Valid Acc:  0.875  at batch 477.\n",
            "Valid Loss: 1.1523070335388184    Valid Acc:  0.71875  at batch 478.\n",
            "Valid Loss: 1.1421433687210083    Valid Acc:  0.6875  at batch 479.\n",
            "Valid Loss: 0.884750247001648    Valid Acc:  0.78125  at batch 480.\n",
            "Valid Loss: 1.147548794746399    Valid Acc:  0.75  at batch 481.\n",
            "Valid Loss: 1.015419840812683    Valid Acc:  0.84375  at batch 482.\n",
            "Valid Loss: 0.8949417471885681    Valid Acc:  0.84375  at batch 483.\n",
            "Valid Loss: 1.007697343826294    Valid Acc:  0.8125  at batch 484.\n",
            "Valid Loss: 0.9693686962127686    Valid Acc:  0.84375  at batch 485.\n",
            "Valid Loss: 1.0358659029006958    Valid Acc:  0.75  at batch 486.\n",
            "Valid Loss: 1.1409062147140503    Valid Acc:  0.71875  at batch 487.\n",
            "Valid Loss: 0.8804465532302856    Valid Acc:  0.78125  at batch 488.\n",
            "Valid Loss: 1.0771820545196533    Valid Acc:  0.84375  at batch 489.\n",
            "Valid Loss: 0.9791459441184998    Valid Acc:  0.84375  at batch 490.\n",
            "Valid Loss: 1.0885944366455078    Valid Acc:  0.71875  at batch 491.\n",
            "Valid Loss: 1.1839330196380615    Valid Acc:  0.625  at batch 492.\n",
            "Valid Loss: 0.9802179932594299    Valid Acc:  0.84375  at batch 493.\n",
            "Valid Loss: 0.9793640971183777    Valid Acc:  0.875  at batch 494.\n",
            "Valid Loss: 1.1381852626800537    Valid Acc:  0.8125  at batch 495.\n",
            "Valid Loss: 0.9734396934509277    Valid Acc:  0.90625  at batch 496.\n",
            "Valid Loss: 1.2333675622940063    Valid Acc:  0.71875  at batch 497.\n",
            "Valid Loss: 1.0157749652862549    Valid Acc:  0.75  at batch 498.\n",
            "Valid Loss: 0.8698816299438477    Valid Acc:  0.8125  at batch 499.\n",
            "Valid Loss: 0.9194008111953735    Valid Acc:  0.75  at batch 500.\n",
            "Valid Loss: 0.9601597189903259    Valid Acc:  0.875  at batch 501.\n",
            "Valid Loss: 1.0867010354995728    Valid Acc:  0.71875  at batch 502.\n",
            "Valid Loss: 1.0851811170578003    Valid Acc:  0.75  at batch 503.\n",
            "Valid Loss: 1.101140022277832    Valid Acc:  0.78125  at batch 504.\n",
            "Valid Loss: 0.9257647395133972    Valid Acc:  0.75  at batch 505.\n",
            "Valid Loss: 1.0561760663986206    Valid Acc:  0.71875  at batch 506.\n",
            "Valid Loss: 1.0269566774368286    Valid Acc:  0.65625  at batch 507.\n",
            "Valid Loss: 1.1346664428710938    Valid Acc:  0.71875  at batch 508.\n",
            "Valid Loss: 1.3920488357543945    Valid Acc:  0.5625  at batch 509.\n",
            "Valid Loss: 1.0249173641204834    Valid Acc:  0.71875  at batch 510.\n",
            "Valid Loss: 1.0459517240524292    Valid Acc:  0.78125  at batch 511.\n",
            "Valid Loss: 0.8656827211380005    Valid Acc:  0.78125  at batch 512.\n",
            "Valid Loss: 1.034576654434204    Valid Acc:  0.625  at batch 513.\n",
            "Valid Loss: 1.0651659965515137    Valid Acc:  0.71875  at batch 514.\n",
            "Valid Loss: 0.9208595156669617    Valid Acc:  0.84375  at batch 515.\n",
            "Valid Loss: 1.0841350555419922    Valid Acc:  0.84375  at batch 516.\n",
            "Valid Loss: 0.9997059106826782    Valid Acc:  0.78125  at batch 517.\n",
            "Valid Loss: 1.045178771018982    Valid Acc:  0.78125  at batch 518.\n",
            "Valid Loss: 1.1497386693954468    Valid Acc:  0.65625  at batch 519.\n",
            "Valid Loss: 1.0490782260894775    Valid Acc:  0.78125  at batch 520.\n",
            "Valid Loss: 0.9329391121864319    Valid Acc:  0.8125  at batch 521.\n",
            "Valid Loss: 0.9752123355865479    Valid Acc:  0.78125  at batch 522.\n",
            "Valid Loss: 1.069521188735962    Valid Acc:  0.71875  at batch 523.\n",
            "Valid Loss: 0.9490885138511658    Valid Acc:  0.84375  at batch 524.\n",
            "Valid Loss: 0.9835284352302551    Valid Acc:  0.90625  at batch 525.\n",
            "Valid Loss: 0.9710018634796143    Valid Acc:  0.8125  at batch 526.\n",
            "Valid Loss: 0.9513249397277832    Valid Acc:  0.8125  at batch 527.\n",
            "Valid Loss: 1.083292841911316    Valid Acc:  0.6875  at batch 528.\n",
            "Valid Loss: 1.0195869207382202    Valid Acc:  0.78125  at batch 529.\n",
            "Valid Loss: 1.008350133895874    Valid Acc:  0.6875  at batch 530.\n",
            "Valid Loss: 1.0070865154266357    Valid Acc:  0.78125  at batch 531.\n",
            "Valid Loss: 1.0854480266571045    Valid Acc:  0.71875  at batch 532.\n",
            "Valid Loss: 1.047404408454895    Valid Acc:  0.78125  at batch 533.\n",
            "Valid Loss: 1.0241880416870117    Valid Acc:  0.8125  at batch 534.\n",
            "Valid Loss: 1.072892665863037    Valid Acc:  0.78125  at batch 535.\n",
            "Valid Loss: 0.9928686022758484    Valid Acc:  0.65625  at batch 536.\n",
            "Valid Loss: 0.853158175945282    Valid Acc:  0.84375  at batch 537.\n",
            "Valid Loss: 0.8937810063362122    Valid Acc:  0.8125  at batch 538.\n",
            "Valid Loss: 1.033335566520691    Valid Acc:  0.8125  at batch 539.\n",
            "Valid Loss: 0.8632190823554993    Valid Acc:  0.875  at batch 540.\n",
            "Valid Loss: 1.0996307134628296    Valid Acc:  0.75  at batch 541.\n",
            "Valid Loss: 1.0665884017944336    Valid Acc:  0.71875  at batch 542.\n",
            "Valid Loss: 1.0685961246490479    Valid Acc:  0.6875  at batch 543.\n",
            "Valid Loss: 1.152966022491455    Valid Acc:  0.65625  at batch 544.\n",
            "Valid Loss: 1.0271836519241333    Valid Acc:  0.78125  at batch 545.\n",
            "Valid Loss: 1.030313491821289    Valid Acc:  0.875  at batch 546.\n",
            "Valid Loss: 0.9968479871749878    Valid Acc:  0.78125  at batch 547.\n",
            "Valid Loss: 0.9476515054702759    Valid Acc:  0.75  at batch 548.\n",
            "Valid Loss: 1.1266040802001953    Valid Acc:  0.71875  at batch 549.\n",
            "Valid Loss: 0.9980430006980896    Valid Acc:  0.84375  at batch 550.\n",
            "Valid Loss: 1.0192073583602905    Valid Acc:  0.78125  at batch 551.\n",
            "Valid Loss: 0.8432077765464783    Valid Acc:  0.875  at batch 552.\n",
            "Valid Loss: 0.9187919497489929    Valid Acc:  0.875  at batch 553.\n",
            "Valid Loss: 0.9561448097229004    Valid Acc:  0.8125  at batch 554.\n",
            "Valid Loss: 0.8963270783424377    Valid Acc:  0.8125  at batch 555.\n",
            "Valid Loss: 1.15580153465271    Valid Acc:  0.71875  at batch 556.\n",
            "Valid Loss: 1.0705091953277588    Valid Acc:  0.625  at batch 557.\n",
            "Valid Loss: 0.9625024795532227    Valid Acc:  0.75  at batch 558.\n",
            "Valid Loss: 1.0593855381011963    Valid Acc:  0.8125  at batch 559.\n",
            "Valid Loss: 0.9689512848854065    Valid Acc:  0.90625  at batch 560.\n",
            "Valid Loss: 0.9923004508018494    Valid Acc:  0.8125  at batch 561.\n",
            "Valid Loss: 1.261837124824524    Valid Acc:  0.65625  at batch 562.\n",
            "Valid Loss: 1.0459003448486328    Valid Acc:  0.78125  at batch 563.\n",
            "Valid Loss: 0.9124231934547424    Valid Acc:  0.8125  at batch 564.\n",
            "Valid Loss: 0.8772293925285339    Valid Acc:  0.9375  at batch 565.\n",
            "Valid Loss: 0.9368862509727478    Valid Acc:  0.84375  at batch 566.\n",
            "Valid Loss: 0.8891698122024536    Valid Acc:  0.90625  at batch 567.\n",
            "Valid Loss: 0.9020009636878967    Valid Acc:  0.875  at batch 568.\n",
            "Valid Loss: 0.8175806403160095    Valid Acc:  0.875  at batch 569.\n",
            "Valid Loss: 0.9178163409233093    Valid Acc:  0.90625  at batch 570.\n",
            "Valid Loss: 1.1537585258483887    Valid Acc:  0.71875  at batch 571.\n",
            "Valid Loss: 0.7880775928497314    Valid Acc:  0.90625  at batch 572.\n",
            "Valid Loss: 1.1162995100021362    Valid Acc:  0.8125  at batch 573.\n",
            "Valid Loss: 1.121887445449829    Valid Acc:  0.8125  at batch 574.\n",
            "Valid Loss: 0.9967350959777832    Valid Acc:  0.8125  at batch 575.\n",
            "Valid Loss: 0.9723653197288513    Valid Acc:  0.78125  at batch 576.\n",
            "Valid Loss: 0.9102465510368347    Valid Acc:  0.78125  at batch 577.\n",
            "Valid Loss: 0.944801926612854    Valid Acc:  0.84375  at batch 578.\n",
            "Valid Loss: 0.8085234761238098    Valid Acc:  0.9375  at batch 579.\n",
            "Valid Loss: 1.0983079671859741    Valid Acc:  0.78125  at batch 580.\n",
            "Valid Loss: 0.9521381258964539    Valid Acc:  0.875  at batch 581.\n",
            "Valid Loss: 1.0192991495132446    Valid Acc:  0.84375  at batch 582.\n",
            "Valid Loss: 0.9898243546485901    Valid Acc:  0.75  at batch 583.\n",
            "Valid Loss: 0.9130071997642517    Valid Acc:  0.78125  at batch 584.\n",
            "Valid Loss: 1.0483943223953247    Valid Acc:  0.8125  at batch 585.\n",
            "Valid Loss: 1.180298089981079    Valid Acc:  0.75  at batch 586.\n",
            "Valid Loss: 0.9950646758079529    Valid Acc:  0.90625  at batch 587.\n",
            "Valid Loss: 1.0297118425369263    Valid Acc:  0.78125  at batch 588.\n",
            "Valid Loss: 0.8363497853279114    Valid Acc:  0.78125  at batch 589.\n",
            "Valid Loss: 0.9987137317657471    Valid Acc:  0.78125  at batch 590.\n",
            "Valid Loss: 1.0349228382110596    Valid Acc:  0.78125  at batch 591.\n",
            "Valid Loss: 1.003027319908142    Valid Acc:  0.71875  at batch 592.\n",
            "Valid Loss: 0.9845462441444397    Valid Acc:  0.75  at batch 593.\n",
            "Valid Loss: 0.8699052333831787    Valid Acc:  0.90625  at batch 594.\n",
            "Valid Loss: 1.1206821203231812    Valid Acc:  0.8125  at batch 595.\n",
            "Valid Loss: 0.9858906269073486    Valid Acc:  0.8125  at batch 596.\n",
            "Valid Loss: 1.0980876684188843    Valid Acc:  0.65625  at batch 597.\n",
            "Valid Loss: 0.9395652413368225    Valid Acc:  0.78125  at batch 598.\n",
            "Valid Loss: 1.0599958896636963    Valid Acc:  0.75  at batch 599.\n",
            "Valid Loss: 1.0512397289276123    Valid Acc:  0.8125  at batch 600.\n",
            "Valid Loss: 0.9924553036689758    Valid Acc:  0.8125  at batch 601.\n",
            "Valid Loss: 1.061863899230957    Valid Acc:  0.8125  at batch 602.\n",
            "Valid Loss: 1.1479867696762085    Valid Acc:  0.6875  at batch 603.\n",
            "Valid Loss: 0.9128091335296631    Valid Acc:  0.8125  at batch 604.\n",
            "Valid Loss: 1.1776729822158813    Valid Acc:  0.71875  at batch 605.\n",
            "Valid Loss: 0.9160719513893127    Valid Acc:  0.8125  at batch 606.\n",
            "Valid Loss: 0.8869335651397705    Valid Acc:  0.75  at batch 607.\n",
            "Valid Loss: 1.0917060375213623    Valid Acc:  0.71875  at batch 608.\n",
            "Valid Loss: 1.0341445207595825    Valid Acc:  0.75  at batch 609.\n",
            "Valid Loss: 0.9058468341827393    Valid Acc:  0.8125  at batch 610.\n",
            "Valid Loss: 0.991127610206604    Valid Acc:  0.90625  at batch 611.\n",
            "Valid Loss: 1.1033602952957153    Valid Acc:  0.78125  at batch 612.\n",
            "Valid Loss: 0.9041450619697571    Valid Acc:  0.9375  at batch 613.\n",
            "Valid Loss: 1.0331071615219116    Valid Acc:  0.6875  at batch 614.\n",
            "Valid Loss: 0.8619098663330078    Valid Acc:  0.90625  at batch 615.\n",
            "Valid Loss: 1.017564058303833    Valid Acc:  0.75  at batch 616.\n",
            "Valid Loss: 1.1096508502960205    Valid Acc:  0.625  at batch 617.\n",
            "Valid Loss: 1.0564278364181519    Valid Acc:  0.84375  at batch 618.\n",
            "Valid Loss: 0.9798846244812012    Valid Acc:  0.8125  at batch 619.\n",
            "Valid Loss: 0.9999120831489563    Valid Acc:  0.75  at batch 620.\n",
            "Valid Loss: 1.0045462846755981    Valid Acc:  0.71875  at batch 621.\n",
            "Valid Loss: 1.0676380395889282    Valid Acc:  0.75  at batch 622.\n",
            "Valid Loss: 0.9500568509101868    Valid Acc:  0.8125  at batch 623.\n",
            "Valid Loss: 1.108130931854248    Valid Acc:  0.78125  at batch 624.\n",
            "Valid Loss: 1.188797116279602    Valid Acc:  0.625  at batch 625.\n",
            "Valid Loss: 0.869617760181427    Valid Acc:  0.84375  at batch 626.\n",
            "Valid Loss: 0.9766748547554016    Valid Acc:  0.75  at batch 627.\n",
            "Valid Loss: 0.9701365828514099    Valid Acc:  0.75  at batch 628.\n",
            "Valid Loss: 0.9890090227127075    Valid Acc:  0.78125  at batch 629.\n",
            "Valid Loss: 0.9850009083747864    Valid Acc:  0.8125  at batch 630.\n",
            "Valid Loss: 0.9983416795730591    Valid Acc:  0.8125  at batch 631.\n",
            "Valid Loss: 1.1547333002090454    Valid Acc:  0.6875  at batch 632.\n",
            "Valid Loss: 0.9788469672203064    Valid Acc:  0.78125  at batch 633.\n",
            "Valid Loss: 1.073756456375122    Valid Acc:  0.6875  at batch 634.\n",
            "Valid Loss: 1.0672402381896973    Valid Acc:  0.75  at batch 635.\n",
            "Valid Loss: 0.9982314109802246    Valid Acc:  0.71875  at batch 636.\n",
            "Valid Loss: 0.957507312297821    Valid Acc:  0.8125  at batch 637.\n",
            "Valid Loss: 1.0355111360549927    Valid Acc:  0.75  at batch 638.\n",
            "Valid Loss: 1.1858755350112915    Valid Acc:  0.65625  at batch 639.\n",
            "Valid Loss: 0.8634358644485474    Valid Acc:  0.875  at batch 640.\n",
            "Valid Loss: 1.249463438987732    Valid Acc:  0.65625  at batch 641.\n",
            "Valid Loss: 1.0688233375549316    Valid Acc:  0.78125  at batch 642.\n",
            "Valid Loss: 0.8293426036834717    Valid Acc:  0.90625  at batch 643.\n",
            "Valid Loss: 1.03294038772583    Valid Acc:  0.71875  at batch 644.\n",
            "Valid Loss: 1.120566487312317    Valid Acc:  0.78125  at batch 645.\n",
            "Valid Loss: 1.0265897512435913    Valid Acc:  0.84375  at batch 646.\n",
            "Valid Loss: 0.9212502241134644    Valid Acc:  0.875  at batch 647.\n",
            "Valid Loss: 1.150251865386963    Valid Acc:  0.71875  at batch 648.\n",
            "Valid Loss: 1.2100634574890137    Valid Acc:  0.71875  at batch 649.\n",
            "Valid Loss: 1.1587703227996826    Valid Acc:  0.75  at batch 650.\n",
            "Valid Loss: 0.9174150228500366    Valid Acc:  0.9375  at batch 651.\n",
            "Valid Loss: 0.7623420357704163    Valid Acc:  0.875  at batch 652.\n",
            "Valid Loss: 1.113222360610962    Valid Acc:  0.6875  at batch 653.\n",
            "Valid Loss: 0.9768893718719482    Valid Acc:  0.6875  at batch 654.\n",
            "Valid Loss: 1.2081618309020996    Valid Acc:  0.78125  at batch 655.\n",
            "Valid Loss: 0.8629971742630005    Valid Acc:  0.90625  at batch 656.\n",
            "Valid Loss: 0.8331135511398315    Valid Acc:  0.875  at batch 657.\n",
            "Valid Loss: 1.154198169708252    Valid Acc:  0.59375  at batch 658.\n",
            "Valid Loss: 0.9305467009544373    Valid Acc:  0.875  at batch 659.\n",
            "Valid Loss: 1.134936809539795    Valid Acc:  0.8125  at batch 660.\n",
            "Valid Loss: 0.9285025596618652    Valid Acc:  0.8125  at batch 661.\n",
            "Valid Loss: 0.8741092681884766    Valid Acc:  0.9375  at batch 662.\n",
            "Valid Loss: 0.9705336689949036    Valid Acc:  0.78125  at batch 663.\n",
            "Valid Loss: 1.208033561706543    Valid Acc:  0.625  at batch 664.\n",
            "Valid Loss: 0.9801621437072754    Valid Acc:  0.8125  at batch 665.\n",
            "Valid Loss: 0.9929901957511902    Valid Acc:  0.8125  at batch 666.\n",
            "Valid Loss: 0.9582722187042236    Valid Acc:  0.78125  at batch 667.\n",
            "Valid Loss: 1.0507291555404663    Valid Acc:  0.8125  at batch 668.\n",
            "Valid Loss: 1.011606216430664    Valid Acc:  0.71875  at batch 669.\n",
            "Valid Loss: 0.9839473962783813    Valid Acc:  0.8125  at batch 670.\n",
            "Valid Loss: 1.1806869506835938    Valid Acc:  0.71875  at batch 671.\n",
            "Valid Loss: 1.0321629047393799    Valid Acc:  0.75  at batch 672.\n",
            "Valid Loss: 0.9854682683944702    Valid Acc:  0.71875  at batch 673.\n",
            "Valid Loss: 1.1056528091430664    Valid Acc:  0.75  at batch 674.\n",
            "Valid Loss: 1.0890090465545654    Valid Acc:  0.71875  at batch 675.\n",
            "Valid Loss: 1.0519529581069946    Valid Acc:  0.78125  at batch 676.\n",
            "Valid Loss: 1.0743210315704346    Valid Acc:  0.71875  at batch 677.\n",
            "Valid Loss: 0.9258543848991394    Valid Acc:  0.84375  at batch 678.\n",
            "Valid Loss: 1.018099308013916    Valid Acc:  0.78125  at batch 679.\n",
            "Valid Loss: 0.9045788645744324    Valid Acc:  0.9375  at batch 680.\n",
            "Valid Loss: 0.8763880729675293    Valid Acc:  0.875  at batch 681.\n",
            "Valid Loss: 0.9059068560600281    Valid Acc:  0.8125  at batch 682.\n",
            "Valid Loss: 0.8875236511230469    Valid Acc:  0.8125  at batch 683.\n",
            "Valid Loss: 1.00531005859375    Valid Acc:  0.71875  at batch 684.\n",
            "Valid Loss: 0.9650998711585999    Valid Acc:  0.84375  at batch 685.\n",
            "Valid Loss: 1.0063416957855225    Valid Acc:  0.875  at batch 686.\n",
            "Valid Loss: 0.8865085244178772    Valid Acc:  0.84375  at batch 687.\n",
            "Valid Loss: 1.0265376567840576    Valid Acc:  0.71875  at batch 688.\n",
            "Valid Loss: 1.0264885425567627    Valid Acc:  0.75  at batch 689.\n",
            "Valid Loss: 1.0600740909576416    Valid Acc:  0.8125  at batch 690.\n",
            "Valid Loss: 1.0487786531448364    Valid Acc:  0.90625  at batch 691.\n",
            "Valid Loss: 0.9702779054641724    Valid Acc:  0.78125  at batch 692.\n",
            "Valid Loss: 1.158173680305481    Valid Acc:  0.65625  at batch 693.\n",
            "Valid Loss: 1.0050352811813354    Valid Acc:  0.84375  at batch 694.\n",
            "Valid Loss: 0.9720091223716736    Valid Acc:  0.84375  at batch 695.\n",
            "Valid Loss: 1.1541361808776855    Valid Acc:  0.625  at batch 696.\n",
            "Valid Loss: 1.1637938022613525    Valid Acc:  0.78125  at batch 697.\n",
            "Valid Loss: 0.9824695587158203    Valid Acc:  0.8125  at batch 698.\n",
            "Valid Loss: 1.1188632249832153    Valid Acc:  0.59375  at batch 699.\n",
            "Valid Loss: 0.9360449910163879    Valid Acc:  0.875  at batch 700.\n",
            "Valid Loss: 1.0948734283447266    Valid Acc:  0.78125  at batch 701.\n",
            "Valid Loss: 1.050391435623169    Valid Acc:  0.84375  at batch 702.\n",
            "Valid Loss: 1.3264683485031128    Valid Acc:  0.59375  at batch 703.\n",
            "Valid Loss: 0.9922711849212646    Valid Acc:  0.875  at batch 704.\n",
            "Valid Loss: 1.1174591779708862    Valid Acc:  0.65625  at batch 705.\n",
            "Valid Loss: 0.9898347854614258    Valid Acc:  0.78125  at batch 706.\n",
            "Valid Loss: 0.9774311780929565    Valid Acc:  0.78125  at batch 707.\n",
            "Valid Loss: 0.9835731983184814    Valid Acc:  0.78125  at batch 708.\n",
            "Valid Loss: 0.9294382929801941    Valid Acc:  0.875  at batch 709.\n",
            "Valid Loss: 1.0061008930206299    Valid Acc:  0.75  at batch 710.\n",
            "Valid Loss: 0.6682583689689636    Valid Acc:  0.9375  at batch 711.\n",
            "Valid Loss: 0.888523280620575    Valid Acc:  0.71875  at batch 712.\n",
            "Valid Loss: 1.1173756122589111    Valid Acc:  0.65625  at batch 713.\n",
            "Valid Loss: 1.0341943502426147    Valid Acc:  0.75  at batch 714.\n",
            "Valid Loss: 0.9377270936965942    Valid Acc:  0.78125  at batch 715.\n",
            "Valid Loss: 0.7940771579742432    Valid Acc:  0.875  at batch 716.\n",
            "Valid Loss: 1.0239934921264648    Valid Acc:  0.84375  at batch 717.\n",
            "Valid Loss: 1.004164457321167    Valid Acc:  0.78125  at batch 718.\n",
            "Valid Loss: 0.8884939551353455    Valid Acc:  0.9375  at batch 719.\n",
            "Valid Loss: 1.0216224193572998    Valid Acc:  0.71875  at batch 720.\n",
            "Valid Loss: 0.977351725101471    Valid Acc:  0.78125  at batch 721.\n",
            "Valid Loss: 1.1131654977798462    Valid Acc:  0.8125  at batch 722.\n",
            "Valid Loss: 1.0610710382461548    Valid Acc:  0.84375  at batch 723.\n",
            "Valid Loss: 0.9968116879463196    Valid Acc:  0.6875  at batch 724.\n",
            "Valid Loss: 1.0139442682266235    Valid Acc:  0.78125  at batch 725.\n",
            "Valid Loss: 0.9207321405410767    Valid Acc:  0.84375  at batch 726.\n",
            "Valid Loss: 0.8867332339286804    Valid Acc:  0.875  at batch 727.\n",
            "Valid Loss: 1.1265907287597656    Valid Acc:  0.78125  at batch 728.\n",
            "Valid Loss: 0.9510334730148315    Valid Acc:  0.75  at batch 729.\n",
            "Valid Loss: 0.9637066721916199    Valid Acc:  0.8125  at batch 730.\n",
            "Valid Loss: 1.0494461059570312    Valid Acc:  0.71875  at batch 731.\n",
            "Valid Loss: 1.0651625394821167    Valid Acc:  0.84375  at batch 732.\n",
            "Valid Loss: 0.982860267162323    Valid Acc:  0.78125  at batch 733.\n",
            "Valid Loss: 1.0166206359863281    Valid Acc:  0.875  at batch 734.\n",
            "Valid Loss: 0.9621176719665527    Valid Acc:  0.84375  at batch 735.\n",
            "Valid Loss: 1.0954145193099976    Valid Acc:  0.75  at batch 736.\n",
            "Valid Loss: 0.9016045928001404    Valid Acc:  0.875  at batch 737.\n",
            "Valid Loss: 0.9420108199119568    Valid Acc:  0.78125  at batch 738.\n",
            "Valid Loss: 1.0029280185699463    Valid Acc:  0.84375  at batch 739.\n",
            "Valid Loss: 0.982765793800354    Valid Acc:  0.8125  at batch 740.\n",
            "Valid Loss: 0.9241272807121277    Valid Acc:  0.875  at batch 741.\n",
            "Valid Loss: 0.9103788733482361    Valid Acc:  0.96875  at batch 742.\n",
            "Valid Loss: 1.219638466835022    Valid Acc:  0.59375  at batch 743.\n",
            "Valid Loss: 0.8764784932136536    Valid Acc:  0.84375  at batch 744.\n",
            "Valid Loss: 1.0328441858291626    Valid Acc:  0.75  at batch 745.\n",
            "Valid Loss: 0.945375919342041    Valid Acc:  0.78125  at batch 746.\n",
            "Valid Loss: 1.0134629011154175    Valid Acc:  0.78125  at batch 747.\n",
            "Valid Loss: 1.0385940074920654    Valid Acc:  0.71875  at batch 748.\n",
            "Valid Loss: 1.0073891878128052    Valid Acc:  0.78125  at batch 749.\n",
            "Valid Loss: 1.1696544885635376    Valid Acc:  0.75  at batch 750.\n",
            "Valid Loss: 1.006221055984497    Valid Acc:  0.75  at batch 751.\n",
            "Valid Loss: 0.9577723145484924    Valid Acc:  0.78125  at batch 752.\n",
            "Valid Loss: 1.085518717765808    Valid Acc:  0.71875  at batch 753.\n",
            "Valid Loss: 0.9696944952011108    Valid Acc:  0.75  at batch 754.\n",
            "Valid Loss: 1.1740844249725342    Valid Acc:  0.71875  at batch 755.\n",
            "Valid Loss: 1.0479480028152466    Valid Acc:  0.6875  at batch 756.\n",
            "Valid Loss: 1.006971001625061    Valid Acc:  0.84375  at batch 757.\n",
            "Valid Loss: 0.8265449404716492    Valid Acc:  0.84375  at batch 758.\n",
            "Valid Loss: 0.9902786016464233    Valid Acc:  0.75  at batch 759.\n",
            "Valid Loss: 1.2151832580566406    Valid Acc:  0.65625  at batch 760.\n",
            "Valid Loss: 1.289081335067749    Valid Acc:  0.65625  at batch 761.\n",
            "Valid Loss: 1.07359778881073    Valid Acc:  0.8125  at batch 762.\n",
            "Valid Loss: 1.0378168821334839    Valid Acc:  0.8125  at batch 763.\n",
            "Valid Loss: 0.9034271240234375    Valid Acc:  0.8125  at batch 764.\n",
            "Valid Loss: 1.2290048599243164    Valid Acc:  0.71875  at batch 765.\n",
            "Valid Loss: 1.0984922647476196    Valid Acc:  0.71875  at batch 766.\n",
            "Valid Loss: 0.755599856376648    Valid Acc:  0.9375  at batch 767.\n",
            "Valid Loss: 1.0233566761016846    Valid Acc:  0.71875  at batch 768.\n",
            "Valid Loss: 0.8761693239212036    Valid Acc:  0.875  at batch 769.\n",
            "Valid Loss: 1.0721286535263062    Valid Acc:  0.84375  at batch 770.\n",
            "Valid Loss: 0.9982530474662781    Valid Acc:  0.8125  at batch 771.\n",
            "Valid Loss: 1.077101469039917    Valid Acc:  0.78125  at batch 772.\n",
            "Valid Loss: 1.029158115386963    Valid Acc:  0.78125  at batch 773.\n",
            "Valid Loss: 1.0742082595825195    Valid Acc:  0.8125  at batch 774.\n",
            "Valid Loss: 1.0719040632247925    Valid Acc:  0.6875  at batch 775.\n",
            "Valid Loss: 0.9034606218338013    Valid Acc:  0.8125  at batch 776.\n",
            "Valid Loss: 1.0883517265319824    Valid Acc:  0.8125  at batch 777.\n",
            "Valid Loss: 1.0013359785079956    Valid Acc:  0.78125  at batch 778.\n",
            "Valid Loss: 1.0005203485488892    Valid Acc:  0.75  at batch 779.\n",
            "Valid Loss: 1.0097163915634155    Valid Acc:  0.78125  at batch 780.\n",
            "Valid Loss: 0.9682818651199341    Valid Acc:  0.75  at batch 781.\n",
            "Valid Loss: 0.8492913842201233    Valid Acc:  0.90625  at batch 782.\n",
            "Valid Loss: 0.9882843494415283    Valid Acc:  0.84375  at batch 783.\n",
            "Valid Loss: 1.0011491775512695    Valid Acc:  0.8125  at batch 784.\n",
            "Valid Loss: 0.936731219291687    Valid Acc:  0.84375  at batch 785.\n",
            "Valid Loss: 0.8453062772750854    Valid Acc:  0.875  at batch 786.\n",
            "Valid Loss: 0.9894698858261108    Valid Acc:  0.71875  at batch 787.\n",
            "Valid Loss: 1.0489736795425415    Valid Acc:  0.78125  at batch 788.\n",
            "Valid Loss: 0.9577592611312866    Valid Acc:  0.8125  at batch 789.\n",
            "Valid Loss: 1.034985899925232    Valid Acc:  0.78125  at batch 790.\n",
            "Valid Loss: 0.9911777973175049    Valid Acc:  0.84375  at batch 791.\n",
            "Valid Loss: 0.947184681892395    Valid Acc:  0.71875  at batch 792.\n",
            "Valid Loss: 1.075037956237793    Valid Acc:  0.8125  at batch 793.\n",
            "Valid Loss: 1.0841110944747925    Valid Acc:  0.78125  at batch 794.\n",
            "Valid Loss: 1.2438063621520996    Valid Acc:  0.6875  at batch 795.\n",
            "Valid Loss: 0.8849757313728333    Valid Acc:  0.8125  at batch 796.\n",
            "Valid Loss: 0.8726162314414978    Valid Acc:  0.9375  at batch 797.\n",
            "Valid Loss: 0.9163864254951477    Valid Acc:  0.84375  at batch 798.\n",
            "Valid Loss: 1.0852609872817993    Valid Acc:  0.84375  at batch 799.\n",
            "Valid Loss: 0.9377432465553284    Valid Acc:  0.84375  at batch 800.\n",
            "Valid Loss: 0.9082661867141724    Valid Acc:  0.8125  at batch 801.\n",
            "Valid Loss: 0.8949166536331177    Valid Acc:  0.84375  at batch 802.\n",
            "Valid Loss: 0.9579476714134216    Valid Acc:  0.8125  at batch 803.\n",
            "Valid Loss: 0.9786720275878906    Valid Acc:  0.78125  at batch 804.\n",
            "Valid Loss: 1.0411359071731567    Valid Acc:  0.8125  at batch 805.\n",
            "Valid Loss: 0.895171046257019    Valid Acc:  0.8125  at batch 806.\n",
            "Valid Loss: 0.9426435232162476    Valid Acc:  0.8125  at batch 807.\n",
            "Valid Loss: 1.04059898853302    Valid Acc:  0.84375  at batch 808.\n",
            "Valid Loss: 1.1069254875183105    Valid Acc:  0.71875  at batch 809.\n",
            "Valid Loss: 1.2481731176376343    Valid Acc:  0.78125  at batch 810.\n",
            "Valid Loss: 1.0056135654449463    Valid Acc:  0.78125  at batch 811.\n",
            "Valid Loss: 0.7914202213287354    Valid Acc:  0.9375  at batch 812.\n",
            "Valid Loss: 0.9454103708267212    Valid Acc:  0.90625  at batch 813.\n",
            "Valid Loss: 0.9501294493675232    Valid Acc:  0.78125  at batch 814.\n",
            "Valid Loss: 1.074959635734558    Valid Acc:  0.71875  at batch 815.\n",
            "Valid Loss: 0.8712666630744934    Valid Acc:  0.875  at batch 816.\n",
            "Valid Loss: 1.031726598739624    Valid Acc:  0.8125  at batch 817.\n",
            "Valid Loss: 0.8644698858261108    Valid Acc:  0.78125  at batch 818.\n",
            "Valid Loss: 0.8951722979545593    Valid Acc:  0.8125  at batch 819.\n",
            "Valid Loss: 1.2041003704071045    Valid Acc:  0.6875  at batch 820.\n",
            "Valid Loss: 1.236591100692749    Valid Acc:  0.6875  at batch 821.\n",
            "Valid Loss: 0.9382576942443848    Valid Acc:  0.75  at batch 822.\n",
            "Valid Loss: 0.9642960429191589    Valid Acc:  0.8125  at batch 823.\n",
            "Valid Loss: 0.962959885597229    Valid Acc:  0.75  at batch 824.\n",
            "Valid Loss: 1.0039490461349487    Valid Acc:  0.75  at batch 825.\n",
            "Valid Loss: 0.9002182483673096    Valid Acc:  0.875  at batch 826.\n",
            "Valid Loss: 0.9629144668579102    Valid Acc:  0.78125  at batch 827.\n",
            "Valid Loss: 0.8938425779342651    Valid Acc:  0.84375  at batch 828.\n",
            "Valid Loss: 0.9912062883377075    Valid Acc:  0.75  at batch 829.\n",
            "Valid Loss: 1.1347362995147705    Valid Acc:  0.75  at batch 830.\n",
            "Valid Loss: 0.9069761633872986    Valid Acc:  0.84375  at batch 831.\n",
            "Valid Loss: 1.042803168296814    Valid Acc:  0.75  at batch 832.\n",
            "Valid Loss: 0.9935516119003296    Valid Acc:  0.875  at batch 833.\n",
            "Valid Loss: 1.003232479095459    Valid Acc:  0.75  at batch 834.\n",
            "Valid Loss: 0.9520421028137207    Valid Acc:  0.78125  at batch 835.\n",
            "Valid Loss: 0.913423478603363    Valid Acc:  0.90625  at batch 836.\n",
            "Valid Loss: 0.9666492342948914    Valid Acc:  0.78125  at batch 837.\n",
            "Valid Loss: 0.9724083542823792    Valid Acc:  0.78125  at batch 838.\n",
            "Valid Loss: 1.0661431550979614    Valid Acc:  0.8125  at batch 839.\n",
            "Valid Loss: 1.0126368999481201    Valid Acc:  0.75  at batch 840.\n",
            "Valid Loss: 0.9638424515724182    Valid Acc:  0.84375  at batch 841.\n",
            "Valid Loss: 0.8374139666557312    Valid Acc:  0.875  at batch 842.\n",
            "Valid Loss: 1.075144648551941    Valid Acc:  0.78125  at batch 843.\n",
            "Valid Loss: 1.2695730924606323    Valid Acc:  0.625  at batch 844.\n",
            "Valid Loss: 1.1424964666366577    Valid Acc:  0.6875  at batch 845.\n",
            "Valid Loss: 0.9304046630859375    Valid Acc:  0.84375  at batch 846.\n",
            "Valid Loss: 1.0300534963607788    Valid Acc:  0.78125  at batch 847.\n",
            "Valid Loss: 0.7768682241439819    Valid Acc:  0.96875  at batch 848.\n",
            "Valid Loss: 0.9751825928688049    Valid Acc:  0.8125  at batch 849.\n",
            "Valid Loss: 1.0325000286102295    Valid Acc:  0.71875  at batch 850.\n",
            "Valid Loss: 0.8115093111991882    Valid Acc:  0.9375  at batch 851.\n",
            "Valid Loss: 1.0590230226516724    Valid Acc:  0.71875  at batch 852.\n",
            "Valid Loss: 0.8979370594024658    Valid Acc:  0.84375  at batch 853.\n",
            "Valid Loss: 0.9466637372970581    Valid Acc:  0.875  at batch 854.\n",
            "Valid Loss: 0.9433790445327759    Valid Acc:  0.78125  at batch 855.\n",
            "Valid Loss: 1.0945521593093872    Valid Acc:  0.6875  at batch 856.\n",
            "Valid Loss: 1.127885341644287    Valid Acc:  0.71875  at batch 857.\n",
            "Valid Loss: 0.974181056022644    Valid Acc:  0.84375  at batch 858.\n",
            "Valid Loss: 1.1694318056106567    Valid Acc:  0.6875  at batch 859.\n",
            "Valid Loss: 1.0387221574783325    Valid Acc:  0.78125  at batch 860.\n",
            "Valid Loss: 1.0645740032196045    Valid Acc:  0.75  at batch 861.\n",
            "Valid Loss: 0.9618447422981262    Valid Acc:  0.71875  at batch 862.\n",
            "Valid Loss: 0.8784029483795166    Valid Acc:  0.84375  at batch 863.\n",
            "Valid Loss: 1.0328015089035034    Valid Acc:  0.78125  at batch 864.\n",
            "Valid Loss: 0.9132993817329407    Valid Acc:  0.84375  at batch 865.\n",
            "Valid Loss: 1.1080372333526611    Valid Acc:  0.75  at batch 866.\n",
            "Valid Loss: 0.89771968126297    Valid Acc:  0.875  at batch 867.\n",
            "Valid Loss: 1.2519457340240479    Valid Acc:  0.71875  at batch 868.\n",
            "Valid Loss: 0.8881268501281738    Valid Acc:  0.84375  at batch 869.\n",
            "Valid Loss: 0.7217611074447632    Valid Acc:  0.90625  at batch 870.\n",
            "Valid Loss: 1.0113095045089722    Valid Acc:  0.71875  at batch 871.\n",
            "Valid Loss: 1.0795856714248657    Valid Acc:  0.71875  at batch 872.\n",
            "Valid Loss: 1.2021936178207397    Valid Acc:  0.65625  at batch 873.\n",
            "Valid Loss: 1.2147618532180786    Valid Acc:  0.71875  at batch 874.\n",
            "Valid Loss: 0.9975765347480774    Valid Acc:  0.8125  at batch 875.\n",
            "Valid Loss: 0.8170539736747742    Valid Acc:  0.875  at batch 876.\n",
            "Valid Loss: 0.9764292240142822    Valid Acc:  0.875  at batch 877.\n",
            "Valid Loss: 0.9818546772003174    Valid Acc:  0.8125  at batch 878.\n",
            "Valid Loss: 1.0308725833892822    Valid Acc:  0.6875  at batch 879.\n",
            "Valid Loss: 1.0858409404754639    Valid Acc:  0.84375  at batch 880.\n",
            "Valid Loss: 1.0701501369476318    Valid Acc:  0.8125  at batch 881.\n",
            "Valid Loss: 1.2471868991851807    Valid Acc:  0.75  at batch 882.\n",
            "Valid Loss: 0.9704921841621399    Valid Acc:  0.75  at batch 883.\n",
            "Valid Loss: 1.0112683773040771    Valid Acc:  0.8125  at batch 884.\n",
            "Valid Loss: 1.1392773389816284    Valid Acc:  0.65625  at batch 885.\n",
            "Valid Loss: 0.915158212184906    Valid Acc:  0.90625  at batch 886.\n",
            "Valid Loss: 0.8787086606025696    Valid Acc:  0.9375  at batch 887.\n",
            "Valid Loss: 0.8252941966056824    Valid Acc:  0.90625  at batch 888.\n",
            "Valid Loss: 0.8852325677871704    Valid Acc:  0.84375  at batch 889.\n",
            "Valid Loss: 0.9580459594726562    Valid Acc:  0.875  at batch 890.\n",
            "Valid Loss: 0.9967466592788696    Valid Acc:  0.78125  at batch 891.\n",
            "Valid Loss: 0.9971917867660522    Valid Acc:  0.8125  at batch 892.\n",
            "Valid Loss: 0.9684563875198364    Valid Acc:  0.78125  at batch 893.\n",
            "Valid Loss: 1.0876452922821045    Valid Acc:  0.8125  at batch 894.\n",
            "Valid Loss: 0.7964458465576172    Valid Acc:  0.875  at batch 895.\n",
            "Valid Loss: 0.9602810740470886    Valid Acc:  0.78125  at batch 896.\n",
            "Valid Loss: 0.8649924397468567    Valid Acc:  0.9375  at batch 897.\n",
            "Valid Loss: 1.0313202142715454    Valid Acc:  0.75  at batch 898.\n",
            "Valid Loss: 1.043351650238037    Valid Acc:  0.75  at batch 899.\n",
            "Valid Loss: 0.945834755897522    Valid Acc:  0.78125  at batch 900.\n",
            "Valid Loss: 1.143505573272705    Valid Acc:  0.71875  at batch 901.\n",
            "Valid Loss: 0.9261434674263    Valid Acc:  0.90625  at batch 902.\n",
            "Valid Loss: 0.8823466897010803    Valid Acc:  0.875  at batch 903.\n",
            "Valid Loss: 0.8402004241943359    Valid Acc:  0.875  at batch 904.\n",
            "Valid Loss: 1.024643898010254    Valid Acc:  0.78125  at batch 905.\n",
            "Valid Loss: 1.0434987545013428    Valid Acc:  0.71875  at batch 906.\n",
            "Valid Loss: 0.9599334001541138    Valid Acc:  0.75  at batch 907.\n",
            "Valid Loss: 1.037881851196289    Valid Acc:  0.6875  at batch 908.\n",
            "Valid Loss: 0.8616822361946106    Valid Acc:  0.90625  at batch 909.\n",
            "Valid Loss: 0.9248042106628418    Valid Acc:  0.84375  at batch 910.\n",
            "Valid Loss: 1.148755669593811    Valid Acc:  0.6875  at batch 911.\n",
            "Valid Loss: 0.8814176321029663    Valid Acc:  0.8125  at batch 912.\n",
            "Valid Loss: 1.2521005868911743    Valid Acc:  0.6875  at batch 913.\n",
            "Valid Loss: 0.9690264463424683    Valid Acc:  0.75  at batch 914.\n",
            "Valid Loss: 1.2996786832809448    Valid Acc:  0.6875  at batch 915.\n",
            "Valid Loss: 1.0939613580703735    Valid Acc:  0.78125  at batch 916.\n",
            "Valid Loss: 1.0079764127731323    Valid Acc:  0.8125  at batch 917.\n",
            "Valid Loss: 0.9330183863639832    Valid Acc:  0.78125  at batch 918.\n",
            "Valid Loss: 1.0443518161773682    Valid Acc:  0.84375  at batch 919.\n",
            "Valid Loss: 1.0313822031021118    Valid Acc:  0.875  at batch 920.\n",
            "Valid Loss: 0.9838019609451294    Valid Acc:  0.875  at batch 921.\n",
            "Valid Loss: 0.9251757264137268    Valid Acc:  0.84375  at batch 922.\n",
            "Valid Loss: 1.0953805446624756    Valid Acc:  0.71875  at batch 923.\n",
            "Valid Loss: 1.0072494745254517    Valid Acc:  0.71875  at batch 924.\n",
            "Valid Loss: 1.1620582342147827    Valid Acc:  0.75  at batch 925.\n",
            "Valid Loss: 1.0420713424682617    Valid Acc:  0.65625  at batch 926.\n",
            "Valid Loss: 1.1240992546081543    Valid Acc:  0.75  at batch 927.\n",
            "Valid Loss: 1.0423022508621216    Valid Acc:  0.8125  at batch 928.\n",
            "Valid Loss: 0.8349523544311523    Valid Acc:  0.84375  at batch 929.\n",
            "Valid Loss: 1.076830506324768    Valid Acc:  0.78125  at batch 930.\n",
            "Valid Loss: 0.896949052810669    Valid Acc:  0.9375  at batch 931.\n",
            "Valid Loss: 1.0267462730407715    Valid Acc:  0.75  at batch 932.\n",
            "Valid Loss: 1.1439846754074097    Valid Acc:  0.75  at batch 933.\n",
            "Valid Loss: 1.1335119009017944    Valid Acc:  0.75  at batch 934.\n",
            "Valid Loss: 0.9759615659713745    Valid Acc:  0.875  at batch 935.\n",
            "Valid Loss: 1.2163023948669434    Valid Acc:  0.59375  at batch 936.\n",
            "Valid Loss: 1.0898687839508057    Valid Acc:  0.8125  at batch 937.\n",
            "Valid Loss: 1.0259101390838623    Valid Acc:  0.75  at batch 938.\n",
            "Valid Loss: 0.9149728417396545    Valid Acc:  0.84375  at batch 939.\n",
            "Valid Loss: 0.9573249816894531    Valid Acc:  0.78125  at batch 940.\n",
            "Valid Loss: 0.9336662888526917    Valid Acc:  0.78125  at batch 941.\n",
            "Valid Loss: 1.081549882888794    Valid Acc:  0.75  at batch 942.\n",
            "Valid Loss: 1.0740396976470947    Valid Acc:  0.8125  at batch 943.\n",
            "Valid Loss: 1.0930259227752686    Valid Acc:  0.75  at batch 944.\n",
            "Valid Loss: 1.065725326538086    Valid Acc:  0.71875  at batch 945.\n",
            "Valid Loss: 1.005034327507019    Valid Acc:  0.8125  at batch 946.\n",
            "Valid Loss: 1.1050246953964233    Valid Acc:  0.71875  at batch 947.\n",
            "Valid Loss: 1.244661569595337    Valid Acc:  0.6875  at batch 948.\n",
            "Valid Loss: 0.9212695956230164    Valid Acc:  0.75  at batch 949.\n",
            "Valid Loss: 0.9751790761947632    Valid Acc:  0.78125  at batch 950.\n",
            "Valid Loss: 1.1982648372650146    Valid Acc:  0.71875  at batch 951.\n",
            "Valid Loss: 1.065213918685913    Valid Acc:  0.8125  at batch 952.\n",
            "Valid Loss: 0.9774477481842041    Valid Acc:  0.875  at batch 953.\n",
            "Valid Loss: 0.9912049174308777    Valid Acc:  0.78125  at batch 954.\n",
            "Valid Loss: 1.2381305694580078    Valid Acc:  0.71875  at batch 955.\n",
            "Valid Loss: 1.1064132452011108    Valid Acc:  0.78125  at batch 956.\n",
            "Valid Loss: 0.9996459484100342    Valid Acc:  0.875  at batch 957.\n",
            "Valid Loss: 1.0344246625900269    Valid Acc:  0.71875  at batch 958.\n",
            "Valid Loss: 0.8962416052818298    Valid Acc:  0.84375  at batch 959.\n",
            "Valid Loss: 1.0549803972244263    Valid Acc:  0.78125  at batch 960.\n",
            "Valid Loss: 1.1372737884521484    Valid Acc:  0.71875  at batch 961.\n",
            "Valid Loss: 0.9257832169532776    Valid Acc:  0.71875  at batch 962.\n",
            "Valid Loss: 1.0138473510742188    Valid Acc:  0.84375  at batch 963.\n",
            "Valid Loss: 1.322704553604126    Valid Acc:  0.59375  at batch 964.\n",
            "Valid Loss: 0.9335889220237732    Valid Acc:  0.84375  at batch 965.\n",
            "Valid Loss: 1.0276457071304321    Valid Acc:  0.78125  at batch 966.\n",
            "Valid Loss: 1.1053595542907715    Valid Acc:  0.6875  at batch 967.\n",
            "Valid Loss: 0.9862917065620422    Valid Acc:  0.875  at batch 968.\n",
            "Valid Loss: 0.9150369763374329    Valid Acc:  0.8125  at batch 969.\n",
            "Valid Loss: 1.0290638208389282    Valid Acc:  0.78125  at batch 970.\n",
            "Valid Loss: 1.0365076065063477    Valid Acc:  0.84375  at batch 971.\n",
            "Valid Loss: 1.1456624269485474    Valid Acc:  0.75  at batch 972.\n",
            "Valid Loss: 1.0340282917022705    Valid Acc:  0.71875  at batch 973.\n",
            "Valid Loss: 0.9541128873825073    Valid Acc:  0.8125  at batch 974.\n",
            "Valid Loss: 1.0013378858566284    Valid Acc:  0.6875  at batch 975.\n",
            "Valid Loss: 1.0337411165237427    Valid Acc:  0.84375  at batch 976.\n",
            "Valid Loss: 1.026624083518982    Valid Acc:  0.78125  at batch 977.\n",
            "Valid Loss: 1.0468506813049316    Valid Acc:  0.71875  at batch 978.\n",
            "Valid Loss: 0.9579333662986755    Valid Acc:  0.8125  at batch 979.\n",
            "Valid Loss: 0.9486238360404968    Valid Acc:  0.84375  at batch 980.\n",
            "Valid Loss: 1.1020874977111816    Valid Acc:  0.75  at batch 981.\n",
            "Valid Loss: 0.8313045501708984    Valid Acc:  0.84375  at batch 982.\n",
            "Valid Loss: 1.062419056892395    Valid Acc:  0.75  at batch 983.\n",
            "Valid Loss: 0.884318470954895    Valid Acc:  0.875  at batch 984.\n",
            "Valid Loss: 0.9451708793640137    Valid Acc:  0.75  at batch 985.\n",
            "Valid Loss: 0.8587860465049744    Valid Acc:  0.84375  at batch 986.\n",
            "Valid Loss: 1.0055664777755737    Valid Acc:  0.78125  at batch 987.\n",
            "Valid Loss: 0.8768558502197266    Valid Acc:  0.875  at batch 988.\n",
            "Valid Loss: 1.0705137252807617    Valid Acc:  0.6875  at batch 989.\n",
            "Valid Loss: 0.9930446147918701    Valid Acc:  0.8125  at batch 990.\n",
            "Valid Loss: 1.0482518672943115    Valid Acc:  0.84375  at batch 991.\n",
            "Valid Loss: 1.1798512935638428    Valid Acc:  0.59375  at batch 992.\n",
            "Valid Loss: 1.0577406883239746    Valid Acc:  0.71875  at batch 993.\n",
            "Valid Loss: 0.9783417582511902    Valid Acc:  0.71875  at batch 994.\n",
            "Valid Loss: 1.0059117078781128    Valid Acc:  0.78125  at batch 995.\n",
            "Valid Loss: 0.9590961933135986    Valid Acc:  0.875  at batch 996.\n",
            "Valid Loss: 0.9002366065979004    Valid Acc:  0.90625  at batch 997.\n",
            "Valid Loss: 1.0907796621322632    Valid Acc:  0.65625  at batch 998.\n",
            "Valid Loss: 0.8879650831222534    Valid Acc:  0.84375  at batch 999.\n",
            "Valid Loss: 1.0586061477661133    Valid Acc:  0.71875  at batch 1000.\n",
            "Valid Loss: 1.0246447324752808    Valid Acc:  0.71875  at batch 1001.\n",
            "Valid Loss: 0.9586760997772217    Valid Acc:  0.8125  at batch 1002.\n",
            "Valid Loss: 1.0826938152313232    Valid Acc:  0.8125  at batch 1003.\n",
            "Valid Loss: 1.0960822105407715    Valid Acc:  0.78125  at batch 1004.\n",
            "Valid Loss: 1.0037873983383179    Valid Acc:  0.8125  at batch 1005.\n",
            "Valid Loss: 0.8782462477684021    Valid Acc:  0.875  at batch 1006.\n",
            "Valid Loss: 1.0000427961349487    Valid Acc:  0.8125  at batch 1007.\n",
            "Valid Loss: 0.9867404699325562    Valid Acc:  0.8125  at batch 1008.\n",
            "Valid Loss: 1.099056363105774    Valid Acc:  0.625  at batch 1009.\n",
            "Valid Loss: 0.8533257246017456    Valid Acc:  0.84375  at batch 1010.\n",
            "Valid Loss: 0.8924691677093506    Valid Acc:  0.8125  at batch 1011.\n",
            "Valid Loss: 1.0148441791534424    Valid Acc:  0.8125  at batch 1012.\n",
            "Valid Loss: 0.9749473333358765    Valid Acc:  0.78125  at batch 1013.\n",
            "Valid Loss: 1.0529340505599976    Valid Acc:  0.65625  at batch 1014.\n",
            "Valid Loss: 1.005599856376648    Valid Acc:  0.75  at batch 1015.\n",
            "Valid Loss: 1.1302082538604736    Valid Acc:  0.65625  at batch 1016.\n",
            "Valid Loss: 1.0387811660766602    Valid Acc:  0.78125  at batch 1017.\n",
            "Valid Loss: 1.2213999032974243    Valid Acc:  0.78125  at batch 1018.\n",
            "Valid Loss: 0.83916836977005    Valid Acc:  0.96875  at batch 1019.\n",
            "Valid Loss: 0.8669463992118835    Valid Acc:  0.8125  at batch 1020.\n",
            "Valid Loss: 1.0807862281799316    Valid Acc:  0.8125  at batch 1021.\n",
            "Valid Loss: 0.9937926530838013    Valid Acc:  0.71875  at batch 1022.\n",
            "Valid Loss: 0.9288407564163208    Valid Acc:  0.8125  at batch 1023.\n",
            "Valid Loss: 1.0556747913360596    Valid Acc:  0.78125  at batch 1024.\n",
            "Valid Loss: 1.1177512407302856    Valid Acc:  0.75  at batch 1025.\n",
            "Valid Loss: 1.052259087562561    Valid Acc:  0.75  at batch 1026.\n",
            "Valid Loss: 1.03196120262146    Valid Acc:  0.875  at batch 1027.\n",
            "Valid Loss: 0.9708653092384338    Valid Acc:  0.8125  at batch 1028.\n",
            "Valid Loss: 1.1048038005828857    Valid Acc:  0.75  at batch 1029.\n",
            "Valid Loss: 1.0450055599212646    Valid Acc:  0.75  at batch 1030.\n",
            "Valid Loss: 0.9753138422966003    Valid Acc:  0.8125  at batch 1031.\n",
            "Valid Loss: 1.035323143005371    Valid Acc:  0.78125  at batch 1032.\n",
            "Valid Loss: 1.0306732654571533    Valid Acc:  0.78125  at batch 1033.\n",
            "Valid Loss: 0.9390428066253662    Valid Acc:  0.8125  at batch 1034.\n",
            "Valid Loss: 0.919684648513794    Valid Acc:  0.90625  at batch 1035.\n",
            "Valid Loss: 1.0645414590835571    Valid Acc:  0.71875  at batch 1036.\n",
            "Valid Loss: 1.0682669878005981    Valid Acc:  0.78125  at batch 1037.\n",
            "Valid Loss: 1.111107349395752    Valid Acc:  0.71875  at batch 1038.\n",
            "Valid Loss: 0.8758552670478821    Valid Acc:  0.90625  at batch 1039.\n",
            "Valid Loss: 0.9309519529342651    Valid Acc:  0.8125  at batch 1040.\n",
            "Valid Loss: 1.0162936449050903    Valid Acc:  0.78125  at batch 1041.\n",
            "Valid Loss: 1.1523633003234863    Valid Acc:  0.71875  at batch 1042.\n",
            "Valid Loss: 1.1112220287322998    Valid Acc:  0.71875  at batch 1043.\n",
            "Valid Loss: 1.013986349105835    Valid Acc:  0.78125  at batch 1044.\n",
            "Valid Loss: 1.0084365606307983    Valid Acc:  0.84375  at batch 1045.\n",
            "Valid Loss: 0.9593707323074341    Valid Acc:  0.8125  at batch 1046.\n",
            "Valid Loss: 1.05225670337677    Valid Acc:  0.71875  at batch 1047.\n",
            "Valid Loss: 1.0875349044799805    Valid Acc:  0.8125  at batch 1048.\n",
            "Valid Loss: 0.9636955857276917    Valid Acc:  0.75  at batch 1049.\n",
            "Valid Loss: 0.9627944827079773    Valid Acc:  0.84375  at batch 1050.\n",
            "Valid Loss: 1.0864461660385132    Valid Acc:  0.75  at batch 1051.\n",
            "Valid Loss: 0.9208024144172668    Valid Acc:  0.875  at batch 1052.\n",
            "Valid Loss: 1.0312526226043701    Valid Acc:  0.78125  at batch 1053.\n",
            "Valid Loss: 0.9423050284385681    Valid Acc:  0.75  at batch 1054.\n",
            "Valid Loss: 0.9994866251945496    Valid Acc:  0.78125  at batch 1055.\n",
            "Valid Loss: 0.8969351649284363    Valid Acc:  0.8125  at batch 1056.\n",
            "Valid Loss: 1.0651485919952393    Valid Acc:  0.8125  at batch 1057.\n",
            "Valid Loss: 0.9180586338043213    Valid Acc:  0.8125  at batch 1058.\n",
            "Valid Loss: 1.0519990921020508    Valid Acc:  0.75  at batch 1059.\n",
            "Valid Loss: 0.9020477533340454    Valid Acc:  0.84375  at batch 1060.\n",
            "Valid Loss: 0.9258996844291687    Valid Acc:  0.8125  at batch 1061.\n",
            "Valid Loss: 0.9965661764144897    Valid Acc:  0.84375  at batch 1062.\n",
            "Valid Loss: 1.0195014476776123    Valid Acc:  0.71875  at batch 1063.\n",
            "Valid Loss: 1.0265061855316162    Valid Acc:  0.78125  at batch 1064.\n",
            "Valid Loss: 0.9527779817581177    Valid Acc:  0.78125  at batch 1065.\n",
            "Valid Loss: 1.0127018690109253    Valid Acc:  0.75  at batch 1066.\n",
            "Valid Loss: 1.087429165840149    Valid Acc:  0.78125  at batch 1067.\n",
            "Valid Loss: 0.9359521269798279    Valid Acc:  0.90625  at batch 1068.\n",
            "Valid Loss: 0.9976092576980591    Valid Acc:  0.71875  at batch 1069.\n",
            "Valid Loss: 1.0254592895507812    Valid Acc:  0.8125  at batch 1070.\n",
            "Valid Loss: 1.0769011974334717    Valid Acc:  0.84375  at batch 1071.\n",
            "Valid Loss: 1.0650238990783691    Valid Acc:  0.75  at batch 1072.\n",
            "Valid Loss: 1.0161514282226562    Valid Acc:  0.8125  at batch 1073.\n",
            "Valid Loss: 0.9939759373664856    Valid Acc:  0.8125  at batch 1074.\n",
            "Valid Loss: 0.9707097411155701    Valid Acc:  0.71875  at batch 1075.\n",
            "Valid Loss: 0.9534201622009277    Valid Acc:  0.78125  at batch 1076.\n",
            "Valid Loss: 0.7973378896713257    Valid Acc:  0.875  at batch 1077.\n",
            "Valid Loss: 1.0734045505523682    Valid Acc:  0.71875  at batch 1078.\n",
            "Valid Loss: 1.0791980028152466    Valid Acc:  0.78125  at batch 1079.\n",
            "Valid Loss: 1.1330804824829102    Valid Acc:  0.84375  at batch 1080.\n",
            "Valid Loss: 1.1679835319519043    Valid Acc:  0.75  at batch 1081.\n",
            "Valid Loss: 0.9796545505523682    Valid Acc:  0.875  at batch 1082.\n",
            "Valid Loss: 0.7887117266654968    Valid Acc:  0.96875  at batch 1083.\n",
            "Valid Loss: 1.148269534111023    Valid Acc:  0.6875  at batch 1084.\n",
            "Valid Loss: 1.112991452217102    Valid Acc:  0.71875  at batch 1085.\n",
            "Valid Loss: 1.1406739950180054    Valid Acc:  0.71875  at batch 1086.\n",
            "Valid Loss: 0.977067768573761    Valid Acc:  0.875  at batch 1087.\n",
            "Valid Loss: 0.951163113117218    Valid Acc:  0.90625  at batch 1088.\n",
            "Valid Loss: 1.0173295736312866    Valid Acc:  0.8125  at batch 1089.\n",
            "Valid Loss: 1.2289083003997803    Valid Acc:  0.75  at batch 1090.\n",
            "Valid Loss: 0.9363601803779602    Valid Acc:  0.8125  at batch 1091.\n",
            "Valid Loss: 1.018249750137329    Valid Acc:  0.65625  at batch 1092.\n",
            "Valid Loss: 0.8394588828086853    Valid Acc:  0.8125  at batch 1093.\n",
            "Valid Loss: 1.040724277496338    Valid Acc:  0.8125  at batch 1094.\n",
            "Valid Loss: 0.9375513792037964    Valid Acc:  0.78125  at batch 1095.\n",
            "Valid Loss: 1.0791547298431396    Valid Acc:  0.78125  at batch 1096.\n",
            "Valid Loss: 0.9399169087409973    Valid Acc:  0.9375  at batch 1097.\n",
            "Valid Loss: 1.072633147239685    Valid Acc:  0.8125  at batch 1098.\n",
            "Valid Loss: 1.0594030618667603    Valid Acc:  0.84375  at batch 1099.\n",
            "Valid Loss: 0.9589189291000366    Valid Acc:  0.84375  at batch 1100.\n",
            "Valid Loss: 1.0076217651367188    Valid Acc:  0.78125  at batch 1101.\n",
            "Valid Loss: 1.0062472820281982    Valid Acc:  0.875  at batch 1102.\n",
            "Valid Loss: 0.9734440445899963    Valid Acc:  0.75  at batch 1103.\n",
            "Valid Loss: 0.9639918208122253    Valid Acc:  0.84375  at batch 1104.\n",
            "Valid Loss: 0.8443089723587036    Valid Acc:  0.8125  at batch 1105.\n",
            "Valid Loss: 0.9896389842033386    Valid Acc:  0.84375  at batch 1106.\n",
            "Valid Loss: 1.005689024925232    Valid Acc:  0.78125  at batch 1107.\n",
            "Valid Loss: 1.2933650016784668    Valid Acc:  0.65625  at batch 1108.\n",
            "Valid Loss: 1.0799015760421753    Valid Acc:  0.59375  at batch 1109.\n",
            "Valid Loss: 0.8691667914390564    Valid Acc:  0.90625  at batch 1110.\n",
            "Valid Loss: 1.100083827972412    Valid Acc:  0.78125  at batch 1111.\n",
            "Valid Loss: 1.042250156402588    Valid Acc:  0.8125  at batch 1112.\n",
            "Valid Loss: 0.9374866485595703    Valid Acc:  0.875  at batch 1113.\n",
            "Valid Loss: 1.0121523141860962    Valid Acc:  0.78125  at batch 1114.\n",
            "Valid Loss: 1.2330021858215332    Valid Acc:  0.65625  at batch 1115.\n",
            "Valid Loss: 0.9187713861465454    Valid Acc:  0.8125  at batch 1116.\n",
            "Valid Loss: 1.1149417161941528    Valid Acc:  0.6875  at batch 1117.\n",
            "Valid Loss: 0.8346001505851746    Valid Acc:  0.875  at batch 1118.\n",
            "Valid Loss: 1.0287013053894043    Valid Acc:  0.78125  at batch 1119.\n",
            "Valid Loss: 1.2303582429885864    Valid Acc:  0.71875  at batch 1120.\n",
            "Valid Loss: 0.7565944790840149    Valid Acc:  0.90625  at batch 1121.\n",
            "Valid Loss: 0.8969169855117798    Valid Acc:  0.875  at batch 1122.\n",
            "Valid Loss: 0.9711546897888184    Valid Acc:  0.78125  at batch 1123.\n",
            "Valid Loss: 0.9626929759979248    Valid Acc:  0.78125  at batch 1124.\n",
            "Valid Loss: 1.0367560386657715    Valid Acc:  0.625  at batch 1125.\n",
            "Valid Loss: 1.0059056282043457    Valid Acc:  0.84375  at batch 1126.\n",
            "Valid Loss: 1.1118413209915161    Valid Acc:  0.75  at batch 1127.\n",
            "Valid Loss: 0.9277852177619934    Valid Acc:  0.78125  at batch 1128.\n",
            "Valid Loss: 0.8839973211288452    Valid Acc:  0.90625  at batch 1129.\n",
            "Valid Loss: 1.0106174945831299    Valid Acc:  0.84375  at batch 1130.\n",
            "Valid Loss: 1.0331194400787354    Valid Acc:  0.75  at batch 1131.\n",
            "Valid Loss: 1.0224332809448242    Valid Acc:  0.75  at batch 1132.\n",
            "Valid Loss: 0.9478153586387634    Valid Acc:  0.78125  at batch 1133.\n",
            "Valid Loss: 1.050175666809082    Valid Acc:  0.71875  at batch 1134.\n",
            "Valid Loss: 1.0272146463394165    Valid Acc:  0.6875  at batch 1135.\n",
            "Valid Loss: 0.8009337782859802    Valid Acc:  0.875  at batch 1136.\n",
            "Valid Loss: 0.9259394407272339    Valid Acc:  0.84375  at batch 1137.\n",
            "Valid Loss: 0.8819700479507446    Valid Acc:  0.875  at batch 1138.\n",
            "Valid Loss: 1.1744194030761719    Valid Acc:  0.6875  at batch 1139.\n",
            "Valid Loss: 0.9348903298377991    Valid Acc:  0.78125  at batch 1140.\n",
            "Valid Loss: 0.8713717460632324    Valid Acc:  0.84375  at batch 1141.\n",
            "Valid Loss: 0.9693732857704163    Valid Acc:  0.78125  at batch 1142.\n",
            "Valid Loss: 0.8737246990203857    Valid Acc:  0.875  at batch 1143.\n",
            "Valid Loss: 1.0221679210662842    Valid Acc:  0.875  at batch 1144.\n",
            "Valid Loss: 0.998862624168396    Valid Acc:  0.78125  at batch 1145.\n",
            "Valid Loss: 0.9635580778121948    Valid Acc:  0.84375  at batch 1146.\n",
            "Valid Loss: 1.0150600671768188    Valid Acc:  0.78125  at batch 1147.\n",
            "Valid Loss: 0.8982999920845032    Valid Acc:  0.90625  at batch 1148.\n",
            "Valid Loss: 1.0654700994491577    Valid Acc:  0.78125  at batch 1149.\n",
            "Valid Loss: 1.0506823062896729    Valid Acc:  0.71875  at batch 1150.\n",
            "Valid Loss: 1.1251565217971802    Valid Acc:  0.6875  at batch 1151.\n",
            "Valid Loss: 0.9802239537239075    Valid Acc:  0.75  at batch 1152.\n",
            "Valid Loss: 0.850193440914154    Valid Acc:  0.78125  at batch 1153.\n",
            "Valid Loss: 1.1200438737869263    Valid Acc:  0.71875  at batch 1154.\n",
            "Valid Loss: 1.007798671722412    Valid Acc:  0.8125  at batch 1155.\n",
            "Valid Loss: 1.126629114151001    Valid Acc:  0.6875  at batch 1156.\n",
            "Valid Loss: 1.0177868604660034    Valid Acc:  0.84375  at batch 1157.\n",
            "Valid Loss: 1.0830605030059814    Valid Acc:  0.75  at batch 1158.\n",
            "Valid Loss: 1.1200168132781982    Valid Acc:  0.8125  at batch 1159.\n",
            "Valid Loss: 1.0346338748931885    Valid Acc:  0.71875  at batch 1160.\n",
            "Valid Loss: 1.2002583742141724    Valid Acc:  0.8125  at batch 1161.\n",
            "Valid Loss: 1.1050727367401123    Valid Acc:  0.71875  at batch 1162.\n",
            "Valid Loss: 1.1508899927139282    Valid Acc:  0.625  at batch 1163.\n",
            "Valid Loss: 0.8867817521095276    Valid Acc:  0.84375  at batch 1164.\n",
            "Valid Loss: 1.030203104019165    Valid Acc:  0.71875  at batch 1165.\n",
            "Valid Loss: 0.9785068035125732    Valid Acc:  0.8125  at batch 1166.\n",
            "Valid Loss: 0.986922025680542    Valid Acc:  0.875  at batch 1167.\n",
            "Valid Loss: 0.9517385959625244    Valid Acc:  0.8125  at batch 1168.\n",
            "Valid Loss: 0.8452938795089722    Valid Acc:  0.875  at batch 1169.\n",
            "Valid Loss: 1.0808932781219482    Valid Acc:  0.84375  at batch 1170.\n",
            "Valid Loss: 0.963282585144043    Valid Acc:  0.8125  at batch 1171.\n",
            "Valid Loss: 1.1524862051010132    Valid Acc:  0.6875  at batch 1172.\n",
            "Valid Loss: 1.0030498504638672    Valid Acc:  0.75  at batch 1173.\n",
            "Valid Loss: 0.8979598879814148    Valid Acc:  0.75  at batch 1174.\n",
            "Valid Loss: 1.0259597301483154    Valid Acc:  0.84375  at batch 1175.\n",
            "Valid Loss: 0.946127712726593    Valid Acc:  0.8125  at batch 1176.\n",
            "Valid Loss: 1.0278149843215942    Valid Acc:  0.75  at batch 1177.\n",
            "Valid Loss: 1.108030915260315    Valid Acc:  0.78125  at batch 1178.\n",
            "Valid Loss: 1.0443427562713623    Valid Acc:  0.8125  at batch 1179.\n",
            "Valid Loss: 1.0885350704193115    Valid Acc:  0.71875  at batch 1180.\n",
            "Valid Loss: 0.9308989644050598    Valid Acc:  0.84375  at batch 1181.\n",
            "Valid Loss: 0.8842974305152893    Valid Acc:  0.84375  at batch 1182.\n",
            "Valid Loss: 1.0239521265029907    Valid Acc:  0.78125  at batch 1183.\n",
            "Valid Loss: 0.9655542969703674    Valid Acc:  0.8125  at batch 1184.\n",
            "Valid Loss: 0.9920661449432373    Valid Acc:  0.8125  at batch 1185.\n",
            "Valid Loss: 1.0217374563217163    Valid Acc:  0.75  at batch 1186.\n",
            "Valid Loss: 0.9399634003639221    Valid Acc:  0.84375  at batch 1187.\n",
            "Valid Loss: 0.8822627067565918    Valid Acc:  0.8125  at batch 1188.\n",
            "Valid Loss: 0.9493304491043091    Valid Acc:  0.84375  at batch 1189.\n",
            "Valid Loss: 1.0534398555755615    Valid Acc:  0.65625  at batch 1190.\n",
            "Valid Loss: 0.9861006736755371    Valid Acc:  0.78125  at batch 1191.\n",
            "Valid Loss: 0.9802278280258179    Valid Acc:  0.78125  at batch 1192.\n",
            "Valid Loss: 0.9176325798034668    Valid Acc:  0.8125  at batch 1193.\n",
            "Valid Loss: 0.9824004769325256    Valid Acc:  0.8125  at batch 1194.\n",
            "Valid Loss: 1.002825379371643    Valid Acc:  0.875  at batch 1195.\n",
            "Valid Loss: 0.9249282479286194    Valid Acc:  0.90625  at batch 1196.\n",
            "Valid Loss: 0.9578673839569092    Valid Acc:  0.75  at batch 1197.\n",
            "Valid Loss: 1.2863441705703735    Valid Acc:  0.71875  at batch 1198.\n",
            "Valid Loss: 1.0007586479187012    Valid Acc:  0.8125  at batch 1199.\n",
            "Valid Loss: 1.0580025911331177    Valid Acc:  0.75  at batch 1200.\n",
            "Valid Loss: 0.9183305501937866    Valid Acc:  0.84375  at batch 1201.\n",
            "Valid Loss: 1.0497361421585083    Valid Acc:  0.75  at batch 1202.\n",
            "Valid Loss: 0.9142412543296814    Valid Acc:  0.875  at batch 1203.\n",
            "Valid Loss: 0.8887994885444641    Valid Acc:  0.8125  at batch 1204.\n",
            "Valid Loss: 0.9371567964553833    Valid Acc:  0.8125  at batch 1205.\n",
            "Valid Loss: 0.959604024887085    Valid Acc:  0.8125  at batch 1206.\n",
            "Valid Loss: 0.8728824257850647    Valid Acc:  0.8125  at batch 1207.\n",
            "Valid Loss: 1.1575223207473755    Valid Acc:  0.75  at batch 1208.\n",
            "Valid Loss: 0.9020597338676453    Valid Acc:  0.78125  at batch 1209.\n",
            "Valid Loss: 0.859607994556427    Valid Acc:  0.84375  at batch 1210.\n",
            "Valid Loss: 1.0143380165100098    Valid Acc:  0.78125  at batch 1211.\n",
            "Valid Loss: 1.0806045532226562    Valid Acc:  0.75  at batch 1212.\n",
            "Valid Loss: 1.1427435874938965    Valid Acc:  0.6875  at batch 1213.\n",
            "Valid Loss: 1.0231140851974487    Valid Acc:  0.71875  at batch 1214.\n",
            "Valid Loss: 1.016086220741272    Valid Acc:  0.71875  at batch 1215.\n",
            "Valid Loss: 0.9085618853569031    Valid Acc:  0.8125  at batch 1216.\n",
            "Valid Loss: 0.8477841019630432    Valid Acc:  0.8125  at batch 1217.\n",
            "Valid Loss: 0.9051937460899353    Valid Acc:  0.84375  at batch 1218.\n",
            "Valid Loss: 1.0396027565002441    Valid Acc:  0.78125  at batch 1219.\n",
            "Valid Loss: 0.7548555135726929    Valid Acc:  0.875  at batch 1220.\n",
            "Valid Loss: 0.9303171038627625    Valid Acc:  0.875  at batch 1221.\n",
            "Valid Loss: 1.063930869102478    Valid Acc:  0.8125  at batch 1222.\n",
            "Valid Loss: 0.940596342086792    Valid Acc:  0.78125  at batch 1223.\n",
            "Valid Loss: 1.073646903038025    Valid Acc:  0.6875  at batch 1224.\n",
            "Valid Loss: 0.9428332448005676    Valid Acc:  0.6875  at batch 1225.\n",
            "Valid Loss: 0.9286321997642517    Valid Acc:  0.78125  at batch 1226.\n",
            "Valid Loss: 0.9453116655349731    Valid Acc:  0.90625  at batch 1227.\n",
            "Valid Loss: 0.8434882760047913    Valid Acc:  0.875  at batch 1228.\n",
            "Valid Loss: 0.9950567483901978    Valid Acc:  0.71875  at batch 1229.\n",
            "Valid Loss: 1.0123779773712158    Valid Acc:  0.8125  at batch 1230.\n",
            "Valid Loss: 1.0369417667388916    Valid Acc:  0.71875  at batch 1231.\n",
            "Valid Loss: 1.0288283824920654    Valid Acc:  0.84375  at batch 1232.\n",
            "Valid Loss: 0.8978561162948608    Valid Acc:  0.84375  at batch 1233.\n",
            "Valid Loss: 0.9138134121894836    Valid Acc:  0.84375  at batch 1234.\n",
            "Valid Loss: 1.2244839668273926    Valid Acc:  0.6875  at batch 1235.\n",
            "Valid Loss: 0.9499558210372925    Valid Acc:  0.875  at batch 1236.\n",
            "Valid Loss: 0.8832564353942871    Valid Acc:  0.875  at batch 1237.\n",
            "Valid Loss: 1.085622787475586    Valid Acc:  0.75  at batch 1238.\n",
            "Valid Loss: 0.9643493294715881    Valid Acc:  0.75  at batch 1239.\n",
            "Valid Loss: 1.026484489440918    Valid Acc:  0.8125  at batch 1240.\n",
            "Valid Loss: 0.9226264953613281    Valid Acc:  0.84375  at batch 1241.\n",
            "Valid Loss: 0.9511927366256714    Valid Acc:  0.78125  at batch 1242.\n",
            "Valid Loss: 1.0809955596923828    Valid Acc:  0.71875  at batch 1243.\n",
            "Valid Loss: 1.0300637483596802    Valid Acc:  0.78125  at batch 1244.\n",
            "Valid Loss: 0.8943990468978882    Valid Acc:  0.75  at batch 1245.\n",
            "Valid Loss: 0.8703596591949463    Valid Acc:  0.875  at batch 1246.\n",
            "Valid Loss: 0.8397504091262817    Valid Acc:  0.90625  at batch 1247.\n",
            "Valid Loss: 0.989870548248291    Valid Acc:  0.78125  at batch 1248.\n",
            "Valid Loss: 1.0396876335144043    Valid Acc:  0.75  at batch 1249.\n",
            "Valid Loss: 1.1843301057815552    Valid Acc:  0.65625  at batch 1250.\n",
            "Valid Loss: 1.0781158208847046    Valid Acc:  0.8125  at batch 1251.\n",
            "Valid Loss: 0.9004251956939697    Valid Acc:  0.84375  at batch 1252.\n",
            "Valid Loss: 0.8854303359985352    Valid Acc:  0.875  at batch 1253.\n",
            "Valid Loss: 0.9842055439949036    Valid Acc:  0.78125  at batch 1254.\n",
            "Valid Loss: 0.9533665180206299    Valid Acc:  0.84375  at batch 1255.\n",
            "Valid Loss: 0.8719481229782104    Valid Acc:  0.71875  at batch 1256.\n",
            "Valid Loss: 0.9929073452949524    Valid Acc:  0.78125  at batch 1257.\n",
            "Valid Loss: 1.129305124282837    Valid Acc:  0.75  at batch 1258.\n",
            "Valid Loss: 1.1764025688171387    Valid Acc:  0.84375  at batch 1259.\n",
            "Valid Loss: 0.9357025027275085    Valid Acc:  0.84375  at batch 1260.\n",
            "Valid Loss: 1.0707861185073853    Valid Acc:  0.71875  at batch 1261.\n",
            "Valid Loss: 0.9848986864089966    Valid Acc:  0.71875  at batch 1262.\n",
            "Valid Loss: 1.2841888666152954    Valid Acc:  0.6875  at batch 1263.\n",
            "Valid Loss: 1.003077745437622    Valid Acc:  0.75  at batch 1264.\n",
            "Valid Loss: 0.8796695470809937    Valid Acc:  0.875  at batch 1265.\n",
            "Valid Loss: 1.1252126693725586    Valid Acc:  0.75  at batch 1266.\n",
            "Valid Loss: 0.9239130020141602    Valid Acc:  0.84375  at batch 1267.\n",
            "Valid Loss: 0.8398336172103882    Valid Acc:  0.84375  at batch 1268.\n",
            "Valid Loss: 1.0105501413345337    Valid Acc:  0.71875  at batch 1269.\n",
            "Valid Loss: 0.8183501958847046    Valid Acc:  0.75  at batch 1270.\n",
            "Valid Loss: 1.0156737565994263    Valid Acc:  0.8125  at batch 1271.\n",
            "Valid Loss: 0.9765702486038208    Valid Acc:  0.8125  at batch 1272.\n",
            "Valid Loss: 1.0430870056152344    Valid Acc:  0.75  at batch 1273.\n",
            "Valid Loss: 0.9349899888038635    Valid Acc:  0.90625  at batch 1274.\n",
            "Valid Loss: 1.0704303979873657    Valid Acc:  0.65625  at batch 1275.\n",
            "Valid Loss: 0.962455689907074    Valid Acc:  0.84375  at batch 1276.\n",
            "Valid Loss: 1.0184047222137451    Valid Acc:  0.6875  at batch 1277.\n",
            "Valid Loss: 1.0823287963867188    Valid Acc:  0.75  at batch 1278.\n",
            "Valid Loss: 0.9914208054542542    Valid Acc:  0.84375  at batch 1279.\n",
            "Valid Loss: 0.9681514501571655    Valid Acc:  0.84375  at batch 1280.\n",
            "Valid Loss: 0.9667944312095642    Valid Acc:  0.78125  at batch 1281.\n",
            "Valid Loss: 0.9321627616882324    Valid Acc:  0.8125  at batch 1282.\n",
            "Valid Loss: 1.0689330101013184    Valid Acc:  0.75  at batch 1283.\n",
            "Valid Loss: 0.8891027569770813    Valid Acc:  0.84375  at batch 1284.\n",
            "Valid Loss: 0.9575539231300354    Valid Acc:  0.78125  at batch 1285.\n",
            "Valid Loss: 1.070499062538147    Valid Acc:  0.71875  at batch 1286.\n",
            "Valid Loss: 1.001097321510315    Valid Acc:  0.71875  at batch 1287.\n",
            "Valid Loss: 1.0547451972961426    Valid Acc:  0.78125  at batch 1288.\n",
            "Valid Loss: 1.0714248418807983    Valid Acc:  0.71875  at batch 1289.\n",
            "Valid Loss: 1.0065323114395142    Valid Acc:  0.71875  at batch 1290.\n",
            "Valid Loss: 0.8038135170936584    Valid Acc:  0.9375  at batch 1291.\n",
            "Valid Loss: 1.0439128875732422    Valid Acc:  0.78125  at batch 1292.\n",
            "Valid Loss: 0.9291110038757324    Valid Acc:  0.8125  at batch 1293.\n",
            "Valid Loss: 0.918207585811615    Valid Acc:  0.90625  at batch 1294.\n",
            "Valid Loss: 1.024631381034851    Valid Acc:  0.84375  at batch 1295.\n",
            "Valid Loss: 1.0341377258300781    Valid Acc:  0.65625  at batch 1296.\n",
            "Valid Loss: 0.9949256181716919    Valid Acc:  0.78125  at batch 1297.\n",
            "Valid Loss: 0.9317763447761536    Valid Acc:  0.8125  at batch 1298.\n",
            "Valid Loss: 1.0488471984863281    Valid Acc:  0.6875  at batch 1299.\n",
            "Valid Loss: 1.0677196979522705    Valid Acc:  0.6875  at batch 1300.\n",
            "Valid Loss: 0.8968062400817871    Valid Acc:  0.8125  at batch 1301.\n",
            "Valid Loss: 0.9977945685386658    Valid Acc:  0.78125  at batch 1302.\n",
            "Valid Loss: 0.9091454744338989    Valid Acc:  0.84375  at batch 1303.\n",
            "Valid Loss: 1.1142915487289429    Valid Acc:  0.78125  at batch 1304.\n",
            "Valid Loss: 0.899402379989624    Valid Acc:  0.8125  at batch 1305.\n",
            "Valid Loss: 0.9462354183197021    Valid Acc:  0.8125  at batch 1306.\n",
            "Valid Loss: 0.9794082045555115    Valid Acc:  0.8125  at batch 1307.\n",
            "Valid Loss: 0.9139994978904724    Valid Acc:  0.78125  at batch 1308.\n",
            "Valid Loss: 1.0038913488388062    Valid Acc:  0.75  at batch 1309.\n",
            "Valid Loss: 1.014788269996643    Valid Acc:  0.75  at batch 1310.\n",
            "Valid Loss: 0.9768310189247131    Valid Acc:  0.8125  at batch 1311.\n",
            "Valid Loss: 1.0938669443130493    Valid Acc:  0.75  at batch 1312.\n",
            "Valid Loss: 1.0415658950805664    Valid Acc:  0.6875  at batch 1313.\n",
            "Valid Loss: 0.9643786549568176    Valid Acc:  0.8125  at batch 1314.\n",
            "Valid Loss: 1.022098183631897    Valid Acc:  0.75  at batch 1315.\n",
            "Valid Loss: 1.070681095123291    Valid Acc:  0.6875  at batch 1316.\n",
            "Valid Loss: 0.9839644432067871    Valid Acc:  0.78125  at batch 1317.\n",
            "Valid Loss: 0.9128799438476562    Valid Acc:  0.90625  at batch 1318.\n",
            "Valid Loss: 1.0193126201629639    Valid Acc:  0.8125  at batch 1319.\n",
            "Valid Loss: 1.1624910831451416    Valid Acc:  0.65625  at batch 1320.\n",
            "Valid Loss: 0.9430049657821655    Valid Acc:  0.84375  at batch 1321.\n",
            "Valid Loss: 0.9909954071044922    Valid Acc:  0.78125  at batch 1322.\n",
            "Valid Loss: 1.2201828956604004    Valid Acc:  0.625  at batch 1323.\n",
            "Valid Loss: 0.8675358295440674    Valid Acc:  0.84375  at batch 1324.\n",
            "Valid Loss: 1.1415389776229858    Valid Acc:  0.71875  at batch 1325.\n",
            "Valid Loss: 1.1632136106491089    Valid Acc:  0.71875  at batch 1326.\n",
            "Valid Loss: 0.972135603427887    Valid Acc:  0.78125  at batch 1327.\n",
            "Valid Loss: 0.9639534950256348    Valid Acc:  0.75  at batch 1328.\n",
            "Valid Loss: 1.0849418640136719    Valid Acc:  0.65625  at batch 1329.\n",
            "Valid Loss: 1.0798511505126953    Valid Acc:  0.6875  at batch 1330.\n",
            "Valid Loss: 1.2012485265731812    Valid Acc:  0.6875  at batch 1331.\n",
            "Valid Loss: 0.7660062909126282    Valid Acc:  1.0  at batch 1332.\n",
            "Valid Loss: 0.8248725533485413    Valid Acc:  0.875  at batch 1333.\n",
            "Valid Loss: 1.147053599357605    Valid Acc:  0.6875  at batch 1334.\n",
            "Valid Loss: 0.9816630482673645    Valid Acc:  0.84375  at batch 1335.\n",
            "Valid Loss: 0.9791887402534485    Valid Acc:  0.71875  at batch 1336.\n",
            "Valid Loss: 1.1625802516937256    Valid Acc:  0.6875  at batch 1337.\n",
            "Valid Loss: 1.161665678024292    Valid Acc:  0.71875  at batch 1338.\n",
            "Valid Loss: 0.9396478533744812    Valid Acc:  0.875  at batch 1339.\n",
            "Valid Loss: 1.0160351991653442    Valid Acc:  0.84375  at batch 1340.\n",
            "Valid Loss: 1.0366640090942383    Valid Acc:  0.78125  at batch 1341.\n",
            "Valid Loss: 1.004089593887329    Valid Acc:  0.75  at batch 1342.\n",
            "Valid Loss: 1.0535601377487183    Valid Acc:  0.8125  at batch 1343.\n",
            "Valid Loss: 0.9314186573028564    Valid Acc:  0.90625  at batch 1344.\n",
            "Valid Loss: 1.0053023099899292    Valid Acc:  0.6875  at batch 1345.\n",
            "Valid Loss: 0.854729175567627    Valid Acc:  0.8125  at batch 1346.\n",
            "Valid Loss: 1.0821561813354492    Valid Acc:  0.78125  at batch 1347.\n",
            "Valid Loss: 1.0840649604797363    Valid Acc:  0.65625  at batch 1348.\n",
            "Valid Loss: 0.9665318727493286    Valid Acc:  0.8125  at batch 1349.\n",
            "Valid Loss: 1.0586438179016113    Valid Acc:  0.71875  at batch 1350.\n",
            "Valid Loss: 0.8441001176834106    Valid Acc:  0.78125  at batch 1351.\n",
            "Valid Loss: 1.0681172609329224    Valid Acc:  0.8125  at batch 1352.\n",
            "Valid Loss: 1.0040156841278076    Valid Acc:  0.8125  at batch 1353.\n",
            "Valid Loss: 0.9980184435844421    Valid Acc:  0.78125  at batch 1354.\n",
            "Valid Loss: 0.9899774789810181    Valid Acc:  0.75  at batch 1355.\n",
            "Valid Loss: 1.0334913730621338    Valid Acc:  0.8125  at batch 1356.\n",
            "Valid Loss: 1.0110974311828613    Valid Acc:  0.75  at batch 1357.\n",
            "Valid Loss: 0.801505982875824    Valid Acc:  0.96875  at batch 1358.\n",
            "Valid Loss: 1.0593547821044922    Valid Acc:  0.8125  at batch 1359.\n",
            "Valid Loss: 0.7918134331703186    Valid Acc:  0.96875  at batch 1360.\n",
            "Valid Loss: 0.9920455813407898    Valid Acc:  0.84375  at batch 1361.\n",
            "Valid Loss: 1.101484775543213    Valid Acc:  0.75  at batch 1362.\n",
            "Valid Loss: 0.9570111632347107    Valid Acc:  0.8125  at batch 1363.\n",
            "Valid Loss: 1.168015718460083    Valid Acc:  0.6875  at batch 1364.\n",
            "Valid Loss: 0.9060460329055786    Valid Acc:  0.71875  at batch 1365.\n",
            "Valid Loss: 1.1734567880630493    Valid Acc:  0.6875  at batch 1366.\n",
            "Valid Loss: 1.0878195762634277    Valid Acc:  0.84375  at batch 1367.\n",
            "Valid Loss: 1.142920970916748    Valid Acc:  0.84375  at batch 1368.\n",
            "Valid Loss: 1.0193243026733398    Valid Acc:  0.78125  at batch 1369.\n",
            "Valid Loss: 0.9606276154518127    Valid Acc:  0.84375  at batch 1370.\n",
            "Valid Loss: 1.1177679300308228    Valid Acc:  0.6875  at batch 1371.\n",
            "Valid Loss: 1.0025633573532104    Valid Acc:  0.8125  at batch 1372.\n",
            "Valid Loss: 0.8311144709587097    Valid Acc:  0.9375  at batch 1373.\n",
            "Valid Loss: 1.102493405342102    Valid Acc:  0.71875  at batch 1374.\n",
            "Valid Loss: 0.9703482985496521    Valid Acc:  0.71875  at batch 1375.\n",
            "Valid Loss: 0.956885039806366    Valid Acc:  0.8125  at batch 1376.\n",
            "Valid Loss: 0.9855824708938599    Valid Acc:  0.75  at batch 1377.\n",
            "Valid Loss: 0.9141392707824707    Valid Acc:  0.75  at batch 1378.\n",
            "Valid Loss: 0.9242074489593506    Valid Acc:  0.78125  at batch 1379.\n",
            "Valid Loss: 1.0602775812149048    Valid Acc:  0.6875  at batch 1380.\n",
            "Valid Loss: 0.8190218210220337    Valid Acc:  0.875  at batch 1381.\n",
            "Valid Loss: 1.0782842636108398    Valid Acc:  0.75  at batch 1382.\n",
            "Valid Loss: 0.9415315389633179    Valid Acc:  0.84375  at batch 1383.\n",
            "Valid Loss: 0.9993866086006165    Valid Acc:  0.78125  at batch 1384.\n",
            "Valid Loss: 1.1937568187713623    Valid Acc:  0.625  at batch 1385.\n",
            "Valid Loss: 0.9339631199836731    Valid Acc:  0.875  at batch 1386.\n",
            "Valid Loss: 0.8905565738677979    Valid Acc:  0.8125  at batch 1387.\n",
            "Valid Loss: 0.9213874936103821    Valid Acc:  0.875  at batch 1388.\n",
            "Valid Loss: 1.067161202430725    Valid Acc:  0.65625  at batch 1389.\n",
            "Valid Loss: 1.0282191038131714    Valid Acc:  0.8125  at batch 1390.\n",
            "Valid Loss: 1.0172119140625    Valid Acc:  0.84375  at batch 1391.\n",
            "Valid Loss: 0.9764192700386047    Valid Acc:  0.8125  at batch 1392.\n",
            "Valid Loss: 0.9942232370376587    Valid Acc:  0.75  at batch 1393.\n",
            "Valid Loss: 1.1327816247940063    Valid Acc:  0.8125  at batch 1394.\n",
            "Valid Loss: 0.9245493412017822    Valid Acc:  0.84375  at batch 1395.\n",
            "Valid Loss: 0.9199125170707703    Valid Acc:  0.90625  at batch 1396.\n",
            "Valid Loss: 0.8490610718727112    Valid Acc:  0.9375  at batch 1397.\n",
            "Valid Loss: 0.9433332681655884    Valid Acc:  0.78125  at batch 1398.\n",
            "Valid Loss: 0.8990367650985718    Valid Acc:  0.84375  at batch 1399.\n",
            "Valid Loss: 0.9113360643386841    Valid Acc:  0.875  at batch 1400.\n",
            "Valid Loss: 0.9984174966812134    Valid Acc:  0.875  at batch 1401.\n",
            "Valid Loss: 1.0396068096160889    Valid Acc:  0.71875  at batch 1402.\n",
            "Valid Loss: 1.0527687072753906    Valid Acc:  0.71875  at batch 1403.\n",
            "Valid Loss: 1.0657912492752075    Valid Acc:  0.78125  at batch 1404.\n",
            "Valid Loss: 1.0714081525802612    Valid Acc:  0.71875  at batch 1405.\n",
            "Valid Loss: 1.1883302927017212    Valid Acc:  0.71875  at batch 1406.\n",
            "Valid Loss: 1.1109422445297241    Valid Acc:  0.78125  at batch 1407.\n",
            "Valid Loss: 0.7448671460151672    Valid Acc:  0.9375  at batch 1408.\n",
            "Valid Loss: 1.1474863290786743    Valid Acc:  0.75  at batch 1409.\n",
            "Valid Loss: 1.1085940599441528    Valid Acc:  0.75  at batch 1410.\n",
            "Valid Loss: 0.9625858664512634    Valid Acc:  0.78125  at batch 1411.\n",
            "Valid Loss: 0.9090282320976257    Valid Acc:  0.9375  at batch 1412.\n",
            "Valid Loss: 0.9629207253456116    Valid Acc:  0.8125  at batch 1413.\n",
            "Valid Loss: 1.0900150537490845    Valid Acc:  0.71875  at batch 1414.\n",
            "Valid Loss: 1.1080282926559448    Valid Acc:  0.8125  at batch 1415.\n",
            "Valid Loss: 1.2198712825775146    Valid Acc:  0.71875  at batch 1416.\n",
            "Valid Loss: 0.9934848546981812    Valid Acc:  0.8125  at batch 1417.\n",
            "Valid Loss: 1.118888020515442    Valid Acc:  0.75  at batch 1418.\n",
            "Valid Loss: 1.1182371377944946    Valid Acc:  0.75  at batch 1419.\n",
            "Valid Loss: 1.0586236715316772    Valid Acc:  0.75  at batch 1420.\n",
            "Valid Loss: 1.1584837436676025    Valid Acc:  0.65625  at batch 1421.\n",
            "Valid Loss: 1.0496870279312134    Valid Acc:  0.78125  at batch 1422.\n",
            "Valid Loss: 0.9786693453788757    Valid Acc:  0.875  at batch 1423.\n",
            "Valid Loss: 0.8864136934280396    Valid Acc:  0.9375  at batch 1424.\n",
            "Valid Loss: 0.9791040420532227    Valid Acc:  0.71875  at batch 1425.\n",
            "Valid Loss: 0.9261926412582397    Valid Acc:  0.875  at batch 1426.\n",
            "Valid Loss: 0.9078032970428467    Valid Acc:  0.8125  at batch 1427.\n",
            "Valid Loss: 0.993791401386261    Valid Acc:  0.78125  at batch 1428.\n",
            "Valid Loss: 1.0627840757369995    Valid Acc:  0.6875  at batch 1429.\n",
            "Valid Loss: 0.9947301149368286    Valid Acc:  0.8125  at batch 1430.\n",
            "Valid Loss: 0.8467442989349365    Valid Acc:  0.875  at batch 1431.\n",
            "Valid Loss: 0.9036107063293457    Valid Acc:  0.8125  at batch 1432.\n",
            "Valid Loss: 1.1120210886001587    Valid Acc:  0.6875  at batch 1433.\n",
            "Valid Loss: 1.0627861022949219    Valid Acc:  0.71875  at batch 1434.\n",
            "Valid Loss: 0.9125545024871826    Valid Acc:  0.875  at batch 1435.\n",
            "Valid Loss: 0.8919991254806519    Valid Acc:  0.8125  at batch 1436.\n",
            "Valid Loss: 0.956527829170227    Valid Acc:  0.8125  at batch 1437.\n",
            "Valid Loss: 1.087700605392456    Valid Acc:  0.71875  at batch 1438.\n",
            "Valid Loss: 0.9456384181976318    Valid Acc:  0.71875  at batch 1439.\n",
            "Valid Loss: 1.2484487295150757    Valid Acc:  0.65625  at batch 1440.\n",
            "Valid Loss: 1.008847713470459    Valid Acc:  0.8125  at batch 1441.\n",
            "Valid Loss: 0.9128933548927307    Valid Acc:  0.84375  at batch 1442.\n",
            "Valid Loss: 0.8954863548278809    Valid Acc:  0.78125  at batch 1443.\n",
            "Valid Loss: 0.9127198457717896    Valid Acc:  0.75  at batch 1444.\n",
            "Valid Loss: 1.149062156677246    Valid Acc:  0.8125  at batch 1445.\n",
            "Valid Loss: 0.8843585252761841    Valid Acc:  0.8125  at batch 1446.\n",
            "Valid Loss: 0.897346556186676    Valid Acc:  0.875  at batch 1447.\n",
            "Valid Loss: 0.9240532517433167    Valid Acc:  0.90625  at batch 1448.\n",
            "Valid Loss: 1.0135363340377808    Valid Acc:  0.84375  at batch 1449.\n",
            "Valid Loss: 1.2827558517456055    Valid Acc:  0.65625  at batch 1450.\n",
            "Valid Loss: 1.132904291152954    Valid Acc:  0.6875  at batch 1451.\n",
            "Valid Loss: 0.9038106203079224    Valid Acc:  0.875  at batch 1452.\n",
            "Valid Loss: 0.8604705333709717    Valid Acc:  0.78125  at batch 1453.\n",
            "Valid Loss: 1.028180718421936    Valid Acc:  0.71875  at batch 1454.\n",
            "Valid Loss: 0.9033094048500061    Valid Acc:  0.78125  at batch 1455.\n",
            "Valid Loss: 1.0069139003753662    Valid Acc:  0.8125  at batch 1456.\n",
            "Valid Loss: 1.0631662607192993    Valid Acc:  0.71875  at batch 1457.\n",
            "Valid Loss: 0.9624653458595276    Valid Acc:  0.78125  at batch 1458.\n",
            "Valid Loss: 0.991754412651062    Valid Acc:  0.75  at batch 1459.\n",
            "Valid Loss: 0.9730148911476135    Valid Acc:  0.6875  at batch 1460.\n",
            "Valid Loss: 1.1416816711425781    Valid Acc:  0.78125  at batch 1461.\n",
            "Valid Loss: 0.9523138403892517    Valid Acc:  0.84375  at batch 1462.\n",
            "Valid Loss: 0.9429928660392761    Valid Acc:  0.9375  at batch 1463.\n",
            "Valid Loss: 0.8768444657325745    Valid Acc:  0.875  at batch 1464.\n",
            "Valid Loss: 1.2031476497650146    Valid Acc:  0.59375  at batch 1465.\n",
            "Valid Loss: 0.9847219586372375    Valid Acc:  0.8125  at batch 1466.\n",
            "Valid Loss: 0.8450722694396973    Valid Acc:  0.875  at batch 1467.\n",
            "Valid Loss: 1.0244814157485962    Valid Acc:  0.6875  at batch 1468.\n",
            "Valid Loss: 0.9155269265174866    Valid Acc:  0.8125  at batch 1469.\n",
            "Valid Loss: 0.8814982175827026    Valid Acc:  0.90625  at batch 1470.\n",
            "Valid Loss: 1.145015835762024    Valid Acc:  0.71875  at batch 1471.\n",
            "Valid Loss: 1.173680067062378    Valid Acc:  0.6875  at batch 1472.\n",
            "Valid Loss: 0.9391518235206604    Valid Acc:  0.78125  at batch 1473.\n",
            "Valid Loss: 1.01596200466156    Valid Acc:  0.78125  at batch 1474.\n",
            "Valid Loss: 1.1893792152404785    Valid Acc:  0.6875  at batch 1475.\n",
            "Valid Loss: 0.9275753498077393    Valid Acc:  0.84375  at batch 1476.\n",
            "Valid Loss: 0.8865943551063538    Valid Acc:  0.78125  at batch 1477.\n",
            "Valid Loss: 0.9959328770637512    Valid Acc:  0.84375  at batch 1478.\n",
            "Valid Loss: 0.8598998188972473    Valid Acc:  0.875  at batch 1479.\n",
            "Valid Loss: 0.9409043192863464    Valid Acc:  0.84375  at batch 1480.\n",
            "Valid Loss: 0.9736541509628296    Valid Acc:  0.78125  at batch 1481.\n",
            "Valid Loss: 0.9667097926139832    Valid Acc:  0.75  at batch 1482.\n",
            "Valid Loss: 0.8895438313484192    Valid Acc:  0.84375  at batch 1483.\n",
            "Valid Loss: 1.024226188659668    Valid Acc:  0.84375  at batch 1484.\n",
            "Valid Loss: 0.9851186871528625    Valid Acc:  0.71875  at batch 1485.\n",
            "Valid Loss: 0.9246797561645508    Valid Acc:  0.84375  at batch 1486.\n",
            "Valid Loss: 1.0596387386322021    Valid Acc:  0.75  at batch 1487.\n",
            "Valid Loss: 0.9233471155166626    Valid Acc:  0.84375  at batch 1488.\n",
            "Valid Loss: 1.1172726154327393    Valid Acc:  0.71875  at batch 1489.\n",
            "Valid Loss: 1.125252366065979    Valid Acc:  0.71875  at batch 1490.\n",
            "Valid Loss: 0.9232437014579773    Valid Acc:  0.71875  at batch 1491.\n",
            "Valid Loss: 1.0822622776031494    Valid Acc:  0.875  at batch 1492.\n",
            "Valid Loss: 1.094069480895996    Valid Acc:  0.71875  at batch 1493.\n",
            "Valid Loss: 1.0470061302185059    Valid Acc:  0.84375  at batch 1494.\n",
            "Valid Loss: 0.9227811694145203    Valid Acc:  0.71875  at batch 1495.\n",
            "Valid Loss: 1.073409914970398    Valid Acc:  0.75  at batch 1496.\n",
            "Valid Loss: 1.1276476383209229    Valid Acc:  0.75  at batch 1497.\n",
            "Valid Loss: 0.9153169393539429    Valid Acc:  0.84375  at batch 1498.\n",
            "Valid Loss: 1.1990066766738892    Valid Acc:  0.71875  at batch 1499.\n",
            "Valid Loss: 0.9527271389961243    Valid Acc:  0.8125  at batch 1500.\n",
            "Valid Loss: 0.910214900970459    Valid Acc:  0.8125  at batch 1501.\n",
            "Valid Loss: 1.0445886850357056    Valid Acc:  0.71875  at batch 1502.\n",
            "Valid Loss: 0.9245100617408752    Valid Acc:  0.90625  at batch 1503.\n",
            "Valid Loss: 1.1064214706420898    Valid Acc:  0.84375  at batch 1504.\n",
            "Valid Loss: 1.0698596239089966    Valid Acc:  0.8125  at batch 1505.\n",
            "Valid Loss: 0.794216513633728    Valid Acc:  0.875  at batch 1506.\n",
            "Valid Loss: 1.1185059547424316    Valid Acc:  0.6875  at batch 1507.\n",
            "Valid Loss: 0.9686346054077148    Valid Acc:  0.78125  at batch 1508.\n",
            "Valid Loss: 1.0320295095443726    Valid Acc:  0.71875  at batch 1509.\n",
            "Valid Loss: 0.9864917397499084    Valid Acc:  0.8125  at batch 1510.\n",
            "Valid Loss: 1.1078627109527588    Valid Acc:  0.6875  at batch 1511.\n",
            "Valid Loss: 1.114786148071289    Valid Acc:  0.8125  at batch 1512.\n",
            "Valid Loss: 1.0942585468292236    Valid Acc:  0.71875  at batch 1513.\n",
            "Valid Loss: 0.8141158223152161    Valid Acc:  0.875  at batch 1514.\n",
            "Valid Loss: 0.8819279074668884    Valid Acc:  0.8125  at batch 1515.\n",
            "Valid Loss: 0.9567009806632996    Valid Acc:  0.84375  at batch 1516.\n",
            "Valid Loss: 0.9614304304122925    Valid Acc:  0.84375  at batch 1517.\n",
            "Valid Loss: 0.8396578431129456    Valid Acc:  0.78125  at batch 1518.\n",
            "Valid Loss: 1.1081781387329102    Valid Acc:  0.6875  at batch 1519.\n",
            "Valid Loss: 0.9231345653533936    Valid Acc:  0.8125  at batch 1520.\n",
            "Valid Loss: 1.0709861516952515    Valid Acc:  0.71875  at batch 1521.\n",
            "Valid Loss: 0.9205177426338196    Valid Acc:  0.875  at batch 1522.\n",
            "Valid Loss: 0.9961938261985779    Valid Acc:  0.875  at batch 1523.\n",
            "Valid Loss: 1.1552960872650146    Valid Acc:  0.75  at batch 1524.\n",
            "Valid Loss: 1.1266587972640991    Valid Acc:  0.78125  at batch 1525.\n",
            "Valid Loss: 0.9923776388168335    Valid Acc:  0.8125  at batch 1526.\n",
            "Valid Loss: 0.8778200149536133    Valid Acc:  0.90625  at batch 1527.\n",
            "Valid Loss: 1.128559947013855    Valid Acc:  0.71875  at batch 1528.\n",
            "Valid Loss: 1.0152701139450073    Valid Acc:  0.8125  at batch 1529.\n",
            "Valid Loss: 0.9592854976654053    Valid Acc:  0.71875  at batch 1530.\n",
            "Valid Loss: 1.0079147815704346    Valid Acc:  0.75  at batch 1531.\n",
            "Valid Loss: 0.9558433294296265    Valid Acc:  0.875  at batch 1532.\n",
            "Valid Loss: 0.95970618724823    Valid Acc:  0.8125  at batch 1533.\n",
            "Valid Loss: 0.9929307103157043    Valid Acc:  0.8125  at batch 1534.\n",
            "Valid Loss: 0.978073000907898    Valid Acc:  0.8125  at batch 1535.\n",
            "Valid Loss: 0.9389723539352417    Valid Acc:  0.8125  at batch 1536.\n",
            "Valid Loss: 0.8680137991905212    Valid Acc:  0.84375  at batch 1537.\n",
            "Valid Loss: 1.0696251392364502    Valid Acc:  0.65625  at batch 1538.\n",
            "Valid Loss: 0.9180512428283691    Valid Acc:  0.8125  at batch 1539.\n",
            "Valid Loss: 1.0091463327407837    Valid Acc:  0.8125  at batch 1540.\n",
            "Valid Loss: 0.9122936725616455    Valid Acc:  0.78125  at batch 1541.\n",
            "Valid Loss: 1.1340309381484985    Valid Acc:  0.71875  at batch 1542.\n",
            "Valid Loss: 0.9359341263771057    Valid Acc:  0.78125  at batch 1543.\n",
            "Valid Loss: 1.2006897926330566    Valid Acc:  0.71875  at batch 1544.\n",
            "Valid Loss: 0.7762468457221985    Valid Acc:  0.90625  at batch 1545.\n",
            "Valid Loss: 1.0012332201004028    Valid Acc:  0.8125  at batch 1546.\n",
            "Valid Loss: 0.9904793500900269    Valid Acc:  0.84375  at batch 1547.\n",
            "Valid Loss: 0.9622336626052856    Valid Acc:  0.78125  at batch 1548.\n",
            "Valid Loss: 0.8469005823135376    Valid Acc:  0.875  at batch 1549.\n",
            "Valid Loss: 1.0389418601989746    Valid Acc:  0.75  at batch 1550.\n",
            "Valid Loss: 1.117070198059082    Valid Acc:  0.71875  at batch 1551.\n",
            "Valid Loss: 0.9448240995407104    Valid Acc:  0.75  at batch 1552.\n",
            "Valid Loss: 0.9609411954879761    Valid Acc:  0.75  at batch 1553.\n",
            "Valid Loss: 0.8252880573272705    Valid Acc:  0.8125  at batch 1554.\n",
            "Valid Loss: 1.0045379400253296    Valid Acc:  0.75  at batch 1555.\n",
            "Valid Loss: 0.9215279817581177    Valid Acc:  0.8125  at batch 1556.\n",
            "Valid Loss: 0.9785106778144836    Valid Acc:  0.75  at batch 1557.\n",
            "Valid Loss: 0.9673004150390625    Valid Acc:  0.78125  at batch 1558.\n",
            "Valid Loss: 0.9963169693946838    Valid Acc:  0.71875  at batch 1559.\n",
            "Valid Loss: 0.9194110631942749    Valid Acc:  0.875  at batch 1560.\n",
            "Valid Loss: 0.9979953765869141    Valid Acc:  0.84375  at batch 1561.\n",
            "Valid Loss: 0.848617672920227    Valid Acc:  0.75  at batch 1562.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgPwLDh62Nz1"
      },
      "source": [
        "def plots(history):\r\n",
        "  loss, acc = zip(*history)\r\n",
        "  fig, (loss_plot, acc_plot) = plt.subplots(1, 2, figsize=(20, 10))\r\n",
        "  loss_plot.plot(loss, '-x')\r\n",
        "  loss_plot.set(xlabel=\"Batch\", ylabel=\"Loss\")\r\n",
        "  loss_plot.set_title(\"Loss\")\r\n",
        "\r\n",
        "  acc_plot.plot(acc, '-x')\r\n",
        "  acc_plot.set(xlabel=\"Batch\", ylabel=\"Accuracy\")\r\n",
        "  acc_plot.set_title(\"Accuracy\") \r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "q7evr2Hv2QUy",
        "outputId": "b7d6bcd9-b450-4134-9719-3cbf165ee0c3"
      },
      "source": [
        "plots(train_history)\r\n",
        "plots(val_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAJcCAYAAABwj4S5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde3gUZZo//O/TnYSQQAKkQ8gBCOlwUDAQjk4jgjrrYZBRmXWdiaAgchp53d/r7P5m1d2dPSlz2HF2Z3GAYRQUYWbnnVFUUGBGAZEeDgmBCBIgHQ45k05CCAkhSXe9f1RXpaq7Oukcu5N8P9fFZbr6qaqnqrrbrrvv536EJEkgIiIiIiIiIiIyYgp2B4iIiIiIiIiIKHQxeERERERERERERH4xeERERERERERERH4xeERERERERERERH4xeERERERERERERH4xeERERERERERERH4xeERERERERERERH4xeEREvU4IcVkI8c1g94OIiIiorxJCHBRC1AghBgW7L0TU/zF4RERERERE1IcIIVIBzAMgAfh2L+43rLf2RUShhcEjIgoJQohBQoj/EkKUev79l/JLmhDCIoTYLYS4LoSoFkIcFkKYPM/9UAhRIoSoE0KcF0I8ENwjISIiIupxzwA4CmAbgGeVhUKI0UKI94UQlUKIKiHEBs1zK4UQ5zzfmb4WQkz3LJeEEOmadtuEEP/h+XuBEKLY832rHMBWIcRwz/eySk/m024hRIpm/RFCiK2e73M1QohdnuVnhBCLNO3ChRBOIURmj50lIuo2DB4RUah4FcDdAKYBmApgNoB/9Dz3AwDFAOIBJAB4BYAkhJgIYB2AWZIkDQXwEIDLvdttIiIiol73DIAdnn8PCSEShBBmALsBXAGQCiAZwO8AQAjxJIB/8awXAzlbqSrAfY0CMALAWACrIN9DbvU8HgPgFoANmvbbAUQBmAxgJIBfeJa/C2CJpt23AJRJkpQbYD+IKIiYdkhEoeJpAP+PJEnXAEAI8a8ANgP4JwDNABIBjJUkqQDAYU8bF4BBAO4UQlRKknQ5GB0nIiIi6i1CiHsgB25+L0mSUwjhAJAFORMpCcDfS5LU4mn+pee/zwP4qSRJJzyPCzqwSzeAH0mSdNvz+BaAP2r68xqAA56/EwE8AiBOkqQaT5NDnv++B+CfhBAxkiTdALAUcqCJiPoAZh4RUahIgvxLmeKKZxkA/Azyl5z9QohCIcQ/AIAnkPR/IP+Sdk0I8TshRBKIiIiI+q9nAeyXJMnpebzTs2w0gCuawJHWaACOTu6vUpKkRuWBECJKCLFZCHFFCHEDwBcAhnkyn0YDqNYEjlSSJJUCOALgO0KIYZCDTDs62Sci6mUMHhFRqCiF/CuaYoxnGSRJqpMk6QeSJKVBTrN+SaltJEnSTkmSlF/gJAA/6d1uExEREfUOIcRgAH8DYL4QotxTh+j/hTzkvwLAGD9FrYsAWP1stgHyMDPFKK/nJa/HPwAwEcAcSZJiANyrdM+znxGe4JCRdyAPXXsSwF8kSSrx046IQgyDR0QULOFCiEjlH4DfAvhHIUS8EMIC4J8hpzdDCPGoECJdCCEA1AJwAXALISYKIe73FNZuhJxG7Q7O4RARERH1uMchfw+6E3KdyGkA7oA8pP9xAGUAfiyEiPZ8x5rrWe83AP5OCDFDyNKFEMqPdqcAZAkhzEKIhwHMb6cPQyF/57ouhBgB4EfKE5IklQH4FMCvPIW1w4UQ92rW3QVgOoC/hVwDiYj6CAaPiChYPoH8xUP5FwkgG0AegK8AnATwH5624wH8GcBNAH8B8CtJkg5Arnf0YwBOAOWQizK+3HuHQERERNSrngWwVZKkq5IklSv/IBes/h6ARQDSAVyFPNnIUwAgSdL/B+A1yEPc6iAHcUZ4tvm3nvWuQ65BuaudPvwXgMGQv38dBbDX6/mlkOtV5gO4BrnEADz9UOoljQPwfgePnYiCSEiSdxYiERERERERUfcTQvwzgAmSJC1ptzERhQzOtkZEREREREQ9zjPMbQXk7CQi6kM4bI2IiIiIiIh6lBBiJeSC2p9KkvRFsPtDRB3DYWtEREREREREROQXM4+IiIiIiIiIiMivPlfzyGKxSKmpqcHuBhEREfWQnJwcpyRJ8cHuB+nxOxgREVH/1tZ3sD4XPEpNTUV2dnawu0FEREQ9RAhxJdh9IF/8DkZERNS/tfUdjMPWiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaPiIiIiIiIiIjILwaP+rlNhxywO5y6ZXaHE5sOOYLUIyIiIupJQoi3hRDXhBBn/DwvhBC/FEIUCCHyhBDTe7uPRERE/VF/vv9m8Kify0iJxbqdueoL2O5wYt3OXGSkxAa5Z0RERNRDtgF4uI3nHwEw3vNvFYCNvdAnIiKifq8/33+HBbsD1LNsVgvWL74LS986jiVzxuDjvDJsyMqEzWoJdteIiIioB0iS9IUQIrWNJo8BeFeSJAnAUSHEMCFEoiRJZb3SQSIion7KZrXgPx6fjKVvHccz3xiLD0+V9pv7b2YeDQAVNxrhckt45y9XsGTOmH7xwiUiIqJOSwZQpHlc7FnmQwixSgiRLYTIrqys7JXOERER9WWXnA1wuSVsPXK5X91/M3g0AFyqrAcA3JUcg/eOXfUZg0lERERkRJKkX0uSNFOSpJnx8fHB7g4REVHIu1rVAACYMXZ4v7r/ZvCon7M7nPjfbPnHxYyUYdiQlakbg0lEREQDTgmA0ZrHKZ5lRERE1AV2hxMfnS4FAMweN6Jf3X8zeNTP5RXX4qlZrd8PbVYLNmRlIq+4Noi9IiIioiD6CMAznlnX7gZQy3pHREREXZdXXItvT01SH/en+28WzO7n1sy3YvvRK7plNqul34y7JCIiIj0hxG8BLABgEUIUA/gRgHAAkCRpE4BPAHwLQAGABgDLg9NTIiKi/mXNfCvePFAAAJAkeVl/uf9m8GgAkYLdASIiIupxkiR9r53nJQAv9FJ3iIiIBhQhgt2DnsFha91o0yGHz1hGu8OJTYccQeqRLBReu6F6bjqrvx0PERERERERkT8MHnWjjJRYXTEsu8OJdTtzkZESG+SeyaQAU496IjBidG5WbMuG2esV2FcCMKF+rYmIiIiIiIi6C4etdSOlGFbWlmOYnBSDstpGbMjKDPr4xo6mzSmBEaXvSmBkQ1YmNh1yICMlVndMdocTecW1WDPf6nebyrlZ9W4OZo4djrySWrz04HhsPFiIyUmxPvsJdcrxrNmeg8emJWHPV+Uhca2JiIiIiIiIuhuDR91MCR6cLb2BF+9P75PBBCUwsva9k4gIE2hxSXjz6enqsfgLLAWy3Zu3W3DwQiVevD8dK+dZMTkpFi/sOIm/npGCP54s6VMBGJvVghuNLdh+9GqfvdZERERERERE7eGwtW6mHe713rGrPsO/givwktk2qwVx0RGorGvCNzTV4W1WC974m6l4btsJ/OTTfF0gqT3ac7Hl8CVsOeyAzWpBQ5MLWw5fwvwJ8W1OYagdTqf8rR3m1ttD3kL7WhMRERERERF1DwaPusC7NpDd4cTq7Tnq4w1ZmVixLRtbDusDGj0R5GirTpHoRMlsu8OJopoGAMAXFyp12z5fXofGZjc2HnJgyZwxauBIe1ztnZuXHhyP1/fk49UP8nC7xQ0A2JVb4lMDSUsZTvfy+3kwm4DV23Ow6t1s3JkYg+e2Hceyt0/oag4ZnefuquekZFwpNmRl6mogEREREREREfUXDB51gXfR5I9Pl+qet1kteOnB8Xhj/8UeL6wcSAHnQAtmt647DACwcl6abtuOypsAgDCTwFb7ZTUDSLu/9s7NynlWZM0ZjR3HitRlryychI0HC/0GYJThdB+dKsXP911Ai8uNm7ddWL09G5/nVyLM3Bok8+6PEjTS9svucOLl9/M6dT3yimt1Q/WUvrWVOUVERERERETUF7HmUYCMCkUDwEOTE5C15RhShg9GQ5MLm5fOQNaWY+rzSl2f7793EoumJmHPV2V+h3l1thg10Bq8+P6Ok8gcPQynPcENm9WCK1VXO3R8SmDkP/ddAABMShyqC4x8fLoMALDwrkT86VwFsrYcQ3SEGVuenakb3rYhKxPPv5MNsxAIDzP5nJvRI6J1fVDOVV5xrd9hcDarBfVNLgBAhCdN6VazG09kJuPJmSlYtyMXtvQ42B1VuvOsBI1+vPgu/Pd3p2HVuzloaGpBVEQYfv3MjA7XKzK6HjbN8D4iIiIiIiIauKQOlI3pC5h51A7vjJXP8yvweX6FmrGyaGoSAKC45pZuCJeWzWrB9VvN2H70it82QNens7dZLYgKN+PA+UosykhU96Pk47SVeaTdtxIY+arkurq+sq2PT5fi255jTho+GAkxgwAACTGRPsel1DOqu91ieNze2T52hxM2q6XNQJk2K6nJ5Vb/PnShEgBwR9JQ7M4rw5xxI3T7s1kt+J/vZmLV9hz8x+6vcavZBbcE3JkY0+mAj9E16O26S0RERERERBQ6OlM2pi9g8KgdSlAFkOvaPLctG89ty8buvDKfGcb8FU0OtLCykq2TteUYvvnGIazbmatOZ3/4YiVuNDa3OezN7nCi/EYjAGDXqVJ1P8Lz2m0r8qnse/X2HDz0X19g3c5c3JUsD1vbe7ZcDaDtO1uhrrMnrxSXnHJdpPIbjYa1hPwdt3fNIADt1gwyWkexdkEaVm/PwfFL1QDkYJL3tu62xgEAzlfcVN/OX5XUdrpOkXd9Je21UYJI3VVjiYiIiIiIiChYGDxqhxJUeWHHSew7U64uz0iOxdnSWl0wY+2CNKzYlq1bv6OFlZUsmIJrN7FkzhisnGf1BK1OIONf9vud3UzZz8ihkQCAf3r0jg4XcLZZLai/3YLz5XX47qzRGBEdAQBIjYvWBdA+yC0BAFytvoXM0XKA6buzRhvWXPI+boV3zSClTVs1g4zWUfzyswK43BLGxUUBAP56RopaXFvp06sffKW2d3vSsGaOHY7lW0/g5ffzfLbZXuBHew2e23YCq7fnqP1TgkiB1KIiIiIiIiIiCmUMHgXAZrUgKiIM7/zlirosr6QWP9t3AavuHacu23iwEC89OF63bkcLK/ubzr7ZJQc7EmMjfYpP2x1O/PqLQmzIysTgCDMAYPqY4T77MRq2pg2Q2B1OuD1tth65jOr62wCASaPkmkdr3zuJf/3orDpcbELCELg92UxCCHV/docTP9mbj4cmJ/gcd1esmW/1O8Ts0YxEPDYtCbb0eABA6fVbukDOlsMO7M5rPW+TE2MAAH8prILJT1ZhRwI/jc1uNLe48cvPLsrZW5MT1BpIyrlbt/Ok3+AfERERERERUahi8CgAdocTJddv6ZZtXjoDAPDjT8+ryzZkZWJykj6wYBTw8FfXR6lxpFCms1+x7bi67GzpDXyoGZKmBDRW3ZsGm9UCSRMhUvbT1phLJUCy5bAD63bmqoGUFx9Ix1clcuBJCAGb1YJws8D5ipvqulerG9Qt7zh6BZJb3t7q7TkorKxX60Fp++O9X62OZuQIzWE9elcSfrRoMspq5euUNGwwbFYL1i/OwIasTLyx/yIyxw5X239VegMAIAF4a9ksrF+c4bN9bRHyf9z1VbuBn9stbhwtrMbtZpfu2G1WC2pvNWN3XlmbNa+IiIiIiIiIQhGDRx5KBo53Js431n+G5VtP+LT/yd58PDkj2We5v5o8gcgrrtVlLq2cZ8X9k+LxWX6luuzpOaNhNgms3p6DNdtzdFku7TGqeKQESH669zyGDgpTM4+Wzx2HKUn6mj7V9U26df/qjlE4V1YHAGhscePpt45h6W/k2dQ2L53hlfWk37vNasGG7+kzkdYuSPOZaa6t2kDakNizW49j0j/txZ/PXQMAjLNE69ZPHBaJwxd8h/DNGCMHlPztx2a1oLahGe8dvaoL/NgdTp+hbsoRmoQ+WBdozSsiIiIiIiKiUMTgkceVqnqs3p4Ds6l1mNPq7TloanHhdovbp319Yws+8kxZr1i29QTWLkhTHytFlI3q6RhZM9+KlfNaM5Lkej3ViDC3BiM+PVOBFx9IR0xkGPaeLcftZpe6L63cqzWtARHP6tfqGnVtlOCKMizuSnWDmnkkBNSaR/nlN7BuZy4ivKZ9kyBh4qghumUuCVhuS0Veca1uljhJArYc1gdoSmv1/dl4sLDTtYHMno4PGxwmr19QhXU7c2E2Aau356CousFwvZNXa7B6e47f/dgdTjUotPXIZXx0urTNwt0AMCUlFs+8dRxbDjt82j40OaHDtaiIiIiIiKjv2XTIoavBCrT+CB3IBDr9ZfKdjhxHdxyzv20s23q8V8/n6aLrusdbDjuwbOtxP61DH4NHGi0uN37xp4sYERWO1/bko6GpBTdvu/DIlASftgWV9Wj0BG4UklvCT/e2DmNbsS0bz209gQ9PlXqvrmP04gaA3x4vwsNTEtDibs3aWTE3Fes/yUeZJ/BiMgmkxUdjxbZsXXDm33afg9mkz6g5WljtE5y5UlWvW0/Z1ZYvCtVll5312JCVichws/4cXLupZh7pjueLQhRV12PjwdZtfPvNL/Hannxdu2OFVbrHaxekYc17OZj8z3vx/R0nfYaIeZ8nocnwUeowVdU3AwD+dK4CwwaH4439FwAA900c6dNPAGhxS3C5jWehe/n9PKzenqPbx4u/zcWyt49jQ1am4VA3APiquBZPzUrBG/sv4uPTpbqA4qKpSe0WBiciIiIior4vIyUWu/PKsHp7jppYsHp7DnbnlQX0I3l/mXynI8fRHcfsbxtz0+N69XweLaxW77W3HHbg9T35mJse1yP76g1hwe5AqFg0NQm788pQ19iCgsp6AIDLDXx7aiKmpsTi0zMVPusoRazVx25JN5Tqlie4pM0c0rI7nMgrrlVf3Eb1dHbllmJoZBhuNLYAAP7n8wJoYx3P3zMOGw8WYvH0JF1wJmvOGGw8KBfRLq6R6wDNGTcCq97NwcNTEvB5fiU2ZGXibGktXtesZxJyAMmsqSL9yJRE2KwWxAwOx/Vbzerywsp6w8CLWQA7jhXhvonxOHBeHnJ3puQGpiTH4EzJDfXYd+fpM7c2HixETGQ4imtu4cF0i8+58K6TpB0KFxMZjlpN3wCg0FmPMJPASw+mo6haX7NKMXHUEEwfMxx5xbXtDv0LMwncBpDoqafkz+0WN1It0djyzEw88/YxXUFum9XCrCMiIiIiogHAZrVg89IZWPVuNpZtPQEBICLMhM1LZwRUdkQpMbJuRy4evisBe89U9MnJd9Tj2JmLhyYnYN9Z/8eh1p197ySenJmCP54s6fAxayctenrOGPzuRJG6jclJsfj+jpP4TmYyPjhV2qPn8+60EXh9Tz7+kFOCC+V1eGXhJN1Io76GmUceNqsFLz6Q7rP80IVK7Dx+NeDtmAzOqDbIpI1yrtiWDbOp9cW94p1szP/pAd26ryychJu3W9THqZYon+0/NDkB3jGcd/9yWa2FpMQuRg6NxM3bLfhDTolav8flBp6aNVpdT9mOyy3hclW9bptREfrMI5fkxpgRvv156cEJSI+PVgNHirOewBEg13d6NCNR9/zaBWlqRtURT2Rey2a14D+fbM320R5zRJjxS3lwhBk/338B72pmytO6UtWARVOTDAuYr1+coRZGB4D6JjkYWHGjUf3lwMiztrF4bU8+Xvvka7glQDvqsa/+WkBERERERB1ns1owcmgkmlrcuN3ixnJbaocDIRNHDcXOY0X4qztH9rnAkcJmtWDGmOH47fEi2KxxbR6HzWrB9VvN2HL4UqcnHJo+ZjhqbzXjVwcdum3YrBZcb2jGW0cu99hkRsoAmakpwyABOF9ehxljh/fpwBHA4JHK7nDil58V+CwfERWOC5oZxtrj8i2PhFcWTlL/XrczF//4gTxz10sPjlfr/OQV1+JWkwtXvGrzuNzAiKgI9bF3Bs2mQ4UwCfhk8dxqcvnMdqatebTVfhl2h1NNpVQogaYV96ThlidY8qevy7HpkEOdyay1rcCQQb7Jaz/ffwFFNb6ZPllzWoNUZpMchNHaeLAQc8aNAADERUUY1gZq0kRitBk9TQZ1qQA5c6yx2fg5AHh48iif/WiHx2k/TKIj5LdLY7Mbz759XDekTSsxdjAAqEP6lPUAtDtjGxERERER9R92hxOFztYf5ZX7sI6sn32lGgCwJ6+8z45isDucOFwgJxd8du5am8fRHRMOKeuEm4RuG705mdHp4taaRzlXanxqAPc1DB55fHy6FC0GkZ+CynqkDI/s0rYvaz4s7kyMwXvHriJl2GCsnGdV0+kOX6g0XDcjJRaDNLWGnp4zRve8EMDvTxT79N0tAWdLaz1t5CjLsUvV6vPNLW5kbTmGZ98+jhZ367pKMs/RQidGxgwCAPxvdjF25Zag9lZrBpTcVsKZ0hvw1tjsNiwy/v7JEvXv1/bk47imPwCwISsTpdfloFPmmOGGtYG0NZa0mUdG1w6Qz01bUoZHqftRgkbaMbLaN3hjS+sOm10S7ko2zh56Y/953eOIsNbrN39CPAD/s7sREREREVH/oNQ48qbUQApk/XU7czE1ZRgAYOW8tD45+Y5yHA9MkuvQPpGZ5Pc4vCccUoa7dTTg9oPfnwYAhIeZ1G1sOezo8rY74mhh6/3u/314Il7fk9+nA0gMHmmEmX1PhwBQXNPo27gDdhwrUv9WXpgXrtVhy2EHvrzoRO2tZhxxVBmum1dcq86oBgDWkfrZzWaNHQ4J8MmuuTttBN7YfxFbDjtwIF+evl7J6gGARk9wp9klocXlW7do1fYcnCpqDdzkl/sWxjYLE4yrORm75dXHRq8A06p3s3G5Ss68uv+OkWo9KMXL7+dh40HjN1u4wbUD5DpUbZEgwWa1YM18q66m0kOTE5C15ZiujtRYzRC92Mhww3MCAE1e57OmobUW0ydflbU5uxsREREREfUPSqmOSE2Jjc1LZ+DRjMSAJtDJK67FhqxMdRbsiaOG9MnJd5TjSBomj9BItUT7PQ6lrUIp8dKRY84rrsXP/2aqzzaOFFR1edsdcXda6/33Mts4vLJwEo4UGN/39wUsmI3WLJDNS2cga8sx3XNthx7aZxZyPR4lcKLEMmaMGY7X9+RjVupwAEB0hFmtqaOVkRKL6oYm9fEfc/SZNNPHDsexS9U+/Zw+ZjgeuGMk3th/Ec/aUgEA8UMGGfbRrSk8rRTM9i4GbmRM3GA0tbhx1U8x6o66eduFMFNrjSDvQuLHL1Wrs6pp+wrAp1h2oE5eqVEDesoH1ertOYj2qu8EAFeqWzPIahub8cCkkfjME5jT0h4DIAcglbPprzYTERERERH1L0pd1Y80s2/brL4TA7W3/rYjlzu1fqhQjuOA5t7J33EY1aLt6DGvmW9V6wYryQ7+ttGT5zMjZZgu+2jlPGufrnvE4BGAK1X1PjWDuotbAh7PTMZvjxfplh9xVCEm0ozjl2sAAEMjww2DRwAwIjoCzptyAMk7KlpccwtGcZ5NhxwYFGbCW8tmoey6nDlVWXfbbx+N/m5PwbV63JE4NPAVAqANuiiR4JXvZuOu5Fhcq7uNcLNQA1vavnY2yDcyJhLLtp6ASQD/893pAOR6UXWNLT5tvUfGGQWO5L5ow0VA3JDW67fcloq7rXEBze5GRERERETUqiPjPkKP1NXMjM7ss/d32W8xDQJQC0v/Iaft+jidIQFwXDMuuH2j0YW7UmIAAOU3jIfGLfnNMVTfbM08sqXrAw67NFFsLaPsoWOXqw3bdtacccPhuFbffsMOiPIUl/79idZgW/1tF44WVmO5LRWr5qV16/4+PFUKt1tCY7MbK7dnY8W2E7pMrLYkxBhncoWZ/H+obzl8CYBxRJ2IiIiIiMi//hEKEX08CBao/naUDB5BznDZvGRGQEO1OkPJLjLyVbFvwWkttwRoE17sBR0r5rXkN8fwX59dAABEhfsOxeqK2lvNMHXzB5hSaPuIowqT/vFTLHv7uPrcVvtl5BZd97dqpw0Z1HpebjW7A86+qrhhnMn11zOSdY+rNMG/xdOTsGJbdp8rckdERERERMEh+kkUon+EvgYuBo88PjptnMETajryhnNLEtwSUOSpSTQovHsvd375Tdzu5oBbmiVa/buxxa0rPt3Q1AK7n8LiXXH9lu8Qta74ndcMb9oz9OmZCrz04Pg+V+SOiIiIiIioO/RGMKyfxNtCCmseQZ4B7cNTJe03DAF3JA7VzYLWFiGEbmCpv0yZUFLhZ/geEJwxsp0xLi4KBZXGw/mU4t9EREREREQDSV+5n+su/e1wmXkE4OPTpQjzM9V7qDlb2vYwNy1XR6pfd8Igs+j2N8SNRuOi4UDHinkHk7/AEQA1cGR3ONVZ/oiIiIiIiKh/6K9ZT30jYtLDymobse6+9GB3IyAPTBoZ7C6ounvIGgAM7uahdaFmy2EH7A4n1u3MRUZKbLC7Q0REREREfcRAy9zpDjxn3afHhq0JIUYDeBdAAuSMrV9LkvTfXm0EgP8G8C0ADQCWSZJ0sqf65M+qe9Pwwo5e322n+Jsevr+41exuv1Ef9pNPz2NwhBmbl87g8DUiIiIiogFA9JeK1zSg9WSaRwuAH0iSdCeAuwG8IIS406vNIwDGe/6tArCxB/vjl81qwer53TsFfE8ZFNa/M3P6uxa3hKgIMwtmExERERFRhzAG1XHBPGeSJu1J6gcVkHosEiFJUpmSRSRJUh2AcwCSvZo9BuBdSXYUwDAhRGJP9aktLX2koM7N2/5rAlHfUHHjNswm1j0iIiIiIqLA9fUhWMEIoPT1cxZKemW2NSFEKoBMAPQZOCEAACAASURBVMe8nkoGUKR5XOxZVua1/irImUkYM2ZMt/dv0yEHzAzjUi/JHB2LN/ZfgFsCti6fFezuEBERERFRD5K6GMEQ/awEc28M4wuF23vtcfaHa9jjY6CEEEMA/BHA/5EkKfCpwjQkSfq1JEkzJUmaGR8f370dBGA2AT/de77bt0tkpKahGY3NbsxNj2PdIyIiIiIiGhB6Mwso1DKOOGytHUKIcMiBox2SJL1v0KQEwGjN4xTPsl51pKAKT80c3X5Dom5wuaoBj2cm4/l5aRy2RkREREREA0rfz8EZmHoseOSZSe0tAOckSXrDT7OPADwjZHcDqJUkqcxP2x6x6ZADc9Pj8OHpXo9Z0QC290wZVm/PQUZKbLC7QkREREREPYizrQVPMDN+ujpcMdT0ZObRXABLAdwvhDjl+fctIcQaIcQaT5tPABQCKACwBcD3e7A/hjJSYrHxYCHmjBvR27umAayvFGgnIiIiIiLqTr0RSwvm3VZ/jRX2WMFsSZK+RDsZaZIcinuhp/oQCJvVgrUL0vD6nvxgdoMGGLNJ4MUH0pFXXMu6R0RERERE1C7+/Nxx/aFQdajoldnWQp3LDaQMH4yimlvB7goNEOFmEwor67F+cUawu0JEREREA8ymQw5kpMTqfsS0O5zIK67FmvnWIPYsOPrK+dh/thzDosLVfm465IDZJNfwTYyNxKKpSQCAvOJaZKTE4uPTpRgbF92lY9h0yIErVfVYNDVJ3e/L7+eh4kYjZo+Lw5r5Vmw65MDxS1Woqm/CDx+eBJvVgpffz8PxS9UYGxeltiu9Lt9vf3y6FMvnjlO3nRYfDZcbWDPfqm5bu//jl6qQEBOJ9Ysz1GMurKxXj62ta+U9bM2o5mxnr7W/181fHFU+bd86fAl/zq/AoxmJKKysBwAsmpqEf9p1BkMiw/DDhyepfQjF1x7QC7Ot9QVmExg4ol71aEYi9p2tgN3hDHZXiIiIiGiAyUiJxbqduep3UbvDiXU7cwdsPc6+cj5S46J1/TSbgNf35CN5WCR255Xh+XeysXp7DswmYPX2HOzOK+vyMWSkxGJ3nlyv1e5wwu5wYlduCQ7kV8LsiSaYTcDn+ZXIL6vD6u052HLYgV25JXBU1uNzTzu7w4kD+dcAAGdLb8DucCIjJRa7ckvw+p58tY2ybYWy7Q9PlcLucKrHvCu3BBkpsX6vlb96Q97tunKt/b1uEodF+rQ1m4AL5XV4bU8+/phTjA9PlWLZ28fhqKxXz1tbxxMKRF8r4jRz5kwpOzu727ZndzixYls2YgaHoeLG7W7bLlFbXl04SRctJyKiVkKIHEmSZga7H6TX3d/BiCi4vrzoxHPvnMB3pidj39kKbMjKHNDlFOwOJ1Zvz8G4uGgUX7/Vredjyo/24ebtFgDA5R8v7PD6a7bnYO/Zcvzq6ekYFhWu6+faBWl4Y/9F3JkYg5yrNQgzCbglCdGDwrB56YxuOQa7w4nntp1Ai0uCW5IQFRGGv/1mOn6+7wIeuSsRhy5U4rm5qfjFny/A7ZaH1wkAkeEmSAAam90wC8BkEmh2SXj2G2Px+5xi3JNuwdHCKjQ2uSABar9ffCAdr3nKyoyIjsDa+Wn46d7zaHFLCDcLmE0CEoAZY4fjXFmd4bWqa2zGXf+yH5HhJuT/+yO651L/YQ8AYPzIIXDevI03n57e6fNkdzjx/R0nMSI6AjX1TXjz6ek4U1KL1z/Jx8p547Dl8CUAQLhZ4JuTRuLTsxW69WenDse58jq4JQlJsYO73J+uaus72IDPPMorrsVby2b2u0roFNre2H8Ri6YmMXBEREREREGRPHwwmlrc+O3xIiyZM2ZAB44AuRZuXWML8kpqQ+58aAswe/dz5TwrbjW7kHO1BoA8MY9bAh6bltRtx2CzWtDY7Fa3PWfcCKycZ0Vjixsf5JZgyZwxGBRmhsvdWpdJAvDwlESMGREFAHBJwB2jYgAA4yzRuNXkwp++rsByWyqa3ZK67YcmJ2DlvNZ7pCVzxmDZ3HFodssD0JpcElbOS0NjsxtHCqq6dK0uXruJe8ZbunSebFYL0izRKKysxx2JMX631eySfAJHAHD8cg2W21IRP2QQLl67ibnpXetPTxrwwaM1863IK65FO7W9ibrVSw+OB2A85paIiIiIqKfleoINsYPD8N6xqwO+nIL2+EP5fHj3c8th4/uJD06WdNsxeG/nSIFTt9/3jl3F0ULfOj+ffFWGgms31cfnym8AAC4569VlW+2XvdYp99n2219e0rXZcrhQ97zRcQaaGnIgv7JL58nucOLrMvm4cq9eb3NbqXFRPstGDo3AVvtlFHvK6Bw837X+9KQBHzwC5LGK1fVNwe4GDTArtmWH5FhWIiIiIurf7A4n/vXjrwEAw6MisCErU1e7ZaBR6swoQvV8nC+v0/WzvVnDlTpFXaEM59NyuSXdftcuSMNnnnpGWrdb3HBroziev393okhd1OJy67ftcvtse/2nrY/DTQKNza3rdPRaebdbuyCt09daed18844EAHK217qdubhcVW/Yvqi6wWfZtbomNLW41cLeq+d3vj89jcEjyKlm6QlDgt0NGkBe25OPb1hHhGxKIhERERH1X3nFtfjRojvVxzarBRuyMj0jMgaevOJabMjKVB+H6vm4UlWv66fLDbyycJJh2399bDIezUjs8jHkFdfi0YxE3bLMMcNx36R4XT/S46N91k2MjUTC0Aj18X2TRgIApiS1/oD+eGaybp15E+J9tn3fxNbHEPpj7ui18m43aVRMp6+18rpJHj4YADA2LhobsjJRdl2eLc67Mk5sVLjPNoYMCsOkxKHISB7W5f70NAaPFCx5RL3sG9a4YHeBiIiIiAagNfOtyBwzXLfMZrUM2Hqca+ZbfX7UDcXz8Vd3jtL1c/W9abr6QFqZY4Zj/eKMLh/DmvlWrF+coVv21zNS8Pay2bo23509xmfd/3xyKu7W9DchRp6F7NvTktRl3ttePd/qs+0tz7TWbw43m3yO2eha+StpbHQ+OnutvV83EiTYrP5rFj00OdFg2Sh8+MI9GBHdGmQLxdcewOARALnuTNn1W8HuBg0gg8JMKKw0TmckIiIiIuot/A2957G6rp5o44T09jxWbfUl4G0MkCvM4BEAswmobWwJdjdoALkzcSh25ZaG5FhWIiIiIur/BsbtbmjornhIdwQ6epN3IEgKQqiyN4JRgR+X/3Z94doyeAR5HGX8kIj2GxJ1k1NFtVg8PSkkx7ISEREREVHo8QnGBCttrIuBjrZWl7rroALcTHeew/6egcTgkUdaPAtmU++JDDfBLRmPuSUiIiIiIlL0hawUYdBJ74yczgZqdBO29aFxlr5d7QMXsg0MHgHISIlFzpWaYHeDBpDHM5MxNs53RgIiIiIiIiItfwGTvhBHMQyXtBEN6wvH5E/7w9cMwklep6LbMq96AINHHu4QvkjU/5iEPNXmpkOOYHeFiIiIiAYg3v1Qd+pQTk0b997t3ZYHmoXVm/WV/A1X69t5Rr4YPALw6y8KdVPjEfW0HceK8OGpUmSkxAa7K0RERERE1IO6GkTwFzAJ5SwVQA4EdbSH7QV9QvyQ+zUGjwCsujcNNQ1Nwe4GDSBhJoHHpiXBZrUEuytERERENAD1t6wI6j1dfu20lT5kEBzqqwGjQGoe9aX3IYNHHkYFvkLNuLioYHeBusmiqUlYvzgj2N0gIiIiIqJ+KBTubiXo+xGMGFBv7NM7W8p/aMG3N30pLsbgEYC84lo8f09asLvRrr70wqK27TtbDrvDGexuEBERERFRD+up+7hQuj/sSC5GW02765j6XrZSKIT72sbgEeTp0kN9vCgA3G52BbsL1A0EgBaXhNXbcxhAIiIiIiKigPRmEeie0AduuTvFX8HswNbtOxg8AmB3OPHe0SudXr+3LnjZjdudWq8vvSAHAgnAkzOTkRYfjbzi2mB3h4iIiIgGsP56Q98V3Z1Y0OWC2X62EErXzqiH/s5jmyWPjGoeaYJmfT2A1p5QPjoGjyDPtnbfxPhOr99bF3hUzKCA28ZFh6t/h5v9vztNAoiKCM2Xwcih8gx4EW30v6/6Q04JfvjwJKyZbw12V4iIiIhoAOoDJV/JI9QCJoHWCw5mhZ9QOGOBBPf60vswNKMGvWjTIQfmpsfhwPnKYHelXUvuHhtw25qGZvVvl9v/qzbMJNDQ5O5Sv7pDbKTZZ1l1fTOGR4WjySUFPYAkIAfausvi6cmcaY2IiIiIiDot1IJKgWprmFdfPSYgtDLBesKADx5lpMRi48FCzO9C5tE96b0TBHj7y0uBN9a8cF1tvIjbeq6zOhNkqW30ref01KwU1DW2ID0+Gk1+OtpbISUJQBsxOACtUeP2Al0mIc+29vL7edh0yNE9HSQiIiIion6pKzV1ekt3zV7enwIwHSoiHvqXmMEjm9WCDVmZONjJzCOzAE4XX+/mXhmr1mQTtUf7nkuPj/bfrgfeneYORI+UF+B4gz5+eqYC766YjT//YAEen5ZkuP6QyDDYrHGd6WaHTE6Kgdnr3aJ9LCB/0KXGRfkNdCncErBi2wnszitDRkps93eWiIiIiIgoxARy69l9s631fhSqLwSAuiIs2B0Itk2HHMhIiUX6yCEdLl4cbhIYExeFouqGdtuaAHR1cFhSbCRKaxsDahsVbkJ9s7zHgsp6v+0iw82Ii45AUc2tLvZO9kRmMj7ILQm4fUS4CY9MSdStkzJ8MIprbmHJnDGwWS2wO5z44qITL96fjl9+XqBb3xofja9KjK9bZLgJjc3dMySvrLYR//DwJPx4bz5cnk2aIOCChDsSh+JcWR1MArhS1YAJCUNwoeJmm9trbHbjlQcncOgaERER0QCh3Hdov//ZHU7kFdcGpQ5mW/fWwe5rR/cfSPvuOKZAt2HUzojd4cSvvyjEqnvTfLZptPx8eR2Ka1pHLqx+Nxur/PT95/vPI2ZwOBZNTcLHp0sxNi4aGSmxyCuuxdHCKhRVN2BsXBSen5eGvOJaFFXX42hhNRqaXFgwMR6Lpiapbc+X1+m2/dNPz2H9J1+rjx9780uEe//SDuCTvDLDvn1xoTVxw3skxrmyG7qavY+9+SXqbrUmUbS4JGw57NCtb3SN37Ff9rR3q/swm4DdXn36991fY8exK0iIiVTPFQB8XXYDdybGYP3iDHW7ynPrF2fg5ffz1G3kXpWTSY5fqoZJOLDvTDkAoPS6/h77+KVqn3ORfbkGmw45UF3fBAB47+gVHDx/DYumJuHXXxQiMTYSafHRcLnlWeLtDqd6PXv7c2PAZx5lpMRixbYTnZr16p0Vs/Hvj08JKMTYHSGMa3WBz7Z2q6V1j8MG62OESbGR6t/jLNH44v/e57N+hFlgcHjHXx6XnDeRGGBhbyGAcLMJT85Mwd3jRqjLS2pu4YnMZLx37Cq2HHZg3c5cbMjKROVN3+M/VVSLxmbfIW8AMFuzza7akJWJycmxiIpoPZeWofJxxkSG49WFkzDOEo20+GgkDRvc7vYez0xWg1BERERE1P9lpMRi3c5cfHnRidLrt2B3OLFuZ25IZqIrfT2QX4Gqm7d7va/K/v/0dQVqbzW3u3+lvb3AieKaBsP2SptP8kpxu8XV5jb9BdaUbew7W4762y1+t+Hd/xZ36xf/P5+rQG1D6zHNTY/Dup25OHj+Gq7VNfos33umTL3f2XrkMjJSYtXbz4MXnFix7YRhX//0dQV25ZbiuW0n8PHpMphNUPs6Nz0Ojsp6fJ5fieVbT+BgfgV2HCuCo7IeVTdv48NTpVj1bjZS46KQPCwSZV4JDNduNqGqvjWg83XJDWRfrvHpw5BBYag0uIdNHt56v6S9NwWA//7zRax8J7t126U3UOhsTdZocUt4bU++7lx7n/ePTpXgzqQYAPKoj4raRggAr+/Jx9elN3T7u1rdgM/zK/HHnBKs2HYCH+SWYFduCS6U12F3XhkO5FcgNS4Kq7fn4IPcEnx8ugwbDxTgo9Ol+PBUKT7ILYGjUk4cOHi+Epcqb+JMaa16Dbz35a2opgFNLW6cLpIDUCcuVePDU6VY/W4OMlJisSu3BK/tyYcA8NGpEqzenhO0ESwDPvMIkOv+hJkEWtoramMgr7gW25bPQtaWY222+97s0fjt8SL18fCocNxsbEGzW5KHPAWwr7uSY5BbFGCQS7PBhiZ9cEXJXjILoODaTfzmy0Kf41+YkYg/n7sGdDBz51RRbbs1j0xCfhOPiArH/2RNx4ptJ9CsGer1ysJJ2HiwEGsXpOGN/Rfx0oPjkVdci4obvllXYSaBeeMt+Dzfd9ihUWQXAGalDscJgw+39qzenoOFdyXidyfk66h8iGaOGY7JSbGoaSjEhqxMAHI03S3B8NqahPw/jSdnpnS4D0RERETUNynlMp7bdgKNzW4MGxyOXy2ZHrRM9LZ+/1b6uuQ3x+Tv7dER2JCV2Wt9Vfav3GO1t3+l/fPvZKOhyYXYweHY6HVutdscZ4lC7a2WDh+TdhtDI8MQbjYZbsO7/9pT/bwnMKI9pslJsXj27eNodkk+y7X3mc/dk6rb1w8fnoif7D1v2NcmlwS4lPtANzYeLFS3q2zjtT35uN3ixtFLNbr1IsKA+iYX1rx3EiOiI/DI5AR8erbCYC+yZj/30Vvtlw3vDdNHDlH/fvF3p3TPSZBQr5nQqbmdkiDe1/hvvzkeL/7uFL6R1lraROlHRJgJt1v097dK15tcbsBzuqIizPjNszMBQH0PDI0MQ3OLG42SGz/ZJ5/zoZFhaPCq3fu/2cXq397xBaP71rlWC9740wX1cWOLG0Mjw3C72Y1fflaA6Ah5Yqn1n+br+haMz40Bn3mkBH8ezUjs8Lqrt+cYpiMKyEOmwjTvlLFxrTV9JifFoKahGc1uCU9kJmFIZGAxvIADR0onPIxqED09ZzS2Pz8HJiFHYEdEh+ue351XhhcfSG9zF09MS/IpDv3ApPh2C0srz9c0NONsaS1cEpAe3/oBsnKeFRuyMuFyA28tmwmXW44inzI4/h8+MhGzx8Vh5FDfbKcJI4ca7v/klY4Hjn5zuBAA8O2prbWX/nqGHPwprLypZkcBckR/2uhhAIC46Aifbc3yZESt3p4Du8PZ4b4QERERUd9ks1owZJD83f/hKaNCuoSBzWpRv7cr5SR6e/+KQPZvs1rUe4J54y2G7ZVll5wNnT4mZZ26xpY2t6FdHm4woY52XZvVogZJvJdrjfe6v1k+d1zA/fbu68p5vkOelNvGp+eMVbOvlswZg4emtH2vPHPscL/Pae8NAylDtGDiyPYbtWFwuBxs+UthlU8/vhFgrdz5E+LVIJvS/+W2VJ/JppbbUjvUt4SYyICWLbelYqgnRvDAHQm652aMHR60z40BHzxSxgl+cdGJcZaoDq9/trRWN94RAEZEh+MHD07QRRr/+88X1b8LPWltEWaBJ2eO1gVpArkgZtH+jGaSBNw7Xn5RGb1HUy3RsFkteDwzGYmxkbhW1+S1AeCnnxpHsRW/+G4mtj03W9eXWePi8OrCSW2uFxkmH+WomEi8sf8ivjM9GfdOMJ7tzma1YM18qxq997ZynhVF1fWGQ/q+lTHKcJsuSQ5ydcSB/EpY46PxcV6puuw/n5yK+yeOxP6vK9QP47ziWmzIysQIg6CRwjJkEDYvnYFHMxI7NVySiIioPUKIh4UQ54UQBUKIfzB4fqwQ4jMhRJ4Q4qAQgumwRL3A7nCixjMJzidflYX0D4navr137Gqv97Wj+7c7nCjx1Jg5eL7SsH13HFOg29AuN8qe0a7rb5ve275Qoa89tLUDs3F791VbN0ih3L7uOHZFt97vs4t82mpld/DH+bZuZQ+ev9ahbXkr8NSeHRvne29/5GJg11t5/WjP11ZPDSUto2VtMRpJY7Rsq/0yahrk+/P9X5frnjt+qTponxsDPnikjCndkJWJe9I7FsF78YF0/PiT89iVW6LLMqqqb8ZP957H4HAThkcpGT2tHxguCXh14SRse242Vm/PwRv7L6hvoJ89meE3MKRkEIWbTZA8Q+38kQDkFl3H5KQYtGg+rGZ5osI//fQ87A4n1i/OwPyJvoGUyAgz4g2yebydLa3VRZDNJmByUtvjL99ePgsPTU5AaW0jVs4bh0VTk3QfUP7GDtusFsxO1Ue1X/0gDzuPFalDwLRn5M0Dvh+IAHBPelzHsrgAzE234OvSOl2BtS2HHThw/ppan8nucKqBrstVcpHyqvomn205rt1EXnEt1i/OCEpxRCIi6t+EEGYAbwJ4BMCdAL4nhLjTq9l/AnhXkqQMAP8GYH3v9pJo4FG+46Z46mP+wyOT5Do9IRhAUvqq2JCV2at97ej+lfZTPPcha+an+bTvjmMKdBve7SINaskq6yo1XttbDsg1j+wOp3rPowyfCsTaBa3nZMthh65ukDdtbdZHpiTA7qjy2xaQJ5LqCKVGkBHRZmjJl/c1Vu4rW+/DW/kbXuetxe3Gim0nsHp7TpvtXB0se3PYIHh1xOD153JLavjAbVBFJlgjWAZ88EjJFLFZLbB6xl4GelJ++VkB5k+0IMxswmDPWESVBLy1bBamJMsfYH/7zQlqsGfNvWlYOU8OMigFlrXpapHhZiQYFJ1W0h0bW9yYm27B32hq5oSZhO4NYhYCs1KHo6y2URd1/f2ab+DVhZPQ4pbUoVjrF2cg1Ssy++ID6WjUjAcdM8K3CPSrH+ThtT35eEWTafTannws33oCYSbhM5W9lt1RBZs1Du8duwoAePrusepzSjDPOx1vy2EHTlyuwT3premGO44VIWvOaDyRmYxwk9BlWRkV0o4MM+H45RqsXZDm85zaxuDD/UiBE3//8ARsXjoDABAXHY7XPcf+i6em+fyPIyZSvhZGRccvVtwMycKIRETUb8wGUCBJUqEkSU0AfgfgMa82dwL43PP3AYPniaibKfcd0Z5haxkpw7AhKzMkM9GVviqUUQC91deO7t87+3/iqBif9t1xTIFuw7tdmMn3nkBZ90hBleE2vZcDwPK5qbp9LZgQePKDyw21r0cKqmCNj/Zpo9yzLbyrdQRHyfVGDBvsG4jRuiMxJqA+SJ67teI2Zvpeda//+zQj3tf46TljAAA3brXo2gkAD0/WDwHzZ1bqCEwYNVRX2ka5D9R6bFqSz7K2jBnhmw01erjvssemJalFxe9O008C9TczRwdtBMuADx4pmSKbDjnw8Sl5SJJ1pO8bSTEorPWUNbvceH5eGjYvnaEW3lLecGsXWH2CHzGR4Vh3X7ouZfCHD09SU1cB4J8/PAuzSeAXT03z2beAXJTricxkfFngxP9mF2NwuAkv3p+OcLPA9YZmpHqG3kUPMuNUkfyhNVrzIhVCYOU8K15ZOEk3/lRb8R6AWlBNUdPQ7FM76X9PFOPpOaN142UzR8ei2eVGuFnoZibTBnWUInHr7k9Xgy6TNR84RmOH7Q4n3th/Ea8snIT3nr9bXX5PehxGj4jGx6dLERlhxhOZrW9gIYTPC3zTkhn4zvRkbDxYCCNTkmLQ1OIb3n08s3WdJXPGoKq+GY9nJqnH7v0/DuV/XI0G25qZGrxxqkRENCAkA9COMSj2LNM6DWCx5+8nAAwVQvgUgxBCrBJCZAshsisrfSenIKLAKfcdWkqJhmCSDIpcBLuvHd1/IO07sk1/+SSBbsOnnZ9kGpvVgm3LZxtu02j5hIShun39aolvQMMfpU9r5luxbflsfPaDBT5tlMDmj749WV22bfls/IvmsZE/rLUF3A8AWGAw8kUxcZRx3Vp/vK+xkhCSatHf0wshjyYJxNNzxuLDF+7B+sUZ6jKj+zft84Ewmg18VqrvsvWLM9Rr8axXXaVUS3TQRrAM+OCRIiMlFmc80/YNjfQfWZU0Y7TCzSZ8fLoUNqsFz98jFysb5AnmeI8pffNAATY8nYm/e2iiLktFCTpcq2sd67h56QzdizPFE9iRPM/94qlpmDY6Fi63hJcenIC7rXEIM5sQGW5ChidV80ZjC+ZPiEdeca1P9NbucMLllj8IFNc9AaxEz1SJSpEwbZ/meb3Z3l0xG689oX/DPDQlEU/NGo3HM5N10dnM0a2ZNg1NLrz4QLpahOyhyQn48HSJ+vx7x65iy2EHNh1qHXaWV1yLt5bNxMp5Vt15zblyHWYTsO9shefcZGKKZ1rG2akjMCjcjMlJrR9Ac9LisGhqEh4yiDo/MS0JV6obYPaaeuLF+9Px53MVmDY6Fh+fLsUnZ8rx4v3pOHRBPw5W+z8O5WUyY4xv8TjLkEGwO5y64yMiIuplfwdgvhAiF8B8ACVQ55lpJUnSryVJmilJ0sz4+I7VCyQiY23NctabOjo8qC8JuSPr+KTeIcUowNip7QSwma5eu0D20e42euiCGfUtVD4PAsHgkYfNasG/PTYFAFCpKb789JzRiNIMSdNm62xeOgP7zlZgy2EHth+9gshwE8LNJtxtjVMDRNc9ha5euC9dVzVfm6Vis1qQ6QkyZBlk3TQ0uTDXGodwzTiwh6ck4tWFk+Byy4GVzUtn4K1lsxAdGYbhUeF4IjMZe8+Uw2yCbqysUT0hu8OJ8+Vy8bWbt1vwRGYyduWW+BRRq/aq35NXXOsTBFkz34r1izOwfnGG7jg+eOEe/JWnUvwTmcm6cbRp8dE4oJmycO2CNLy+J1837E2JlL/8fp5u7Olby2biZ/suYNpoedY7u8OJ0tpG3JNuwZcFTiyenoSy2tbruc1+Cet25mLRVN8Uw0HhZtxudvmMha28Ka//F0cVdueVYUNWJl56cKLPUDUj58rrdMPsAMBRWWdY04mIiKiblAAYrXmc4lmmkiSpVJKkxZIkZQJ41bPseu91kYio5/TxWE3AuiNQEgxBC1z2oUiN8PQ1lK4xg0caT80ajW9YR6BIMwbzo9Nl6hUbMsgMk+YFZ7NasHZBGt7YkyCVrAAAIABJREFUfxGPZiTi7WWzsHnpDDVYsyErE3WN8lhL73Gg2iwVu8OJS5X1WDlvHP54ssQnGLEhKxM7Vt6tblspzLxynhVr5lt1aZH7zlbgzaen4xdPTcNby2Zi48FCXX0fo3pCecW1anbTC/el4xdPTcN9k+Lxs70XdOs56/SV4Iuq6wMOgtgdTuRcrfFk7FTq1nG5oaubtPFgIV7xBMYCMSjMhISYSH3x8/EWvLpwEj49U6E7/l9+VmCYdQQAf8gpRrNLQoJXofAPT5Vi89IZeDwzGY9mJPoNAmopgbZH7hqFr8vq8Ihmn/nlN7F2QRqHrhERUU85AWC8EGKcECICwHcBfKRtIISwCCGU74EvA3i7l/tINGCF0s1gf9OHYgNdInroQKUefnEG/aUf5DdfRy6b0tS7xz19jdrC4JGGnIFzU1c3BwBeWXgHAGDIoHD8/cMTdM8VVtbj8cwkNdNGG1CwWS26ekP+9rluZy42PJ2JVxfeaZjNEkiwAtAX/9a2d7mBrDnyD5BG9YS04yWVINfz89IwSFPs+ZEpCSi9cVu33s5jRQEHQZSgjlHGjhIIWzQ1Ue2jEhjztn5xhm443Lqdudi8dAbWL87QHb+yzYcmJ6Cwsl5t//y8cUiLj8aKbdnqsuRh8lC9iaOG4pWFrTWolALn00YPg81qUTOqtPyNk65rlLfhdkvYkJWJjUtnqkPYkodFBhwYIyIi6ihJkloArAOwD8A5AL+XJOmsEOLfhBDf9jRbAOC8EOICgAQArwWls0QUND01NCeY+ntgzieQ0EP76angVGD77tr6/s5JKLw0OvL6VM5DMINF3hg88tBmrfziqUyM9xTa+uYdCbgreRgAICrCrCu0bHc4se9shc8QqI4Uk/MX8PEXIOpMobiMlFjsPVNhWItJ4R3kslktuiDN+ydLcY9n26M9WUqPew0/a0t7x2h3OHGkoKrNPmr7ZhkiF6TWBsOMjn/R1CTszitTH2+zX8YvPyvASw+OV5eVXm/EI1NGoay2EZOTYvHcPakAgDCzwIv3pyO/vC7gqRA3HXLA7nBibJxcoO2RKXJA7OX383Cpqh4josJRWXebQ9aIiKhHSZL0iSRJEyRJskqS9Jpn2T9LkvSR5+8/SJI03tPmeUmSbre9RSLqLgMlOyaYul43p5tv2HnNAQR6GniytEIndMTgkUobxLE7nKiqb1LrBuUVyyUAhkaGYUNWpvpy9jelPNAaRNAyKpLc0zMZaINiRlk/3v2UJEntp81qQfIwOVD08JQEfF1+A09kJqG45haeyEz2GX7WlraOsb0+Gh2TW0JAgSZ/JifFInO0HBR8PDMJG5fMwIasTKzenoN37JcN61cFsp+MlFis25mrDlvLL7+B1dtz1FpJcUMGYdroYQFvj4iIiIioJ/TngtmhdMNNrQK7Ll27eiGUqOPDKHDsL5gcikFmBo88lCCOPgNpmqcg83m5kRCwWS14bm4qAOMhYAoliKAUzD5XdiMoRZLby2xS+ll7q9mnn3aHEw1NLfjWlFHYlVuKR6Yk4NAFJ15ZOAmHLlRi7YK0bgmCdCT7qqOBJqWY+KIMOQNouS0Vm5fOwMenS5FfXocnMpN8Zk2bMGqoYf0qf9lgWkrfz5TKbTcdKsSjGYnqDHpCAHFDBgW8PSIiIiIiCkwo3nD3pFAa0tQhvXCdBthLoVeEBbsDocY7kJFXXIsnpqfgHftlCMjBi9/nFMNmjcN7x67ibmucYQBJCSIotXU2HnRg45LpvV4k2SiDSanNpO3ns28fBwC8ecCBXz8jD1dbtzMXbz49HXnFtcgcOwxv7L+Ilx4cj5XzrJicFKueq64GQdrro1ZbgSaj9mvmW+UhcY7WIXFDB4dh39kKvLVspi5g+NDkBDXIo1C2bZQh5o/NasF3pqfgdyeKkDVnDF751h0BHx8REREREfUzHAUHINCsoK4dnb9aXqEQZ+tMH0Kh3woGj7x4BzIyUmLxqwMFAID6283qNPHr7k+X/9vG0DWb1YLn7xmH/zlQgGe/MTZkgwU2qwWPZiThg9wSfG/2aNisFmw65FCPS+m3EjBS1tEGcH78aX6v9LUjgSZAn6lks1pwtzUOK7Zl46UHxwcUgOpMkMfucGL/1601phZMjA/Za09EREREwRFKN4X9xUA7p919uH6LTQ+w4Ffvvo6Mz4bwO99a8HDYWjtsVgv+9duTAQD1TS4AULNT2itubXc4seP41S7V5ukNdocThy5U4sX70/HHkyWwO5w9XouptxhlKr21bKZPoe/uOrZAhtUNtP+pERERERFR39HR4E6gM/cF1q7nbpaCfRvWkWGVrbOt9UxfOoOZRwF4YnoKsq/UYMcxORCkDar4y0wxynhpK0spWPpKPzuro5lKXdXesLr+XJiQiIiIiAIXKvV5QunmtKu665x2+ynpoWvdW9euu1+rohde/F05NT3Vvb7+XmPmUQDsDic+PVPeoQyijhSBDqa+0s++or9kbBERERFR/xYqwSvquN6+dn0p6NEdfQ2l4w2hrjDzqD2dzczp7YyXzuqpfm465PCZWc7ucKrFp4mIiIiIiChwoTq7WsDd6sXuG8XX+lK8VK14FEKXnJlH7WBmTudkpMSq09wDrUE474DSQBTomGAiIiIi6r9C6aaQ+qi+Omytezen09Yp6VNvuRBMDWTwqB0chtQ5SpANAIYNDu9XdZS6IgQ/A4iIiIioF4XK90EGr/qenq4V5O810V2vFb7kfLV3Sb0TD4L5vuWwNeoxNqsFy2yp2Ga/7FNonIiIiCiYlCH22u8nfWmIfV/vf39mdG1efj8PALB+cYa6LK/4Oo54Zjn2t153XlPt9jcdcmBUTCQAoK6xudv3NdAs23ocJgEkeM7pwfOVuHm7RX2+rrHFZ50Hfn4QADB73Ajd8sfe/BJ3JsZg0dQkn3V+vi8f6z85h2aXHEFY99uTAfdxxbbjOFxQBZdbwrDBYRgWFeHTptkzJfXbX15Sl03/9/24Jz2+zW0v+NnBgPqg1A7ecrjQb5sff5of0LYUj7/5Ja5WN2BQmBkJMYPQ4Jkh/fjlap+2v/zsYkDb3HTIgR99dAaDwszqMuV6aRkta0t++Q2fZR+fLvFZNv3f9qPW877ceNChe27rl5fgkiRkpMTi49OlGBsX3WvvWWYeUY+xO5z46HRphwqNExEREfUGZYi98v2krw2x7+v978+Mrs3uvDLszivTfR/+yd583fXq6Wuq3X5GSixe3fUVACDcbOp3r5+u1gbq6Opz0+Pwef7/z969B0ZRnvsD/85ubiQkIckGSAgQsohBIBhA0CiC2lotWpVejdCCiOiR9py255wWe86v7ekRerO3gxWMSBSJ1SqiQhUrCgQWE3KBBSRcNlxyI2QTyJVcdnd+f+zOZGZ2Znd2s5fZ5Pn8Azs7O/POzG5m5pnnfd4W7KhqwM7qBjS398oGjIQsLd2wtHTjncp67DI38dNrmjqxs7oBT7xagVXFR0SfaWzvxeX2Xv516Vn191d7a1rQb3PA7mDR2j0AS0u32zw2h3PD9YIoQVv3AN4/1uhx2dauXo/vc5o7+gAANZc7Feepu9qjalmco3XtaOsewJWOXhyta8eZ5i4A8gE7a1e/qmWebOxwLrNzcLvk9pfcNG9tlerstbtNi4vWwxXHw8lGccCpuasXeh2wZlsldpmbQvqbpcwjEhT+FhofCShFmBBCCAk/rov9k69V4tacNFRduhpR1ylc+594tQL3zRiPfWdaIqr9wxl3bNZsq0T+xDE40diBzcvnAgCe2V6Fqz3OjIKf3JcrOl4FRgM2PpqPx4uP4OtzsvDhicsBPaaDy6/A0jkT+D5E1q7+YXSdHp4+gasXOjM/ntvtW9aMXgf021lMSIkVBTv67Q4+eCDEZRwF258+UZehw4mN0sPW7x4E8Yfcdqv6nIpdkxwXhXYvQb1AtGWoGtuVg3F2B/Dbj04jLlqPzcvnhvQ3S5lHJCio0DghhBBCtK7AaEBXnw2fnGrGsgWTIu7GucBoQE+/HTuqGyKy/cNZgdGAzl4bDpy18semwGjApNR4fp68rDFun5uZlYzeAQe2l10KyjHNmzgGvQN2lJRdwjfmZvHTh8/3J3xPaVcvNCIxzrfcDLsDGJsYi/PWwQyWPpt84CiU+n0MUmUJvtdqjBkV7dP8geJL4AgAMpLjgtSSoRmws1hZkB3y3ywFjzRo036LWxcvk8WKTfstCp/QHio0TgghhBCtE15vRWIX+0hv/3Amd2xMFquou465/prb58rPO+u0xOiZoBzT8vOtAIBoHYO3K+vd2jhcBLuwtJyiUovXrmpSDIArnX3BaVAIWa50+TT/tesDQWpJYDV5yAAKt62mCyH/zVLwSIMiaZj74RDoIoQQQsjIw11fcTYW5ovqzWhdpLd/OJM7Nmu2VWLNtkrcecNg4eHffFQjOl4mixX/8fdjAJzdgAJ9TE0WK378lnP5Oh0j6uE13L4/Q6155KuiUgue213jc6c5FoBOI6PvDQVXK2m40fqhWbOtMqS/WQoeaZBwmHsAmu6DHEmBLq0Ynn9aCSGEkMjCdbHnRFoX+0hv/3Amd2weyMvAA3kZGC/oBvOT+3JFx8tc347ffmO26HOBPKbm+nY8/y3n8h0si/UPzwrausInMLf70uHRvTl0rhV356bDMNp9BDNPUuKjMcWQ4NNntCguyrewwvSMxCC1JLDiY/TeZwqTzcvn4oG8jJD+ZqlgtkYVGA24O3csPq25ouk+yNyJprCoDIC2A11aEI4UWkIIIYS4k+tKz9WliQSR3v7hTO7YbFiaBwD4750n+GmzspJFdY+eWmREO9edx3XJGMhj+tQiIz98fLRehzmTU0TvD4/vT3ge0xavnA8A+Pbmw2jpch8iXsl3b8uGtavP51G7tObLM8bjAy8jsgmtudOIf3vzaBBbFBiP3ToZLx2oDXczZIXj90qZRxplslhxtO5aRAxzX2A0YOXt2QCGU7E9QgghhBBChpeR8hgzUh7YRkgzR6xQd3/UOgoeaZBwmPsf3Xuj5vsgmyxWvHe0MSICXYQQQgghhGhWkO9VR8qtcCTd9EdOS5X5ur8jJWgWQV+jkKDgkQZF0jD3kRbo0gL6I0QIIYQQQpgRkwcUSpG1T+k7QCIJ1TzSoEjqw+4p0KXF9oYbnR4IIYQQQoiiIF8s0rWotjAMHRMto2f+YhQ8IkMSSYEuQgghhBBCtEJ2RC+6W9WEUPYUGA6HfDhsA/EuaN3WGIZ5hWGYKwzDnFB4P5lhmA8YhjnGMMxJhmFWBqsthBBCCCGEEBIpKBslOIJdC8nXWj50nEkkCWbNo2IA93l4/xkAX7AsOxvAYgDPMwwTE8T2EKIRFJsnhBBCCBmJIqVQMAkN+j5oG9WqFQta8Ihl2QMA2jzNAiCRcY6jONo1ry1Y7SFEC+gEQQghhBBClMh2ZQvGeuimWDPo9oBEinCOtrYRwHQAjQCOA/hXlmUdcjMyDPMkwzAVDMNUtLS0hLKNhBBCCCGEEBJSDD1xHJJIiY0xDBMxbfXIx42IlO93qIK5kSKcwaOvADgKIBPAzQA2MgyTJDcjy7IvsSw7j2XZeenp6aFsIyGEEEIIIYQEhNqMn2DX5iHaECExlBGLfoZi4QwerQSwg3U6B+A8gNwwtoeQkKA/QoQQQgghJBxGSrBihGxmxKLjE5nCGTy6BOAeAGAYZhyAGwHUhrE9hATdSDlhE0IIIYQQ/wWrW4/wISZdlwYe42NYxNf5SWhRBqBYVLAWzDDMG3COomZgGKYewM8BRAMAy7KbAPwKQDHDMMfhDD7+hGVZa7DaQwghhBBCCCFk+KNb/tAarrWBhudW+S9owSOWZR/18n4jgHuDtX5CCCGEEEII0Sq5pIZQJjqMxKQKrW3zSM3+GqnbHenC2W2NkBFJY+csQgghhBCiMcG6uR4p16GREpuIlHYSAlDwiJCQon7NhBBCCCGEaJvWMpS0brjur+G6Xf4KWrc1QgghhBBCiH827bcgLysZBUYDP81kscJc346nFhnD2DJt8rS/AMi+99KBWjx5Z47qfcytw1zfzv+r1wF2h3P5Lx2oxe1T02B3iD+3bocZG5bmwWSx4mRju6rtYVnlbeLaLWxH+flWjEuKw4OzM/npHxxrxOS0BH77zfXtmDZuNADA7mBRUn5RtM4VW8tx+9Q0rF7ovu3rdpjx4OxMvi0mi9Vt+Ur7UW471u0wAwA2LM2T/YxacssGgI9OXsa9M8arXgZ3HDnFh86D0TH8tgHg22ayDJbpNVmsKDAa+HaUn29V3XYA+O2e04jVR/7D5dauPp/m/9WuL4LUksB6p7Iu3E1QxP2GJqclhOycQJlHhBBCCCGEaExeVjLWllRj/5kruN5vh8lixdqSav5Glohx++vAmRb09NtE+4t7759fNAMA/97tU9OwtqQae081w+5gve5jbjl6BnhmexXOt3Rh/e4aXGrtxjPbqzBhTBzW767BgN2BvAmDy9h1rAkvfHYWz2yvgk7QH81b1zRufZ+caoZD0D6u3QN2B9aWVONSazc+rWnBjqoGPPlaBewOB9Zsq8Quc5No+x0OB3701jEAQL/dAb2kAbdPTcP63TXYvN+CpvbrKCq1DG6DuQlrtlXiH+ZG7Dt9RXb50nZy+5F7f1/NFfTZnN/lXeYm7DI3yR4vX3DL5o7h1Z5+AEC2K6jlyaGzVnT0DkCvA9bvrhH1D/jtntP8cVyzrRJPvlaB3PGJfDs53LpnZjrns/uYqWJ3sOgZcHifUeMqL13zaf7mDt+CTeGi5WOzs7qB/w2GCmUeERJiNOQjIYQQQrwpMBqwsTAfy14ug4MFUhNisLEw3y3Dgjhx+2v5lnLYHazb/nrmLiNWv1aBB/IyYLK08u9NG5eI771yBLMmJKPh2nWP+5hbx+pXK9Ddb8ebFfVIS4hGSbkzO+HDE81Yc2cOnv/4DO7OHct/rrPPht/tOYPxSbE43jCYeeTpkpBhnOtb/8hMPPFqBRZMScXZK118+xJiovDsuycwb3IKv/4+mwN9NuB3e84gMS4Km5fP5bdlY2E+CovKkBinBwDoGOD1zy+J1sllHD23uwYbPhQHUzYvn4vHi4/gX1yBE+nyldop3G/O9UchWq/D5uVzAUDxeKlVYDTg/76Tj8e2lGF6RiLOXekCANw4PtHrZ1e9dgS9Aw6kxEfj2SW5+M2Hp/n3Hr8jG89/fAaLb0wHAHT32bFi6xG+nYVFZQCAP3/nZizfUo4ZmUkj+hrf7hi52x4uUa7fUSjPCZR5REgI0cgChBBCCFGrwGgAd0+2bMEkChx5UWA08Dex0v1lc6WE7DI3id7Ln5QCADje0K5qHxcYDRiXFMe/bu0e4P+/bMEk3Oxa3qc1V9w+e7mjDzeO8x7UEJqU6sygKTvfJmpflN55G1dx8SpiZLo9rSzIFm0L9//OXjsAwMECD+dnun1O2GVtmiAAU2A04KaMJMXlZxvk2+m+fhv/vqfj5YtbjWkAgFNNnchKiVf9udgo5z58IC8TqxcaYRMEQG6e6DyO+063YGVBNl9oXNrOW7JTAQAnGzuwND/Lr/YT4g/pbzAUKHhECCGEEEKIBglrq7xedkn0mrjztL8utnYDAOZMGiN6r/x8GwAgRs+o2scmixV1V3tk33u97BI+cXWNyzHId5s63dzpcfnS/I1jdc7uQGkJMaL2cRk2E1NGoV+mr9RW0wW32jxCOgbYWd3o9jlhV7UzlwfbarJYRVlT0uUrtVNu/dz7gfp+H7YM1hmqVzg2cjp7bQCA9481irYbAD6tcR7H7LR4bDVdUGzn57XOdesY4J2qep/bToi/pL/BUKDgESGEEEIIIRojra2ysTAfa0uqKYCkwNP+MlmseNcVKJk7OYV/r6jUgv/4u7MGUGyU3us+5tYxRSEwdP/McXjbFUBITYiRnWfWBHX1SRjX+jZ8eAoAkJ4YK2r39s+dxa6bO/sQHy1/S7dmWyW//cJ9Azgzj5bdOkk0rajUgvW7a/jXj86fKFqWdNRg4fKf+4eznYbRsW77Ue7YrNlWiTXbKkXT/Pl+myxW/OAN57J1DJCXNQYAcPqy5yAd4NynAFC4YJJouwHw35eUePFx5NrJ+eGbR13rZkCDKpNQstkd/G8wVCh4REiIUY9gQgghhHhjrm/HxsJ8/jVXN4YbPYyIedpf5vp2PDIn0+29Q+da8btvzgbgvD7zto+5dSiVtmm41otvzJkAAOjoHZCdx+FDXRxzfTvW3T9dtt3Lbp0MAFgwJRXRUe63dJuXz8UDeRn89gv3DQBE6xlIy9QcOteKZ5fk8q/vn5XB//+BvAzcO2Oc4vKfdbWTq9Uk3I9yx+aBvAw8kJchmubP99tc346/PDq4bC5od8GVaSYl3ORR0c76T3a7Q7TdwGCXvs7eAb4+k7CdnD9++2bnclngf742w6e2EzIUD+dP4H+DoUIFswkJIXogQQghhBA15IZe5urEEHfe9tdLB5zdkrjYDfdepyTI42kfc+uYYkjAWVe3MaHilfPx0YkmvF3VgOy0BJxpdp9nRmYyqlwjU8mFkYRFl59aZMSppg7Z9v29wlkkOz0xFolx0Wi/blO9HQCg1zF47NbJ+Ou+we5axSvnA3AWzJbasDQPG1zZRXLLV2ontx1yywOAN8rrZD+j1lOLjHzdJOH+/MqM8V4/y7iKkX5n/iTkpI8WbffdueOws7oRk9MSFOs3AcBtrnpLeh3D17siJBS431AoUeYRIYQQQgghZFiTdrniRGpGOBOAUVi8jQ7my+BhI3igMUJGDAoeEUIIIYQQQkY0X0IxauIkamI7crNIg0JegzJsCEfz1XgKva/N8xY88xUF0MhwR8EjQkKMTiyEEEIIIeGhdBkWqMuzoV7nqQ1ohCKOQ5es6vcBS3uLjAAUPCIklEL2aIgQQgghhHCULsFCfWUmbIencEMguqWNNL6GbwKxj7nukM6YHwWQyPBGwSNCCCGEEELIiBCIDHBPIYdAx3yCGUOiAJUyLgPMlz0kHb2OkOGGgkeEEEIIIYQQEgBD7rYWmGZ4X4+KFflSEyic3baGWrtoKJ+m7mpkJKHgESEhRqcYQgghhJDhTWl0N/WfVyeU15Vqtimc2UzCNSvFk4SBpkC3lOqakuGOgkeEhBAlBxNCCCGERLZQxAi8rSMUMRp/tjPQI5j5KlzX2iwoC4kMfxQ8IoQQQgghhJAIxLLskLOchgsWlOFPSDBR8IgQQgghhBAyIihlh4Q7Y0bKWzgoEJlHXhfhxy7RShHuUDeDZVnqtkaGPQoeERJi4b442bTfApPFKppmslixab8lTC0ihBBCCAmuUAU1Iil+EElt9RUFcggJPAoeERJCWngYk5eVjLUl1XwAyWSxYm1JNfKyksPcMkIIIYSQ4FIKKgQ6uOTv4pQLPfvfFn/5UsMnnMEa4apDfakt3G4KWJHhLircDSCEhFaB0YBfL52FwqIy3D9zPMrOt2FjYT4KjIZwN40QQgghJCgCGVQIRYCCCz5p4cEjR01bNNTckKOC2WS4o8wjQkag1IQYAMCHJy5j2YJJFDgihBBCyIgWjrICgVgnhSvE/BshbujLo+NARgIKHhEyAp1oaAcAZCTH4fWyS241kAghhBBCRgJ/uquFMlCg2M0uDDk+wy6rKMAbRN3WyHBH3dYICSEtnHRNFiv+8M8zAIDMMaPw43unYW1JNXVdI4QQohmb9luQl5XMn5c27bdArwPsDuCpRUYAwLodZgDAhqV5/OdMFivM9e38PIFuC/d/APx6pOuUtt3Xdsl9npvu6fNDXa9aatfj73xyxzoQ21FSftFtWlGpBQfOOB+g2R0sVmwth44BnliYAwB46UAtJoyJw6c1LVh8YzompyWg/HwrTl/ulF3H48XlGDPKmd3d2tXntU3vVjcgf1IKvw9eOlALm90BAGjvGcC6HWaMjnXerl1o7cbjxeWYPyUNTy0yYu+pZgDAYYsVcleYm/Zb8HltK6719GN6RpLodwIANgeL7Z+77xPhA8U9Jy6Lpr90oJZ/vaq4HLdMScOR862ovnQNYxNjAQDdfTZ+e6S/C+GyzPXt0AvSGFZsLcftU9Ngd4DfFzrJZpksVvzmoxrclJGEyWkJ/Pfm6y+aADiDN180Oh+Q/ubDU/jdnhrMn5LKz/vBsUY0XLvuts1//OdpJI2KFk3b7Gov42qbcL8KPflaBb9u4XyEBNuc//kYE9Picf/MjID+jfeEMo8IGWHM9e344Zen8a8LjAZsLMyHub49jK0ihBBCBkkHd9DrgPW7a/ibTZPFil3mJuwyNwV9AAhhW/KykrFmWyXWbKtEXlay7DqHOjCF9PPC6b58LhT7w9N6/J1P7lgHYjtyDKMBDGZfF5VasH53DW6dkgoA6LU5+EDRyq1HsGZbJfQMsL2sDtbOPuwyN6GurRuf1rSgqb1Xdh2f1rRgR3UDAKDi4lWvbbphrLNN3D6YMCYOB8+1AgAGHCzerqxHUel5AECfzYHPalr4/aJ3RVbiY6JglQlU5WUlY8KYOByta8eOqga37xMDYHpGkmgat685b1bU8/9fVXwENsdgas3emhYUHbBgb00LbA4Hapq7AAD1165Dr4Ps74KztqQadW3dWL+7hp+mZ5zHva6tm98Xn9a0iNq2ZlslzlzuxC5zE7+OolILzjQPBvMuu47N+dYeWFq6sbO6AXodsGZbJXaZm3C7MCjr2px9p1uwy9wk2hdfNDmX2dbdjwlj4vjpdW3dqGvr5l/fZkzj/2/t6gchodLWM4CzzV0hHfSICfew4b6aN28eW1FREe5mEOKXpX89hITYKGxbtSCs7ThyoQ3f3HQYcyen4J2nC8LaFkIIkWIYppJl2XnhbgcRC/U1mMlixb9sr8KN4xJx9koXnl6cgz99chb3zRiPz063YGNhPgBW+MwaAAAgAElEQVTgiVcr8I05Wdh1vMmvLNrsn+4GAFz49RKPbVlVXIFvzcvCO1X16LexePyObLxVUS+7TpPFitWvVeC+mePxWU2Lz+0yWax44tUKfH1OFra5skOU2idsv7CdH5j92x9q2/f061XIy0rGycYOxfWYLFY8+Vol7s4di4PnrF7n+/JN47D/TAueXpyDP/7zLB7Jn4APT1z2aTuUjufWQ+fxyw++AOAMUDkcwLNLcvGdWyZh1i8+RlyUDjodgxvHjUZ1XTt0DOBggRi9DmCAe6ePxe7jlzF1bALOXul2W69amWPi0HjNGeDY+cztuHniGH4fPPFqBYyGBBxv7HD7nI4B1n01F3/Zew4FxjQcPGdFd58dDOS70X1p+lhUXbqG+2eOw/ayOrf3Y6J02Pfvi1Hw609F828szEdhUZlzPzGA3bVwBs5AicnSKrtdY0ZF4dp1GwBnbU25Y8Ydm7tuTMdnp1swIzMJJwXbumBKCsrPX8XMCUlouNaLpxfn4DlXgCkmikFslB6bl88FADz5WiVmTkjC57VtWHiDAaVn5Usw6BkgSq9DTJQOm5fPxS3ZqbjhZx+K5vn9N/KQmTKK324AeGh2Jt471ggAiNYzGLDL3zMvv3Uy/xvVAXDIzkVI4MXH6PHy9+YF/G+8p2swyjwiJMS0EK/VQvc5QgghxJMCowGjY6NQdr4NX5o+FqsXGtHdZ8c7VQ38YA+zJiSjp9+O1z6/GNQBIHIMo3F9wI5XD1/Egilp6Lc7sGl/reI6C4wGZ1srG/xqV/7EFPT02/mbUrWmpg+2M5j7o8BowPV+G0rPWlE4f6LiegqMBnT12fD+sUaP7eHme7faub9WLzSip9+O7WWXgrIddgcwIWUUVi808jWP9DoGPf12VNc5M5O4JJv5U1LRb3Ng1/HLYIEhBY4A8IEjQFwwu8BoQE+/XTZwBABJcdFYvdCIzl4b9pxsxh1TnftE6bLyk1NXsGzBJHz/7mkKc8jPL9zXwnjJLdkpfKBLDhc4AuD1mH122plRdFKyrWXnr0KvA443dOA7t0zEt+ZO4t/rt7FYWZCNAqOB/758XtsGAIqBI24b+mwO/rNybp6U4vbeXblj+f8rBY4AiH6jFDgiofTEHVNCXnKEgkeEhJA/RRkJIYSQkchksfJdUD46cRlFpYO1RrjBHsrOO28eY/RMUAeAOHLBuZ7RsXocPOdcx8wJSYrrFE7zp12f1zrnj5IWfVHZzoQYfVD3h8liRb/rhrqkrE5xPWr3g3Q+uWMdaPVXr4vWY3fIBwgOu45FZnKc7PuB4m0b23sHRO3lvoeevF52Cb/56JTqNnja10cuXMXRumtDXo4nyXFRsLkiMG+UX8Krhy+I3t9qugCTxerzshnBZ+UcvXTV7b3Paq6oWnZiHJUQJuHx8sHzIR/0iIJHhIxAGkh+IoQQQhRxtVeyDfEAgEfmZInqo2wszMeabZX4t78dBQDERumxsTBftlZQINry3ztPAHB29eGMS4yTXae0boyv7TJZrPjRW8cAANGCisLePm+yWPGzd53tHB0XFdT9sbakms9i/vOjN8uuR+1+kM739OIct2MdiO04LOlutXiaAet31+BVk7OmUK9NPm/EVb8aVzq9F8D21XFX/SXpPpDDshDtly9PH+d1+ffPHId3qxtl33M4WFRdEtdl4va1nLhonWKXNamnF+fI/i68ae+18d+rr8wYzw/wIrSq2FmPyhfjkpyBvzXbKvHKwfNu7//8/ZNuy+S6rHlDj4VJuPT02/HEqxUhDSBR8IiQEYxOeIQQQrTIXN+OjYX5SIxzjoDkcLB4dkku/36B0YAH8jLwlRnjRNOCMQCEub4dv3p4JgBn15nCBc6uNK3d/bLr5Nrub7vM9e34w7dmy0739rnnHpnp93rV4rZP58qKui0nTXY9aveDdD67qxZRoLej1tolej0vOxXPLsnluz7FRsnfFs12FaMdnxw7pPXLOXfF2SbpPpATo2dwV246/9qhog5Cw7VezMhMlH2PBfCFpNsYt6/lbFlxi+pMOLsDsr8LbyanxvPfq8Zr17HwBnGXnM3L52La+EQ8kJehqh0cFiw2L5+LB/IycEjmRnvxjeluy7wpQ36/SfUpBB0JCbbU+GjcMG50SAc9ojw7QkKMpbyfiBCqIY8JIYS4G/w7exoA8HD+BMydnMIXzwWADUvz0NE7gHeqGvhpXD2UQLelqd05vPfo2CgY050jZN2UmSS7TrlzhC/temqRET39Ntnp3j53WTIKWLD2hxCrsB61+0E6H/daeKwDsR3Lb83Gz98/KZq2eqERj86fjJk/34MoHQO53KJ7Z4zHsfp2zMwcg/qrl2Xm8N8j+RMAeD+2AJCVEo9XVszni07fM30c3j/W5PEzxSvno+HaddzuKootpNcxWHbrZPx1n3joeU91qdYsysELn1lk3xfitkf6u/j1hzVKHwEAZIyJw+WOXtgdLF767jxc77cj/1f/FLXhvWfuAAC8Ue5eBFxJemIs/x0asDvcCmb/8MvTMHVsomiZq+/MwQ/fPOZ12YbRsWi4dl11WwgJlKr/d2/I10mZR4SEkNYyfSiMpSxUQx4TQgjRvlAPdqGFwTW80do1TbBwRa21VrZSbR3NQI6szQT5qLNs8K9NA/3birSRywkZCso8IoSoNpKycbjU7ae2VeLeGePwqR9DLRNCCAkUbdygaS2AIBWO7Obhcu+stBlaGexEC7uZsucJGdko84iQEczXy6FgZuNs2m+RLaK5ab/39OhgKTAa0NFrw9t+DrVMCCHhwjDMfQzDnGYY5hzDMD+VeX8SwzCfMQxTzTCMmWGYr4ajnd5o47Z9UKgCJUONVwQ7QyQSDXWfDpcgGU+D2xOsJvlz7NR+RoO7kZCgoeARIUQ1LhvnX16vwn+9exxrS6oDlo2jxW5i/g61rMVAGCFk5GAYRg/gBQD3A7gJwKMMw9wkme2/ALzFsmw+gO8A+GtoWxlZNJJ8QggvEF9JX7/XQQ9KRmAkZtgFFQnxgIJHhISYFk4yQ2lDgdGAa9cH8HrZpSFl40gDLAVGA55enIPlL5dj5s8/Cmhgyh9DGWpZi4EwQsiIMh/AOZZla1mW7QfwNwAPSeZhASS5/p8MQN241CQihKJ7UaQF1KTN5bqjqa1ZE4zt1cAlISGEqEbBI0JCKNIutOT4m40jJRdgeXFfLewsi64+e8C6ifmbBTSUoZa5eVe/WoHZv9wT9kAYIWTEmQBAOBRRvWua0C8ALGMYph7APwB8X25BDMM8yTBMBcMwFS0tLcFo65CE6oFMpBTMpu5qykZqoCaQ391gX8eyCEHFbLn1DmGdVAeKjCQUPCJkBPL35D+UbBwpYYDl3j/ux9qSajy9OId/fyiBKSF/s4CeWmR0C/YUGA2qC4MXGA3o7rej/bqN6iURQrToUQDFLMtmAfgqgG0Mw7hdF7Is+xLLsvNYlp2Xnp4e8kZqWSgCSpHw0ClSb55plCztkR6SQH3/g3mo6WtERhIKHhFCVBtKNo4cLsByprkLi6YZ8OK+Wv69oQSmpOvgglQ/eedYyLKAApWhRQghfmgAMFHwOss1TWgVgLcAgGXZwwDiAGguyu3tvixUwRXherQc0AllICfSspz8ba1Wgkzu3e7C0oyg0sae9k0ktpkQf1HwiJAQ08I1iL9tGGo2jpQwoPLRiWZR5tFQA1PSNnb32/HmkfqQZAG5ZWg9GphAGCGEqHQEwA0Mw0xhGCYGzoLY70vmuQTgHgBgGGY6nMEj7fVLG+EiofuaFq5r/MH4GH0JRrBGuEitBKk8CXW8KlC7JJhB6Ag4bIQEDAWPCAkhrTyle++o8wG08GQZ6tHApAGWLSvmiTKPgKEFpqTr4oQiC0iaoXWbMS1ggTBCCPGGZVkbgLUA9gA4BeeoaicZhvkfhmG+5prtxwBWMwxzDMAbAFawGrx71cZZM/QiIqskEtoopLBTw/ml92Xd/rZT6VfNgvU58BHsfRWKP0Fy2Xlyq9XeX0NCwo+CR4SMQMb00QCA9usDAMIzGphSFzg5agNbcsWxi0otWFVcwb8OVHc4T6QZWiwbuEAYIYSowbLsP1iWncayrJFl2edc0/4fy7Lvu/7/Bcuyt7MsO5tl2ZtZlv04vC32T8gLWVMnlRFncFS28LZDGljRygPJQKJfFyHaRsEjQoYRtSOLzXIFic40d2HJX0rDMhqYUhc4jrcC15v2W7Buh1m0vXodsHLrEazbYeY/+4ePz+JH994gWgdlARFCCNGycAcqyPChlMXmT/Ap1OGqwBXMDuYPin6sZOSICncDCBlpgvnUkhtZjAsEcYEXaUaP8Bx6srEDP7h7qt+Bo037LcjLSnYL/Jjr24eUafO9V8pxS3Yqai53yga28rKS8cJn57DL3ITNy+cCAP6y9xx0DPBWRT2a2nthrm/HlhXzUGA04LndNfxnC4yGkAbK6LKCEEKCI9Tdu0Kd7REJ3dciJdDlbVdqfTukNZrUfjc8dVvTGpYNT7uGskatf28ICSTKPCIklPy4CJTLsDFZrFi3w+yWUcRl1TxeXIGZP/9IdUbRUOoAcQErb5lCvhqwszBZWhULXBcYDdi8fC7sdgcKi8rwWFEZAGDLiltgd7DYd7olJMWxOXJZX4QQQoYX8Y2tdu8aQ3EDHgFxrYAKRiDPl8CDBkuSBZxwG8O9ucN/bxPiOwoeEaJxeVnJ2GVuwpptlTBZrDBZrFizrRK7zE2yAZoCowG9A3Z09dkVgycnGsRdtoZSB4gLWK15rRIL1n+CtdsD2wXOU2CrwGiAITEOgPMkv7IgW/Vn/eGpW6BcEI0TiAs+tV0SCSFkOAr3jaQw44hGMhPTYgaLHK1ncQXrWAf0+IRgJwp/X5Hx/Sdk5KDgESEax2XYDNgdWPZyGZa/7Myw2bx8rmyARs3IYpaWLrd1DKUOUIHRgAGHA80dfVg6Z0JAs308BbZMFisarvXwr4tKa7FmW6Wqz/rDU5YVtw+f2V6Fb206jGe2VwVknWrWTQghw5W34dQj4ebSH0O9Rw9FgEvrwRi1wvkdGi77MFBYRE4wkjMSMsII4VDNI0JCzJ9zzMSUePQOOPjXKwuyFQNHa0uq+ddc8ESaCfTQzROwveyS6LNDqQNksljR52rf21X1uHv6WNGyPNVF8kYY2JJ+3hkoYsA997GzAOwO2c8GAre8J1+rxNjEWFy7PiDatwVGA3LHJ+FwbStuN6bhkKUVQGCeSnHr/pfXq3D/rPHYc7I55EXOCSEkkOTODdyABxuW5onn3WeBjT3Hv+bOIYfOuR4OCG7C1dTd49YtPD+YLFb817snkDQqCtMzkvg2FJVa8Kd/ngUAdPfZ8G51PQCgpqkDK7aW48k7c/DBsUZMTkvwWutPuM2b9ltwsbUbOekJOHSuFU/emQMA+OBYIzKSRwEABgTnNDVe//wiAKDPZuenrdthRnNHL+ZPSePb52kfeTsu3Pt2x+DZTW553Hycn71rxpK8TJjr23GxtRsfn7yMGL0eh5+9R5RFy+1T6XJ8raXIfe6lA7XQM0Dd1eui99+urMM7lfWYmOp5X5edd57L+e9aAL24zwLD6Bhc6eiFyXXNoORCaw/ueX6fT8t/vLgcN4xNlH1vwM7iiVePiKY99MJB3JSRJDt/UakFrx2+oGq9t63fixszEnH71DQcOteKjOQ4VZ+ra+uBzfW9eur1ChhGx4re574ba0t8e0DX3WeDyWLFSwdqsfL2bLf3v19ShZ9/bYZo2m8+PKVq2dzIxYSEGvd7VXPuCRTKPCIkhPx9wHS4VnxBsdV0QTabxlzfLiqOHYqRxbiAVWKcMxa94ZFZbtk+Q82akRvm3lzfjgfyMjAjc/Aip3jlLXg4f4LXzw5FgdGArj4baq3dbt0CTRYrquuuAgCqLl0L2DqF6752fQBvlNeFtJ4TIYQEA3du+PB4E6xdfTBZrNhlbsIucxMOnbWCZVl09jpvzEy1rcgU3IA+s70KdW3dKD3rPK8M2BxgWVb1+YVbt15wJbx2ezWmGOJxtK4dO6oacPCsFUUHLHhudw16XcEYm53lz6nVde3ITI7z2JVcab2mc1bMykzGzuoGrN9dwy9nVfER7DI38e3S63y7cpjuuvG/1jOA0rMtMFms2FndgM9qWqBnAIfD+z7i2njQdQyEx+XgWSvyspKxZlslBuzOm/zPa1tll8cth7O9rA7fe6Ucegb4e0UdWrsHYO3uw6GzVtENCbcvOHod/Mq0vdTajbXbq9He04+9NS243m8TvT8pNR5TDPHYf8ZzUKi9x/kd7Lxu8zifP/bVXMFbFXXO9g14DxRaWrr5/3/yRbPX+T+tacHBsy2K79dc7hS9PtnQgXcq62XnfW53DbLGjPK6TgBo6+nHhDFxWL+7BhPGxGHXsSa8U9ng9XPt1wf4h6z7z1ix86j4M5nJcVhVfARt3b4FbBquXcea1ypxuzENa7dXu71f29KNVcUVomktXf2qlu2gxCMSJjVNnarPPYFCmUeEaJzJYsUv3z/pNn3Ntkq3rmtyQZJgjyzGBay+77pAnJed6pYpxAWxCovKMCMzCU3tvX5nzXBPErltvev3+/j3uG19o7xu6BumQNot8FZjmmhku/tnjse71Y1YMisDb1fJX4AFet2EEBKJuHPDd7eUY3xyHHr67fzomYVFZbhx3GjUum6Wvz4nC9tcWTUAcLVnACVldfj3e6fh9x+fQa/Ngfv+dAAtXf2qzi/Cen2ctp5+VF26hodnZ2LnsUYs2+LsJh4fo8fvvpGHZ0qq0WsT3+CXlNdhdGwUXvqufFdypfU+XnwEvQMOjI7Vg3UtR8+4MmgHHHi59DwA34NHcyenAHBmvC7fUo5oPYO4aD2evXcafvfRGTz3jxqkJsR43EcFRgOe/2Yelm0pw4IpqTh7pQubl8/FuStdWLalDI9IHtL8+K1jeOGxOW7L45azUnBTPmBn8dw/nKOf3pObjr01LXjMtZ85JeV1oodtL+6r9euaocR1LdDW0+/KPOoVvb//jBWpCTHIn5iM6rp2PuNF6pgrWKjTAQ7fEsG8GvAx8vCzJbn86LE7qr0HYwDgZFOn4nvS1SvtA86Jxg5V6xywO/DhiWY8uyQXf9l7Dp196gJvo6J16O4f3MnSZDDuuyHcD6qwQGefDS+XnpftFjfgYDHgsMt80LuEGD26+/37LCFDEROlUyxjEiyUeUSIxpnr27HoxnTRtM3L5+KBvIygZhSp9dQio+wFozSQxc1zsrHDp6wZaYaVXgesKq7gp0svq4M54plSt0AuXX9jYT4mpsQDACamxvPzBaI7vKd1E0JIpCowGpCeGIv6q9f5cwN3fjjd3AXD6BgAwJXOXrfPThufiO8KBko43dzl0/mlwGjAFEOCaNqSWRn49vxJomlP3DEFcyenKi7n6z7W+iswGjDgCkJ997ZsfrpdcK749i0TVS/PkwE7i5UF2Vi90Ih+1524mn00PcP5JLvsfBs/f3ef8wb53eoG0QAV375louLyuOVIZSbHYcuK+YrrF542A5Fpa1c4Dy9bMAkFUz0ve4Ir22ZyWoLH+UJh9cLQdE0ZKgfr3LerFxqRY1C/34SBIyW3ZKf4vB9SEpx/R5o7+0S/uUCI0tPtNAkPpTImwUTfdkJCzNc4wlOLjPi3L01zm75haZ6odkG4R93iij4qFTpUU8hbjrS724v7avGje2/A41uP4Kt/PoC6q4MFs3/27nFRgCXQPHULlAuihWrdhBASqUwWK1o6+5CVMoo/NwjPD1ZX15G2bvcuJGcud+JV0wXRNF/OLyaLFaebxVkZH5ibsKW0VjTt5YPnUXmxTXE5b1fV+xTIN1msfDBDqYbMm0f8y6CVPqzQMc6u7kWlg9cIavYRt70JMXp+/outziywOZPGYKtgv795pE5xeUr7rbG9F6uKy71tjur2+uv1skvYbW7yOE/DNWetpAut3R7nCwXhcdS618suoajU4tY1zhM1cZgjF676vB+uCv5+qK3bpNaAjbKOSHgolTEJJgoeERJCSqNq+DoMOxdQ4WpEaGPULeXU+qFkzWwszMcTr1Zg3v/+ky/+vXqhEb02B75o6kSmoP/99rJLWLZgkoelDY3aLCvAWfSRw4IdcoDPl3UTQkgk4M4NxvQETEqNd3Yj21YpqnczxeDM4qyWqSN3d246nv/4jGia2vMLt+5ZE8TnzpsyEvFJzRX+dfroGPT02/Gvf/PwYIJ1diVXc07j1hvt6o4mLDot7KL2xMIpAJw1lnxRefGq6PW8ySmw2R1YL+ji420fmSxW/OzdEwCAhNgo/ri86+omlSmpe/P7b86WXZ5wOULjk+IQpQP21ijX4hF6enFO0DJt8ycm40Jrj8d5Zmc560ixAe6y5g+fumqF2f0zx2H97hpMTov3PrOLmvrwcdE63/cDI/xvYIe461FRq4qQYOi3OVSfewKFgkeEhMim/Ra3ERm4gIKvBaW5Eb/m/e8nWLOtUvOjbg0la6bAaEBPvx3Wrn4+dV34R7Lxmnj0lNcFo8j5GpSTkptP7R/o3ccHn2QqFRMlhJCRjDs3JI2KBuD8e/9AXgYeyMvg50ka5exuMi4p1u3zY5PisPAG96C6mvMLt26HJFWn/up1TE0f7GZjSIzDz5bkIjZKr7is9Y/MUt2VnFuvXu+8gX0wL5N/76aMwVGxuJtoafu8OdUkrkmTNjoWD+dPwF25g93fve0jc307nntkpmj+B/IykDve2b6Wzj6+NhUAzJ+SKrs8c307/lewHKFvzpuItIRoVdtkdyBombbnrT3I9hLcSI53fgcTR4W/VKwxPfhd56J8rLOlpOFaL55dkst3dwzUuresuAWpKr87HOFob/8nuB4NhADtLkJ8lpuRGPIyJhQ8IiRE8rKScba5Cx2uAJIwQMRdyC3fUo6CDXv5DBulgJC5vh2ZY5wnwpmZyXxAJdxd1wDI9ssbStaMMFjDpd4Ln0pzNYY4wiCVNCgH+DZii3Q+aQaVnMpLzqe+X501fnCd26vxlRnjNB3gI4SQUBOeG7gYyYalediwNM9t3hmZ7n+3vzZ7Al54bI7bdDXnF27dMyWZR1tX3oJfPTyLf82yLFYvNGLvjxcrLmvO5BRRV3I16+W295cPDQZXUhIGA2Sr7nBmHsVE+XapvuzWyW7TNizNwyuS+kKe9tFTi4xuNZ42LM3DfTOdQb2bJ40Rnc9YheU5l5Pitvz0xFhsWJqHyv++V9U2cfssGJm2n/77YiwRBCvlLJiSJvo3nDx9DwMlLlo5UOqL4pXzsXqhEV++aZzqz8Sq+L4XGA2oUvnd4cTHDAb+bs0J7HEULjvUfnp/btjWTcLvvWfuUH3uCRQKHhESIgVGA9JGx6Dmcid+8f5JtwBRgdEAu4NFY3svn2EjlzkDAOXnW3GmuQsAcLTuGopKLT4FRdhAVHCWUOqSN1TCYI3DweI3H56GTZDXLN0SaYCGy9Li+BLEEc73/ZIq/ph5MjbRGdTrFowsMmB34MHZmUof8UjpO6CJQCEhhASAmm4kcucYpRp7Q+Xr+Wwo579AnzuDdS4G/Nvfge4iFAxBuCQiJCS0/+siww0FjwgJoeaOPgBAsemC2+ghcgWlucyZo3XiWg+HzrXiNteTk/gYPZ7bXYOU+OhhmdkiDNZ099thc7DInzT4JNNTX3ou8NQlCOT4EsQRBmg+MDepqqfEjbL20clmflr0EEbikMue4qYTQggZGrmbr1AGE7QauPAUhJIGhLS6DWqpbX6Eb6ZqFJDwTTAeyKoVzGAxIXKCFjxiGOYVhmGuMAzjXilvcJ7FDMMcZRjmJMMw+4PVFkK0ZsGUVNHoIUoFpbn//++uL0Sf77MNZt60ukaQ8KUgIRPEs02gT6HSgFhiXJRo5I6LkkKXwiDLxsJ8PF58xO91SwM0W00XRF3m1Po/HwqESxUYDVj/yEwUFpVh5dZy0XRCCBkuvGW1KN2fDfW+zdvHufOlx2CKH+fUYJ2GtRbIkTuukXrDG84gQSTTwn4LZgvCuXWRkNlHhpdgZh4VA7hP6U2GYcYA+CuAr7EsOwPAN4PYFkLCThg4uDUnTTTaiaeC0gVGA5LjxYUBf7YkF4drW/nX8TF6PLEwx+P6hd2fgtJtLeBLlBcXrRPtq8Q4cV9zYRDuZGM7eiWjYARyxBa5LmX1bTKjtrBDK/Y5LsnZFe6z0+pGpiGEkIiiqfsfRhTc4M6Xobj/Fe6G8N9uO2mlHcGk9uungRgIIYSEVdCCRyzLHgDQ5mGWQgA7WJa95Jr/iod5CYl40sCBMEDkqaC0yWJF07Ve0XtxklFfHro5U3aoRmG3K6XuT4EW6IsraXtbOvuxr0b5z8XTiweDaL/bc8btfV+CONL5VhZki0aYkRslb5e5CVI/fOsoAPhd0O54g7Md4xLdRxsihJCRKjhP3cMXIYjY2ISHhg+nzIiIPT5hFsxsdy0IZ1BxmO9aokHhrHk0DUAKwzD7GIapZBjmu0ozMgzzJMMwFQzDVLS00JN3EpnkAgfeRg/hurPdOD5RNH3rofOi19wwoR8caxRNF3a74oJVT22rxGuHL/rafK/kTmCBKPYsN7rZS6WD2y8dlvjFfbX8/3UMIFduSG0QRzrf62WXRK9vc2WQPVZUhiV/KcXakmpMGzfabTl//NbNfmcdmSxW/H7PaQDAeMFQs8EOAhJCSCgp3YBxpxa5t1mwQb95UtNtLSjrDe3qgiZYRc3DQQvdr4a7SNzDw+k7Tog34QweRQGYC2AJgK8A+G+GYabJzciy7Essy85jWXZeenp6KNtISFhx3dnGSLqt1VrFXaNKyurwtdkZeKO8Dkv+UspPl8tm6ui1YfdxcXaMXJDHZLH6NaKX8CQaiGLPj86f6PH9mzKTRK+FmUfRep3bDYkvQRxpu4W1qADnzU6B0QAWwMnGDixbMEqmX7kAACAASURBVAn3THcfknZ+TqrfWUfm+nb8x1dudK5PMp0QQoYDbQVK/KhfFLY1B48vbfH15llL2+mLERMiCOMBCtaqKfBHSGCEM3hUD2APy7LdLMtaARwAMDuM7SEkYj27JBcNrq5tJxs7FOdTylaR6361tqTapyCPXGo6l+309OtVWLOtQjRdrRJJto83wsyjB/Iy4JBcL+RlJWPF1nJVwTKlrobSz3FeL7uEOrmaR0Pw1CIj8rLGyE4nhJDhQunWztstX6TfE1LWQvipHm2NDpVfhnvgZphvHiEi4QwevQfgDoZhohiGiQewAMCpMLaHEM3hgjrXegY8zrd6oRFP3um5YLZ0RDchLihSWFSGub/6J9aWVGNjYT4f5Nm034J1O8xuAZd1O8xes5MKjAa0Xx/AHsHQ9b7403du9vj+lfY+0Wth5tGDszPd5l9bUo3bp6apCpYpdTXkmCytbqPkfXjisttnAnVh0dVrE732NzuMEEK0RE2XMLlZtFLzaKTXHRkpN88jZDPDivYxIdoWtOARwzBvADgM4EaGYeoZhlnFMMxTDMM8BQAsy54C8BEAM4ByAC+zLHsiWO0hJBJxQR1P2USAcmBIGOyRjugmty4AaO3ux7IFk0RBkrysZOwyN7kV5d5lbnILuEgvItftMMu2S23Q49acNI/vW7vFwSNh5pFchtPGwnysXmjExsJ8PPlaJb69+bBbsEwtc/01t1HyvjprvE/L8MVFQVaTP9lhhBASqYJ1UxnuwIdSAIxuorVjcMS9kXFUwhkLjcRdHIFNJsRvwRxt7VGWZTNYlo1mWTaLZdktLMtuYll2k2Ce37EsexPLsjNZlv1TsNpCiNYITzTe6g2pCWgoBYaE3a7kRnSTrpPzetkl/jXXjs3L5wIs8L1Xyvn5Ni+fyy9T7smr0uhja7ZV4mJrt+x2+8qQIB6BzFOADBjcnwVGA7r6bCg73yYKlikV+Zbz5J05bvt0Ymq823yBurCYJFi2vwEvQgjRJD//UAY+6yd844NFagaTz4dOYxuqrdaMbMH6agi/owEPUFH0iIwg4ey2RsiIxgUp5IpKCzNK1AQy8rKSZYMIauviSDOXuMLQwvYBwPSMRAzYB8+ScuuUFnUWDm3PyUlPQE56gmidRaUWrCqucJvXm7HJ4uCRt2CKsKsaRxgs47a39GwLOns9dxcM1fXCjqp6AMDo2Ch+2qJp6VQ0mxAyLHgK1wz3G3ut1zwS3mhHYlaIGmrrag3X7R8RgnjstP4bJiSQKHhESJgIgzJ/+vbNeKyojH+PyyjxVKdI6HuvlON/Pjjpd1ukmUtcdzkuOPGVGeOwqvgIyi9cFX1OGICRu8BXynb6yX25eHFfLZ5eNFifaP3uGvzo3hvc5vV2sZYSH6P4nlzgbW1JNYpKLYrBMm7bVxVXYNYvPva4brm2yd0EqUl1l6srtW6HGY8Xl6Opw1kM/UpnL//ezuoG6OkvOCFkhBvqDb23TIdgBa9Cld/kdyaHh89Jl+lrdy6tBQTVNn+kBAkYjWWGaR0FFclIQrcehIRB5cU2AODr7nz3lXLZSxJvdYo4A3YWrxy64Hd7pEEe0zkrFkxJ4wNcOekJuD7gcPuctAaSLzYW5uPPe8/xrx/On4DVC30fQezQOfH6hbWU5DJzNhbm49C5Vo/BsgKjAf129+2VCuSFpLSulMlixc7qBnxW04KJY0YBAJo7Bus7PbvEGYDjujiqGT2OEEK0ytvf02DdoHlbLve2p9tpfwJBoQpE+L3fgpqpEVkirb2RjAIxhGgbBY8ICYOxiXF85otO5ppTGJQ52aiua9KMzKQhtUkYfFj1agWMz/4Dz2yvwsbCfNS2dMt+5oG8DLcAjfQJpFL2DwDkjk/kp+0/0yJbXPvz2laP7RZ2owMgKiCtNFpa8cr5bhlRBUYDP78vATE1ARpv10LCulI2uwPLt5SjsKgMdtYZJHq97JLbZ7ii3+b6dj7I91lNM/psdiqmTQiJKJGQ6BDqe9qRUpxZC7x9/6jbGiGEOEV5n4UQEmgTU+Pxh2/Nxne3lMPmcL8asdkdWFVcgaljE3CiwfNIa5y8rDFeR2VTuhi9bcNeUX0frjuUMX00PjjWiF3mJoxPisPljl7R5zYszeP/r5TmLJf989SdOSg6UIuKi4Pd4J5enIO/CDKROD9886jyBskQZVD5kRWltqsg4LyQlAZo6gQjoqnFBX82FuaLMrzys8bgrhvH4bndNbLtLDAa+O3dWJiPwqIypCZEA2ComDYhJOJs2m9xq+HX4aX23NZD50Wv1+0wIyc9AXaH8wHCuh1mNHf0Yv4U58idF1u70dzRi3FJcdiwNA+X28Xntbcr6nBX7lj+9aXWbqzYWo7ffD0PShhmsO0fHGvEvtMtmJ4x+HDknuf3gQGQEBeF7l4bmjt60ev6W7+ldLD9V3v6+f8/s73KuWzJ/vFWy/D1zy+KXte2dKGo1AJhMq3JYsV/vXsCjy6YCLvDuU8enJ2Jk43tOHSuFRnJcbgkOZeZLFaUnZd/mMNdWWzab4FeBxw614on78zBSwdqkRIf7Tb/mcsdeOiFg0hLUO52LiX8bgj3NeC8FuEePn3RpO6aifOl5/ehSfIdkPrTJ2cAANUXr3qcLxQeeuFg0NfRcd3zb06tgg17MSpGj+sDdtWfUTOvyWLlj71atdbBh6CzfrHHp896I3cdHypvHqkL27pJ+K0qLofJ0ooFOWkoXjk/JOukzCNCwuTgOaviCcfBOk+gx1UGjoaqqX3wQhYAuvucJ++ay51odgWM/E2xl7vQXf9hDT473YJo/eBl8Yv7avGDe6a6zfvHb92sel3S8JVwZDi1PHUVlHYFe7m0VnST869/q8bu4+6jy3GUupdx63yi+Ii4LQ3t+Fu5e9YRALci61w72roHRKPHEUJIJOCC8dK/beeudAEQB1Y4DAPMyBQH8HdWN2D97hrodRB1/9XrnMvfWd2AT2ta8N7RRhSVWrDv9BXR5zcdqMUu8+CNaXe/HbdPTfPY9ooLbcjLSsaabZXYWd2A1q4+fFrTwr9vaenGuZZuHKtrx7mWbnT3D94gC2vXnWgYfNhy4KxzH9gF1wlqsklzM8RZyAzA7w/O2pJq3GZMxfrdNahr68YucxNWbj2C53bXYMKYOOysbsChc4OBIu6hSkbSKMXt57ZlvWsZa0uqoWeAd6vdb/J7Bxw41diBzwT7yBvhd0O4r3eZm1BUasEucxPeO9qIU14eoklJj4cc7ghct3nvzh5sx0MwUEagQiGN7b2wtHTLZtgPxZptlfh7hf9Bk3AGewLtrOvvIxmZ9ta04PqAw+s5KpAoeERIOLAszHXKFwB9gguUQJ7iPKVc6wSZQ9xs65fOwmFLG742O0NUb8ffdUjddePg092NhflQUWbI87olr6Vd2tRQKvINAE++VoFp40bzr2dOEF/Iv3e0Ufbintsn0hsjafeyHkldKZvDgZcPip+qc4Q1mrhlcYSjxxFCiNZxpx+u/txT2yr597j7PHP9NbfPsSwwPztVNC1Kr0OUjsHv95zBd7eUI0qvw7NLcvGnT87io+OXodcxiNYxGLA58NzuGgxIbiT1DLDt88Gg/bjEWKxeaPRY1eg/3zHjncp6vr1fvmmcx+1lWSDKdUf91321/HThOZB7uNIruB5Q81Bg7uQU0etaazeeXZKLP38ymNm7sTAfzz2Sh2eX5GJ7WR10cF536Bjg7xX1iNLr8K+uhznWrj4+M3ZSWrzsOn/81jH8x9vH8OK+Wjy7JBe7j19GVsoo7K1pwdT0BLf5GcZ5zTEvO0VmafK478bq1yrw4fHLAAC9Toec9ASs312D2VljnMd2mI8kEWlxDx2A+queM7t81dlrgwbieJoQ6MAciTw/W5LrV81Yfw3vv7CEaFTd1R7VtYxyBcGKYJK7iKtp6sDSOZnYUeVberBaB84OPnUsMBpkAy9r31DXhYwTzKBJV58djxdX8K8XTHGP9Mt10xO+9/TiHDy1rRKPFx/B2pJqPL04B+b6dtkU7Nty0jBf4eJaWqNJafQ4QgiJBNw9cYHRgI5em9v7WSnyWS9ScyalYMDBos/mgM3B4uGbnYMxdPfZ8drnF3FLdioGHKxb0IgjfeaQNjrW6zq7++x4p6oBS/Iy0GdzYLcruKFk6tjRfJCjq899WwH/Hn7ImZQWj9ULjaL1cEEo7oaj3bW/HSzQb2exsiAbj906mZ/fWzZrd78df6+ox7IFk7B6oRFJcdH8ufCcTM3EMaOi0Wtz4MgF37qBFRgN6O6zY9vnF7GyIBujY/U4VtcOFs5s7pUF2V6ziEhoUYwnuCItmEgCa352SkgDRwAFjwgJi6pL1/Cl6Z6fTHJqmtWmpA7tDGKyDKaocw8ycjOS8OGJZiydk+nhc86uXNyT4ypXTQCui5baAIbJYsWKV464Te+zqb8QZOA5eBNoLFi37VuSlyE3I0wWKy62duPFfbXo6LXh05orWDTNgBf31UKvA/acbHb72IIpqfjlQzO9tkPa1U46ehwhhGiZcLQypXNG/dXrqpZ12CKuy7Ojqh5FpYPdjU0Wz4MwSLV2ObNu1Zxh369uAADZOj9CZ690YWCoqbYKpIkIl1p7RNsPDO5j6XTOVtMFbBfUTlKbzfp62SUUlVrQcG3wWEXJ3Glc7XHW1PE1a0LYhq2mC2juFGdEbzVd8G2BhBASwcovXFX8Ox4sFDwiJAzmTBqDvTVXvM8IiOoC+Up4oWWyWLHu3eOqPucsuuz09OIclJTJ9y2XG9Xr/71/Eut2mKHXOfulrxF0P5BaefsU/v9rS6pxxw3umTy+PH1loa4mRKCwrHuwanKqe1p/+fk2rC2pxoOzM/H04hx++s7qRtw8MRm1Ld2ydZYutHarGoVIrqudMDOJEEIigacBC2ZNkP/bXn6+TfRaOjCE3cFivWDQAYePj+qbO/vUX5y7/mBzwRHF2eBf3RU1AZxKSVHnHEOCaPsB5/n2Z++a3aZzbHYH/rxX3M1tbUk1LrV6HhDi6cU5WL+7Bklxg+PxeOpe5MsukPtuSLvJ2ykNg5BhY1KqumzTke653TUhDSBR8IiQMJiYEo8n7pjifUYAD+YpZ/14s2ZbJZ/9s2ZbJf7hoZiznFNN7bA7lIex5eogCAMXC28w4K0jdfj9njMYsDvQo5CSDwBT0we75C1bMAmvrHAfKeA/vjJNdXu5gJW/5Apae7Ll4HlVx/E/3znGB4deFNS3mD1xDI5cuIpdZvnj8tW8TOxw1dEghJDhjGVZjwMWpCa4dx9jGLh1AZ8uKRh9z/SxuCs3nX89QyEIpWR0jB6HzrV6rHnEWXd/LoDBekZKYqN0qpYnpSab9JRktDEWwLNLckXTNhbm47ClzW065+H8CbhDUICVy2Zt6vCc/WV3ONclzKqKj3a/1eAeivmSeST9bmxePhejY/WieR662f/rJUKItmSOoeCRN/fkpmNUtE40wEGwUfCIkDCZnpnkfSYAdpUVqLlR0YQcDhaFRWX43hbnqGN/fWyOqmW1djufmj62YDL0OuWng8I6CFyAyZg+GnbWWXyzd8DhVj9C6JxglAiltPjc8er2E+C8cO2UqZWhltxIP5784Z9nUPDrT0XT5AJt35ybhQKjga95xDnT3Ikf3DMVD+RlyHbZ+/U/TqGoVL5gNjDYZVBpFDfhyHCEEKJV3N9NTwMWyGFZYMXt2aJp0mLVP/zyNNGDiSWzZLoWezApLUH1EMizssYAAG7ycn7/w7dvRly03uM8ctRkkwprFQFATvpot5oYBUYDPv33xYq1MjYszcMfv53v9hm5On/S9q1eaES2YbBIdopM0G+iK0M3ITbK7T1PyxZ+NwqMBhgk9ag2LM1TvTxCiP8mhCCwM2eS+oL6I9WWFfNx6lf3qz5HBQIFjwgJk9OXO1XN523UFk7pGfeAB3eBNuBg8aXpY3Fbjm9DOf6t/BL+8PFZxfeLSs+7pUrWtXlOaxd67fAF/v9cWrzUv/7tqOrlDRX3dPWZ7VWqP3NFUnNBLtb3VmU9P7ywMPOocMEkvLivFg/OzkS/TP2L081duOMG+RspYZdBLugl9x4hhAwP1CVJK7wlDPky6iohhJDIQcEjQsKg7moPXjpQ631GiAuJepnRzZkrgwGqndWN2KIw7LtUmqvm0Uul55E/aYzifD+69wasl/S13eEqGKrG3bmDgbECo0GUlcPp96FgdiAUGA1IjPNc7NRXv/16HtaWVOODY42itPscw2hsLMyXHWmNc7TOfXhqwL3L4N25YxXfI4QQrfMWb1AbkFBTJ84vKparxVGzg7Y/hoDRYqMIIYR4RcEjQsKguaMPT97pHiiRc7q5w/tMcNYaknIIklnionXY8KF8cUypuGhnKnm/zeFxZJoZmclYNM2ADf+oQXO7MwPnkXz1NQf2nBQPZyw3+Mz377lB9fICwWSxokHlqD5y6q66Z17Ny07FxsJ8TE5LcKtZ8cGxRrx3VDl49LXZ8l0sMpJiMWB38FlGi6YN1vTwNqwyIYQMB3IxiEBnvfDr8GG5atowkuMngdp0CkIRMnzRz1ubKHhESBjMm5yC3PGJquZ95eAFVfNdlBkFZWLKYJ/kLStuQUKMuhoLfQPqsn3WllRjYmoCHCz4bldZKe6jjSn5yozxotdy9Rx8qXmkBlcfSK4ekMlixariCtFoc756XyEQxI1+JuxKduBMC041daCnX3l/76iSz+Q62dSJ771yBE++VoGNhfk40zyYZaZ2WGVCCIkU4bqR8CUYNZxudoayLdRtjZDhLRR/61T3vCAhRcEjQsJALjtFyU0Z6oInF1q73aY1S+rx9HkaM1fA2t2var6NhfmwOcTL3F52UdVnAeDOad6zY/7z7WOql6dGXVu3Wz0gLpBkrm/HbcZUtKrcfjnpiTFu04pNF/j/CzOCyi+04lhdO5Z6yNbyNvJwn82BX7x/Ehs/dR9WmQJIhJBIQQEHbaHjQQhRMpwC5cQ3FDwiJAw+ONaEvx2pUzVv+YU2VfPpZP6S35aTyv9/bUk1phjUZwWpcbKxHW9LhpJvc43UpoZwtDUlv/16YEdP2V5Wh6cX54iCOMJA0m3GtCHVZa276j7q3U0Z8llmbd0DiNIz2H38suz7atjsLM40d4lG9+EKf6sZ1pkQQsJNTfcjpWCG9yDH0O5ygnGTFMr7LgoCEUIiEQWotImCR4SEgc3B4tOaK6rmVVtDSK7m0djEOP7/GwvzA14I+nd7zmCmlyGJPdl6aLCAd6iyZGZmJuG3H51G9k9389O4QBI3GtqYIXRb08mc7G7JTnWf6BKlYzDgISNM6bqfWw13chV2W9u038J3kyOEkJFEC7ESVhOtGJohdVsLXDMIIRpEXcpGLgoeEaJxLZ3qulDJdbVq6RzMgglGAeUBmwNfNHV6n1HBnMkp/P9XFVfgp++Y3eY50aiuYLhaF9t6MGB3v7Q1Waww17djY2G+T9lTUjeMc88yEq5NGiSzO1h46kxok6siLlgml3FmF/RvE2ZSEUJIJNB6wEFN+7R4QxWOp/esl3QnyigghHhDfya0iYJHhGjcwXPqMnLk/sgerlXX5c1fLIDxSXFe51NyTRDwuj5gl+3K972CbL+XL6dfJsvHZLFixdYjKD+vPLKcWhYvXfGkXcnkAllC3moe2VwzCOejkdYIIZEklDcJoQhcqBptLfjNcK0nvLdgcvubb5PWI4aEEFkUAB65KHhEyDDRed3mNu22nDTR62Bcp12RFOX2RU2z95pHgWaXicasKj4CPQN8XtuGf38rsAW6AeDI+cEgnrQrGV07E0IIQlacRws1gBgmdH/7h0MXOkLICEQRKk2i4BEhw0T9tetu08YNIStIrRvHjw7q8r2lv/tKL1OUaMDO4kf3TgPLsmhsdy947YtbJQE7APiiydn1zpnhVC56LzMEx4gQQrTM33sEBpEZgNdCAIsQQvxFYZ2Ri4JHhAwTt091D1o0dwwtEKLGyQDXJJIK9DV2rszIZ3YHi7/sPYeH8ycgJX5oRcXluhkuv20yTBYr1pZUux2nlm7/M7eUhKr4OCGEhJPc+cFbYCaYD7O5ZYczOBToBy6hWjYhhAhRgEqbKHhEyDB2uHawho/JYg1KMEmuQHQgVVwIbN2mn9yX6zaNBdA3YEdOegLar/tfLFvJz3eewNqSamwszMfqheJuaw5vRY38IK2rRAghWqfVsEQwgk0ME9k3RlrpTaKRZhBCyIhBwSNChokL1h63aTcJsmxWFVcE5ULriyBnHv3gjeqALm9tifzy+u0sfvPh6YCui/PhyWYsmmaAub4dRaUW0Xte6mX7RVpXiRBCtEzNuSlcwSUu2SbQSTchq3nk54qGsr2hSlDSasCRkOEuFL89nVai1ESEgkeEhMim/RbvMw1BXZt78Ki6bjADZemcTDR3BL6LVLD1Bzi6kjxKvlta8qgo2BwsEmKjAro+AEiJj8a71Y3Ye6oZ63fXBHz5hBAyEsndWoz0AtFMBNxwjfRjRAjxLgL+lI1IFDwiJETyspKDunyHl4ux94828cO6j2Tnrd2y09uv2/BI/gTE6AN/trra4+wKd+TCVczITAr48pWYLNagBy0JISQQ/M+QCe55zZ8bGO8tYkLW5Wo434AN400jRNOo/NnIFfhH7IQQWQVGQ1CXr2MY2D1csnb22YK6/kgRrWMwoBBE2328CemjYwAEvu4RAEwbNxo1lzuDsmwprkD3xsL8kKyPEEL8daz+Gtp7bHj4hYP4T5m6dABwrO6q27R/e/Mobs1JFU17o+yS6DXDAI8XD45y+dd953xqW01TJ+b+6mPYHd7n3fjpWQBAk8zop0K/eO8kuvvtPrUDAGb9Yg+M6QmYnpGEyWkJKD/fiqvd/fz7RaUWvFvVIPrM8fprWLfDLJp224a9uDs3HScUup2v2FoOm2CD1+0w48HZmfj45GUAQPGhC9ghWQ8APPLCQSTHx6Du6uBDmssyI5hebnfunx6V+6Bgw16wAB6/I5uf9tALB3GhVfwwiAaLICQ0Lsn0dgi0l0rp4ac3D71wEDe5zgehKlnBRNrICfPmzWMrKirC3QxC/JL9091BW/ZdN6bjs9MtQVv+SDAqWofrAyruEPx0W04qctITsL2sLmjrAIAlszJwuLYVGwvzgx60JCQYGIapZFl2XrjbQcSCdQ32pef34VyLMxAQG6VDn03d3+EvTx8LU20ruvsGgxA6BhA+H7gtJxWHawcHXpgzcQyq6q4FpuES3MMJBsGpCaJnnHXyYqN0+MbcCW7nEgZAlJ7BgKC7t54BYqLE5zaunVE6QG5X35Objr01g9cT0XoGUToG/TYH7KxzH0fpGNlu5fkTk0Vd5mP1DPoUup9HS9rqcdt1gMMxuF9j9O7rl5tGCCHDVWyUDjFROmxePjeg1/uersGo2xohZETJMcQrvve12ZlBXTfDMCgJcuAIcGZQLVswiQJHhJCIMCktAdE6ZyckpcBRWoJ7vbpVC3Pw/Ddni6ZJE0uFgSMAQQscAVDMag2U2GjnZXufzSH7ECImSucWjHGwcHsowrVTKUYnDBwBwICdxfUBBz/Aw4OzM1H8+HzZzwoDRwAUA0f/n713j4+ivvf/X5/cyIXcN4SEBEI2hIAYiOGiCxHEivWAVbQ91igVVAQq5dfS9rTSb8/pOedbPac9X9vTpgVELhWLvVJbpYpVISJBrpEIEiAbAoRAkg3kfk8+vz92Z3dmdmZ39r6bvJ+PBw8yszOfz3tmdmc+n9e8L0K7WogIA4aGpYLcmMhwpMRKvxMkHBGEf0iOVc4f6k18kEVixOEL4cgZJB4RhJ/wtTv1RxfJ60gLjsL3fn+i3qd9t3b3Y9NS5ZAMbzIhKRqvH71CLvwEQYQEDM6Fl5nZyYr7zclJsd84wPhKwlA6B2LumGj/uS9syUqO8WiyEh3h2vRjcBhIkhW7WGXIQVJslNs2EESoIv75uPpb8hZCLk9fMmey8r09OyXGo3YDdMp8wipDjt9fFI+g00cQwU1VfZvzjTxASz4GAmj1wwNPjZrmLpy87Lu33gKzspNRVlqE9XsqSUAiCGJEoJTzCACO191UXD8SOVmnfA4ETvjpXNTf6vHo2dKrMSxRTGuP9Nm9s6IOrd39KlsTxMhF/PNx57fkDRbk+V6wOH5J+X5Wf9NxTjlnBOiUeZ2IMIadFXV+H+eTeEQQfsLXicx8USVsJKLVTd4X9A8O490zN3zeDweHQa9DWWmRz0VLgiAIT7lyU7kKppiWLnvh/9VDtfj2H0/7wqSgZMDJWyJfh80JvHW6ASt3HHO+oQ/pGxjCzQC+DCKI0czHNdoEi3Fj3fcOVBuuu3uXUwp9DmUGhzn6B4exZvdJvwpIJB4RxAiBYv21MZo0NoNe57fqCwRBEO5ys8u5B4nSwP9MQzuWTE/3hUlBSaSTB9iifP+EL8REhiN17Bi390+IDnd5nzGyWJNpmQkIG0XPcyK0iYlUn3LPyk50qa30BNtvLyE6HN74Gbj6W3J0PGKiIl37rYvtiI9WLgrvzvFGhgGJMVHQp8UhKiLMrXuQt+83zsSsMAbEj1G2c3FBGvRpcSjIiMeywgy/vigm8YggiFHFpNS4QJvgcz4jbyOCIEKIIoVcPXIKs5Ls1v38sVn4wdLpvjApKPnmffkOPy+e5J/8T6vmT8aRF+51e//0BNdzlpRMSZMs//X5BcgZBc9zInQpGB9v/fujf1msut2bzy9wqd3H5ky0/p2eEIPMJM9yAAHALx+/w6Xtz/3nA5q22/7UHJfazUi0HYvay8/H501UXO+If3/odnz4nUX44NuLcOH/PoCqH33R5TaWFnq3qM7JHy5BtAMRrnhSMo7/n/sUP9uxci4++PYi/PX5BXjpkUK/vigm8YggiFFFz8CQ841CHvJCIwgilCAXEoIgRhaMMdHfXmyYj9wxzrKCfwAAIABJREFUnlfPk5fb9fdTigXpc5HEI4LwE1vKjYE2gQBwva030Cb4nIzEGPq+EQQxKhjB8yiX8dXEK2gZbcdLhBRcdHPyqnbkxbYEguXeIb6fB4tNgYIH6YtgEo8Iwk8UZrkW00wQ7lJ5tZW+bwRBEKOMkSykjfaJJBHaMB99gUfy7yJYPW9GOyQeEYSfoKpXhL+YlW3ODULeRwRBhAJaJkBKuoivJmQEQRDehHtR2R3JIjFhI1jFMxKPCMJPkCcI4S8umTqxfk8lfecIghjReHNCFgo4m0yEytlwx85RdqkJgggyfPGuIlgFIkeQeEQQfsKg908JXYJo6RxAWWkRfecIgiACADlEOWa0iX4E4c1vvC9y4QTjLcub91FvNOWLc+TwWgbjRQGJRwThNyqMpkCbQIwSaOJCEARj7IuMsfOMsRrG2PcVPv8ZY+xTy78LjLHWQNgJeDZGDsakonQL9j70XCNCmWEKW3MZ+skHJyQeEYSfoJxHhL8IYwxvnW4ItBkEQQQIxlg4gF8BeADAdACPM8ami7fhnH+Lcz6Lcz4LwC8B7PW/pdpRmjBRzqPQxFvXja4+ETJ4UfARN+WtsCdf3Uo9Efe96nlENwuvQeIRQfgJyj9D+ItZ2Ul4u+o6XthbFWhTCIIIDHMB1HDOaznn/QB+B+AhB9s/DuANv1g2CgiUqBUqHgnuhK0p7RIih0sQ3g1bEzUWjJ6X3sKb+YBCMbdQsELiEUH4CfI8IvzFjfYe9A8OB9oMgiACxwQAV0XL9ZZ1djDGJgGYDOBDlc+fY4ydYIydaG5u9rqh5j7c2y9Yc+cEapoykieSBEGEHsEi2njj3ujvlwLBcebsIfGIIPzE2oX6QJtAjBLqb/XiO/fn46VHClFhNGFLuTHQJhEE4QaMsQcZY74eq30VwJ8450NKH3LOX+Gcz+acz05LS/OxKeqEUthBMIaAeJNAXAulPkPoK0GMcrypcwfLfUALwWJrsIhYrhKM70dIPCIIghhhxESGYXWJHhVGE9bvqaSQSYIIXR4DcJEx9hPGWIEL+10DkC1azrKsU+KrCHDImpaBfTAOov2Ns/MUqhMkghjpBIuIok7w3TuC7YVBkJkTMCICbQBBEAThXXoGhrFyxzFUXWtDWWkRDHpdoE0iCMINOOdPMsYSYM5JtIsxxgHsBPAG57zDwa7HAUxhjE2GWTT6KoBS+UYWQSoZwBGvG+8HGGOU+IYgiKBEkpvIq0mPvNgWEVBCUfAnzyOC8BMUOkT4k4MXmrEwP42EI4IIcTjn7QD+BHPS6wwAywGcYox9w8E+gwDWA9gP4ByAP3DOzzLG/oMx9iXRpl8F8DserMmDQpRATQiC37vBTGhYSRCjh2Dz8vE6QXp8ju7ZwXpNyPOIIPzE5ZauQJtAjCLGxUfh3TM38JXZJhKQCCJEsQg9qwDkAXgNwFzOeRNjLBbA5wB+qbYv5/zvAP4uW/evsuUfedtmdwjWQbLbBOh4SAIkiOBBfF8jxyNipECeRwRBECOQcfEx2L5yNtbvqUSF0RRocwiCcI9HAfyMc3475/ynnPMmAOCcdwN4JrCmec6WciMe+tXHuNTc6XTbqze77da9sLcKrx257AvTPGLAR9Uudx+pc/j5Lz686JN+5XxY3eSRN/WVFvtr6YybXf2S5dv/bT+aOnrdtoEgfE31DVtk8RtH1e9TL+ytcqndNyvrrX/X3+rGjbYe142TUfZhjUvbr9x5TNN2e0/WO99IxK2uPuvf//uB8v3sTyeuKq53xMHzTda/t5Qb3RoXv/f5DZf3cUT+D95B34D6s+Lk5VuY/18fKH4WyGgWEo8Iwk88ODMz0CYQo4ip4+Nh0OtQVlqEqvq2QJtDEIR7/AiAdZTOGIthjOUAAOdceVQZQhRmJeLCjQ5UNzoXj2pN9t679bd60NDquhDha3zlGdDQ5lgs8ZfnUfWNDly96b439eCw64aeunxLstzRN4jOXsUCgQQRdJQdUJ/sv1113aW2mjpsQmp3/zCGvPC7/+yaa+PE+XmpmrbbduiSS+12i8SUzt5BxW363Tjg7ORY69+FWYlYv6fS5TY6+7x7vxkaHnb4rBgY4rjZPaD4WSAL4ZB4RBB+gkKHiEBg0OuwdqE+0GYQBOEefwQgfjU5ZFk3IjDoddi+cg7C3AzzKp6UjL2VDd41inDK0DDHb4+6/vbfE5QmWRS+Q4QK0ZHqU+6tK4r9aIkyUeGuSQKrS7SNK1eXTHap3QiRGWq/b3ceF3njxlr/Fl6susrtExLc6Fmd3c/Og7PTHqHycAzknJLEI4LwExQ6RPiT8zccFWIiCCJEiOCcW18zW/6OCqA9Xseg17mdIqjC2OJVWwiCILxFcmyk9e+n56uLKK4KAbdlSkWMybo41wxT4L7p6R63ocQjxVkubT84DCREm1MyZ6fEKG5zmxdEHINeh9snuOa9M3V8AsZEeE86Meh1iAoPd7hNML78JfGIIPzEW6fp7SjhP1LGRjrfiCCIYKdZXB2NMfYQgBH1JqLCaHI77CLaiwN5giAIb3KrewARYQzRkWHYefgS4qNtdaqiRPcuV18un21ot3qkRIQBl0xd1uVIFU+VcMtqoVfxZtGRYfjH5412672BPOdRRBjDF2eMl6xjABbkmV8izMhMQHvvIKaOj8fVm8q5nM5eazfbLTqHURFhiLIcZEZCtN0+TFaVocJowrVW5fbl52CKxWvpQmO74vlRO+dKiLesMJrQP+Q4FG6rJbeRmgdSIKCnLkEQxAglkAn1CILwCmsBbGKMXWGMXQXwPQBrAmyT16gwmvDMruNu79/ro8TUBEEQ3mBwmGO+PhW9A8PoG7AJBWIPljW7TzptR5AOBKFicJjj3oI0CLfAwWGOJ+ZlIzJcWWQQBPphAEXZiRCnHXv0jgnoHzI3pE/T5sW07ZC28aU851FuWhzy08dK1j0wYzwO15gwKzsRZxracW9BGhpae6zHrBsbiXBZ5bqIMGCNyCsnjNlyId1ot88NV9Nky6tXYTRh/Z5K1dC1YW4+RwIXLftW1bejZ2AYsVHh+MHSAquIMuBCDrdIUZzailePYkjlEcYsxyS0/b0HpuIHSwskxxAoSDwiCD8xKdVzt1KC0MrhmpaAJtQjCMJzOOdGzvmdAKYDmMY5N3DOXSuLE8RU1bchf3y82/u7mKaDIAjCLyTFRCAmMgwpsZG4ZOrGpqUFmCYKNxPnOVpWmOG0Pd1Yc7RySX4aZmUnQp8WhyEOLC5Igz4tDosL0pCdEof/s2y64v4L8lKxuCANiwvSkBgbhVkiceRaa681hEurDHK4RlvI8LOynEcJ0ZG4LKu2mJ4YjU1LC9DWM4gn5mVjzuRULCvMwFiLp1b/IIdelLMoKpzhtWfm4XKLLWl/38CwNdzt+UX2oV7iap1V9W0oKy1SDRd8Yl42EmNt0eEFFrFrbHQ49GlxePWp2VhdokeOJVxQp9HTPzMxWiKchYeFqYZs35aZgPSEaKTGRZqv9bA0z1QgC+FEON8EYIzFAejhnA8zxvIBFAB4h3OunAKcIAg71i7U47/eqQ60GcQo4bbMeErSThAjAMbYUgC3AYgWXO855/8RUKO8xNqFeqxdqEfO9/e5tX9y7BiYOvucb0gQBOFH3t5QgixRhS/APPkX7nXi8dlLjxTijWOOE9BPThuL5s6bWF2Si7v06pXOzjYoiwo//2oRdGPHSNYJtuxaNRf/+LwRq187gZzUONQ0Oa+kuGvVXE337UfuyLLzPrpvejr++qktlcdXirMxPTPBLgl33rh4/Ofbn+PR4ix8JhJLvjw7Gwa9Dre6BvCmpZ3lRRNwrO4m2nsH8djciSg7KPWMWlwwzvq3szxCP15eCMB2fp5ekIt/+XMVHrgtAz/9ykzrdnNyUlBr6sK3lxTghb2fOT0XFS/ci/beART+6D0AwIUfP4CCH76D3gF796PkuCi8vaFEta1A5kLS+s7mI5gHLRMAvAdgBYBdvjKKIAiC8IwOKmFMECEPY2wLgMcAfANmT/avAJgUUKMIgiAIh8hz7HjcnldbU4f7uIShN08LF/tJ+fAEcSf+WK50Ld+Wqezt6+vgCVrFI8Y57wbwCIBfc86/AvNbMPUdGNvBGGtijJ1xst0cxtggY+zLGm0hCIIgnNDTP4gKo4nyHhFEaGPgnH8NwC3O+b8DuAtAfoBtIgiCIBzgbS3D11pCINMxOxNngh1PrA/FY9csHjHG7gLwBADBR81xbTmzZ9IXnTQaDuC/YfZmIogRy5ZyY0CTmxGjD1NnP579zQlJTDhBECGHkPmzmzGWCWAAgPMEGaOG0Bt4EwQx8vGy49GohKu439CpDSxaxaNvAngBwF8452cZY7kADjjagXP+EYCbTtr9BoA/A2jSaAdBhCSFWYlYv6cy0GYQo4hhzjE4xPHgzMxAm+IySmIreVERo5S3GGNJAH4K4BSAOgB7AmpREBHMrv0EQYxe1MKR3G/PM0L5XsnAFF8T+OuYhH58IQiqfU+CWXzUJB5xzss551/inP83YywMgIlzvsGTji35k5YD2Kxh2+cYYycYYyeam5s96ZYgAkJVfRvWLcoNtBnEKGKYA7uenhOSSbMFsVUQkISyqlQ9jhhNWMZbH3DOWznnf4Y511EB5/xfA2xa0BDC8yGCIEYwwTz5d4Tvw+M8PzHu2Bis12PEhq0xxvYwxhIsVdfOAPicMfZdD/v+OYDvcc7tU4zL4Jy/wjmfzTmfnZaW5mG3BOF/CrMSsflgbaDNIIiQwKDXoay0CGt2n8SP/nYG6/dUOiyrShAjEcv46Fei5T7OeeDq8xIEQRCaCDatwpl44k9xJdS8oARzVb2E/GdKUKA1bG0657wdwMMA3gEwGeaKa54wG8DvGGN1AL4M4NeMsYc9bJMgghKDXkeeR4TfWbP7ZMjm2jLodejoHcSuist4ct5EEo6I0coHjLFHmbdL9xAEQRC+I0Tv2Gp5hgh1gc2TM+bt8EZ/oFU8imSMRcIsHv2Ncz4ADz3bOOeTOec5nPMcAH8C8HXO+ZuetEkQwUqF0USeR4RfyU6OAQC8dbohwJa4h1j0ev3olZAVwQjCQ9YA+COAPsZYO2OsgzHWHmijggWa6BAEEYwEmyjg7FYZzK8nGJNqcUFs6qhAq3i0FeYkjXEAPmKMTQLgcPDCGHsDwBEAUxlj9YyxZxhjaxljaz0xmCBCkar6NszKpnwthH9ZVpiBSalxgTbDZYQcRwJlpUWSHEgEMVrgnMdzzsM451Gc8wTLckKg7SIIgiDUCWYxJqC4eV7oNUHwEKFlI875LwD8QrTqMmPsHif7PK7VCM75Sq3bEkQoUpiViF8dqAm0GcQooqGtFw/OzAzJcK+q+jaUlRahdNtRALYcSFX1bSF5PAThLoyxu5XWWyrajnpoQkEQRDBC2pHv4LL/fd6fSkejVSDUJB4xxhIB/BsAYRBTDuA/AFDiRoLQQFV9G7auKLZOhgnC12QnxwRMaNlSbkRhVqKk/wqjCVX1bVi7UO90f6VtDHodCUfEaERcnCQawFwAJwEsDow5wQVFrREEQThHq9Dh7Vuq0j1asy1Bdn+X2+2OfSMhfaHWsLUdADoA/LPlXzuAnb4yiiBGGmsX6mniS/iV3gGnhSx9RmFWoiTMTAhDK8yi0E2CcAXO+YOif/cBmAHgVqDtIgiCIEIHpzmPgthXSm6Zvy3l5OMqQZPnEQA95/xR0fK/M8Y+9YVBBEEQhOeEAdh2yIjDNS3YtWquX/sWwsye/+0pFE9Kwakrt1BWWkQCKkF4Tj2AaYE2IlighNkEQRA2PL0levuWOgIcbURID8adYxsJzyyt4lEPY2wB5/xjAGCMzQfQ4zuzCGLkQcl+CX9yvb0XL+6rxqalBQHp36DXIT0hGu+fa8SXiyeQcEQQbsAY+yVskQRhAGYBOBU4iwiCIIgRh49EHrlWwhTWjQA9ZVShVTxaC+A1S+4jwOwy/ZRvTCKIkUlVPaUII/wHB7C4IA2rS5znGPIFFUYTapo6AQD7zzbikTtMJCARhOucEP09COANzvnhQBkTbNCcgyCIYCTk7k1BaLAr4WLBHHY30tCU84hzfppzPhNAIYBCznkRKFkjQbiElkTBBOEt4qLCsWOlf8PVBIQcRwXj4wEA6+/Jk+RAIghCM38C8Drn/Dec898C+IQxFhtoo4KGIJzwEARBBIqRFSZmRssx+TIvkZpnlDcSZodiPiWtnkcAAM55u2hxI4Cfe9ccgiAIwht09Q9h2yFjQDyPqurbUFZahF8fMAIApmcmoKy0CFX1beR9RBCu8QGALwDotCzHAHgPgCFgFnmRLeVG/Kaizu39O/sGvWcMQRCEl3hqxzEUZiViUmoc1i7UW6vQCohfpml5sXai7qb1b0f5LP90sl5x/9eO1CEhJgKHa1pwZ26qXQGTczfMU/wrN7ud2gIAL+yt0rTdN39fKVlu6xnAD988Y7edo2NqaO3BjbZe6/KNtl6s3HkMSTGRku16B4YAAL89etmujYtNnXh61zEMc2DXqrnYUm7E5ZYuRZtf2FuFSalx1mVjs/nxK+g+8mMXPnfGyp3HMCcn2bpcYTShf1C5uM2Fxg5sKTdaHQ/k3x9hf61VjL2J1mprSoxAbZMgfMOWciN5XRB+JSE6HC/uq8bTu475vW+l6oIGvY687wjCdaI559aRqeXvEeN5VJiViJbOPrf3D713tgRBjAYuNnbi7arr1gm/UIVWYM3uk4p/qzFkudntq2rAi/uqMT8vVXG7KePGKq5vuNVj3U9uS4XRZH3ZFx+tza/kr582aNruSotUjLrQ2IH23gHJur9U1isek+DZ88G5JjBmu9t/XGPChKRovCmyobmjDze7+gEA08Yn2Nnx6qFaHKhutvZRmJWIt6uuK9osvm4A8PvjVwGYhY8KowlvV13H21XX0dRhFrReP2IvVikxISka/7P/gnV5ze6TGFZ5iJk6+yU2KF2zQFUx9kQ8omc2QWjkckuXpocDQXiLcQnRiI4MQ4vlYRoIQtEdlyCCjC7G2B3CAmOsGCOoYIlBr8OupwMTXksQBOEroiLCsHVFsfVFmlCFFgBio8Ldbve3R69g09ICVa/ymdlJiuv3Vl6z7ie2JSqcYf2eSjx/j7m9pNgopzYwAOFh2nxIfrx8hmSZc/O5EbPj4zqHx/SF6eloareNZXN0sXjnTCMem51lXXf00k2kxJltv2NSsl0bA0Nc0odBr8PWFcWK/YmvGwA8PT8HAHCxsQPr91Ri64pibF1RjCO1LeYNNLrTvHOmEd+5P1+yTu00Ts+Il9ggvmbREWFYv6cyYFWMHYpHjLEOxli7wr8OAJl+spEgQp4HZ9LPhfAvdaZubF85B399fkGgTSEIwn2+CeCPjLFDjLGPAfwewPoA2+RVKJSVIIiRxtPzcxQ9sJcVZqC7fwirDDl4bE42AGCVIcdu/+hI6RR9lkUUmj0p2WE6ArU8PHNypPsZ9Do8ODMD/UMcT86biGkZ9t46qn1YbF6pYLecR4uz7fZdlD8OAJBg8XKS2yYnMykGqywCTk5qLM7f6MST8ybivx4tRGZSNAAgVxeH6Eh1UU6pD4Neh0fumGC3rfy6bbxvKu4tGIdjdbfw5LyJMOh1MOh1eGb+ZADA6pJcazvTMsy5PvNEHmAzJpjP7ZPzJuIpw2Tr+lWGHOtx3T5Bev6VRDyDXocHCzPQOzhstSMQOBSPOOfxnPMEhX/xnHOX8iURxGjGkcJNEL4gdWxUwCdlVP2CIDyDc34cQAGAdTBXvp3GOR9RbqwU0k0QxEhjZ0Wd3b2twmhChbEFGxbnYWdFHf7+2XXr33J6B6S5cD692oq5Ock4cfkWth0yumTL1PHxOF4n3a/CaMLhGrMtrx+9gnPX21X3j5C5xywrzMDOijr8+VQ9NizOk3wmF73k5yAqnOHghSYAQHvvIObmJNvZJqehtQd/PFmP5UWZuNzSjeVFE/D60St49eNatHWbQ+BqTV3WnEdKnFDoo8Jowj8+b0S4TA2R2/zJpRZUXm21nivzdTThjeNXsWFxHnZV1OEfnzdieVEmqq93YEGezlptGADOXmu32vybiksAgMgwhp0VdfjDCfNxfXatHVERYYgKN5/rth5paJ9g12Fji8SOQOBJ2BpBEAQRpDS199GkjCBCHMbY8wDiOOdnOOdnAIxljH090HZ5iwqjCSt3+D8vG0EQhC/pHxzGmt0nreMwIUdNWWkR7tTbcvvEx6j7YvxgaYFk+b7b0rFpaQFe3FetKrZU1bfarWvu6EPpvGzrfmJbNi6ZKilwcqvLPgfd0DDHvQVp1uX3zzViyJKsR3wsALBj5RzJ8jO7jlv/Tk8YgzGR4dYk0dkpMfjDWoPqMQlOVO+fa8S6Rbkov2DCpqUFKL/QjAdmpOPFfdW4p8DsxTRvcoo159Gpy7ck7USGM0RHhkn6qDCasGb3SQwODSMyTCqHiK8bALtztWb3SazZfVJyLQeHhvH+uSaUzsvGxzUmRFpEoDBmFtTeP9eIB2akW3MeCWF/g0PDePfMDcRGhWNMRBgeucMcine2od0uqbrcjkBVMSbxiCD8QIXRhJU7jzvfMMQw6JUT9hGBJzoyDGt2n9RcEYMgiKBkNefcOhvgnN8CsDqA9niVqvo2pI4dE2gzCIIgvEpBRjyWFWagqr4NgK0KrUGvQ1V9mzVvzuGaFsXIhGkZ8ZIwq5TYSByuacHqEj02LS3A4ZoWxX4vNNpX/iorLUJ2Spx1P7EtgDk64nmLB1FHr30Fy01LCzDEbR5IeWlj8dCsTGxdUWw9PgG5x3v++Hjr37qxY7B1RTESos1V0uKizMKZs2P6wrR0DA2bj2N1iR5lpUW41tqLTUsLMGyJ0xsXH23NeSRUjhMIYwzbV87BPQVp1j6q6tuwrDADDxdNwIq7Jkm2F1834fyJz9WywgwsK8yQXMuHiyZgWWEGslPisLggDXdPMYttqXFR2L5yDpYVZuBaa68159EQ59b98sfH49WnZmPrimLr8aSNHSOxQemaCVWM/Q2FnhGEH3jrdAPCR2AET3ZyLADlmz0RWJJjotDRT2WsCSLECWeMMc7NI0rGWDgA5xlNQ4S1C/VYu1CPnO/vC7QpBEEQXkOeb1JcbVb8t1p6gTV3S/PznPrXJda/V5foVXMEfbk4C7tkYXBCjh5hXyWEnEfZKbGok1VIE/qb9sN3MTg8hDeeuxPxFgHIoNfhv96pVmwTMJ8H4f7OmHn7/3x4Br7xRiX0orxAjo5pQlKM3TkTjuePJ67i75/dAAe35jx6Yt4kbCmvVT0HgPQa/O7YFcm2Lz1SCADW45JfI+FzcTvytj+pbcEH1U2YrBsr6buzbxA/3X8BkeFhdjYBQN/gMP5woh754+NVvzNqx+QvyPOIIPzApNQ4bJe5co4ElhZmBNoEQoXGzj5suDcPk1LjAmYDVVsjCI95F8DvGWP3MsbuBfAGgHcCbBNBEAQRhDA3X1Rr2c3dtm19eP8tOvPUKHh+XIpturje3e0CAYlHBOEH1i7UBzx58cSUGK+36YsbLuEd4sdEYPPBWhRmJQbaFIIg3Od7AD6EOVn2WgCfAfD+zZwgCIIgggi1ynGjgWA+dBKPCGKUMHZMpNfbfLvqutfbJLxDa88A1i3KDahoGQzV1raUGxUrnmwpd61aCUEEAs75MICjAOoAzAWwGMC5QNpEEARBjF684e3jWofqH/ERpjAFftTsHBKPCMJPeJIRPzLM89vJ9dYej9uQk6tzPyRKSLw3NX2sky0Jd4gKZ9h8sHbUV1wrzEqUVKQQKlZ42yOLRCrCmzDG8hlj/8YYqwbwSwBXAIBzfg/nvCyw1hEEQRDBiKdaSqhqMY5eVjo7JH++6HRmSyicfhKPCMJPeJIRf2DY89tJa8+Ax23ImZ6Z4Pa+S6anAwAm63wrHoWCiu8LhjkwKzsxIJUYggmhIsWTrx7Fvf/voLXUqbc9svwlUhGjhmqYvYyWcc4XcM5/CWAowDYRBEEQIxB/OhOFgkDiLdzN/RnMcxcSjwjCTyhlytdKdKTnP9UxEd7/ub/zmftha++fa/KiJepER4Zjg6UE6WgiIozheN0tt8ULb3jSBEvCbINeh2EOGJu78OS8iT4J5TPodfj5Y7Owaudx/OdbZ30mUhGjhkcAXAdwgDG2zZIsO5jHkwRBEESI48txm7xtZw+0YBlDepOR8BAn8YggQoAdXqjU1js47AVLpLxx7Krb+z5oqdR2ydTpLXMUebgoE3fqUxEVPhJu2dr52l052HBvHl75qNb5xgqMJE8asQj2+tErPgvl6+4fRN/gMLYfrvOZSEWMDjjnb3LOvwqgAMABAN8EMI4xtpkxtsTx3gRBEAShHS2hW4EaRQdD/kx3CWXb1SDxiCBCAG9MQrWmTXLFQcmTsLUvWMLWrt7yfi4mMW9WXsNTO45hwZTRNZFvaO3B5oO1eO7uXLf2F8K9nt55HDP/fb9bnjTB8NAURC+BstIiiSjmTaqvdwAA9GlxPhWpiNED57yLc76Hc/4ggCwAlTBXYCMIgiCIkEE+JvSmX9GI8VIKgcMg8Ygg/IBSCJAreGMSmhCtrdqaKw5KNU2eew2lxHm/CpyY/PR4cA58XNPi0378TXrCGIefv3v2hsdhUwa9Dr2Dw2jrGQxZT5qq+jaUlRZZlwVRzNu5oCqMJrz68SUAwJRx8T4VqYjRCef8Fuf8Fc75vYG2hSAIgvAd/i5o5gruVjgTjknroXktebfzjNk+Q03UcnZswXz9STwiCD8ghAC5y5rdJz22ob3X+wmzHy2e4Pa+73/eCACIi4rwljmK1DZ34bVn5uK79+f7tB9/09rt+Hp647njr3AvX7J2od5O9DLodR7lIFOiqr4Nzy6YLOnDFyIVQRAEQRCEEp6KDsFYbU3LMQWDp7sYpmK01usTjNdd5O6OAAAgAElEQVRBgMQjgvADwkTSXfq9kK/InQdKhJNYt7dOu58w+8+V1wAAF73gveSI7JSYgHnM+DLPUk5qrMPPw8MY3jrd4Hb7/gz3GgmsXahH/vh4yTpfiFQEQRAEQRBKuCs6BLOni6c4C2nz56E7vT4hcB1IPCIIP2HQ6zDeSagRoByO1KdRPHIk9syYoC3RsVjv2HDvFIfbfsODKma5ujgAQFZyjNttaOGSqRsVRhN2fFyHcNEdT2sOKE8YGFJ+SizKT/O4bWNzl8PPV9w1CfvPNrot9ngj3GvExKDLUKtE997ZGwGyiCAIgiAIwneoedO4ysgcGUpxFtoXymIdiUcE4ScqjCb0Dzq/ZY6Lj7ZbtyAvVVMf/3L/VNXPnN2nkmMj8WBhBsR6x7QMxwmxb8t0v/JWa88AZmUn4spN3ybM3nBvHtbsPonW7n5EhNluecN+eHrFRCrfYtM0iIjOmCrzcpGjTxvrUdiUv8K9QhG1SnQ5qXEBtowgCIIgCCL4kOsp/tJPAhnS5nLfIaCskXhEEH5AmFyWPeE8dK3+VrdkOS0+SnOy52dL1CtrnbnW7nDfr9+jR0ffoKTa2rnrjvfx5Hb8wIx0fHq1DenxngspjpiWkYBlhRm4S58qUfrHKJSVe2hWpmIbSseZGOM8V1PPgLLH2NLbM5zu64yp6Y7Fo5rmTlTVtwVU7Am2GHRvIXhhPbXjGL7++klrJTpngp4Sal5MW8qN3jKXIAiCIAgiJPF0JBlIL3hKmE0QhFtU1bfh/tvSNW17S5YIubmj32nuIYFPatVFpvGJjkWazQdrMT8vFUMiveN/37/ocJ8fvnlGk11K7Dl6FfcWpKG9d9DtNrSw6/AlTEqNw9zJqdiw2BaGxxiQNjZKsu2H1U3YoBCKFzcm3G5dQoytSpza5RFv422cPVjeOHoFhVnue4YRjjHodRgY4vj7mRvWSnTuDE/UvJjo2hEEQRAE4U+05EwKAeeYgONpwuxghsQjgvADaxfq8eDMTE0V17KTpWFrs7ITERNlL14osf4N9fbvv82xt0tZaRFWl+gxNtrmUTPk5ClS3+p+yNnDRZkY4sAjdyh7+3iLI7U3UZiVKPHASYqNRGR4GKIjped1WWEGNi6Rhv4VZSehq2/Irt32HrPINzE5RjUELkrBuwnwTtz4O2cc59f56tyJAUsUHgp46vHjqBKdK5dX8GJ6ZtcJvLC3yurFJL525J1EjFS2lBvxwt6qQJtBEAThVbYdMmLlzmOatlV6lv/+2BUs/9XHknXOnvsrdx7DXyrrFdc76//zBnOKg7Ye+0q+K3cew7ZDRknxngqjCSt3HrO7f8uXlcYuQrVlZzZdaZFGYigdvxYhS5jKqJ2/g+eb7fredki6nStjri3lRpy5Jk0ZIZyvo7KX/KE4liPxiCD8RFV9G9YtUg8rE5gxIUmy/MUZGdhwr7bE1I7av9HmWOgRJqsJ0TZvmXmTkx3u85378x1+rk9TzgEzISka5RdMmJ+Xir2n3K8IpoU7c1Ng0OtQYTTh1wfNN+iZWUnYuqIYDa29km1feqTQbv/iSckYG20v3rX1mD2mGtp6FZOcO8IbLx5mZSU5/FyfNtYLvXhGMCfMFjx+9lU1YHiYu+Txo1aJ7vyNDgCuVzsx6HXoGRjCG8euWr2YlGyVeyddbukiUYkIaQqzEvF2lftVOwmCIIKRF/dVY77GfKVK446jdTdRedUmQGgZo8zPS8WOj+sU1zvr/xcf1gAALjR22H0+ISkaL+6rto7pjl26ifV7KjE/L9Xu/v2mpZKywJrdJ61/d/UNYv2eSkx0Ui1YsOlvoorBnnplO9o/O0VqT3iY+fq523dhViJ+9v4Fu/3n56Vi4x9OO2w3mMfNAiQeEYSfKMxKxOaDtU63k1d3v3qzS9N+ACQhZ3I+qG5yuK8wCe0bsHnZnG2wf4iIcZYwu/5WDyIVytVnJceirLQImw/WYuMSxxXdlHBFfFk1fzIAs3j39UU27yODXofMJPvk5EqMHWMffpZkCUnLTomFqbPfJTud5ZKSEx7G7JJvV1695XAfY3OnS334kmDMfWTQ6/DDpdPw/J5KPLH9qKLHjxpqleiutDiugKeGIy8mcfvrXj+F/7f/vNVWwZvR05A38mwiAoVBr8PWFcWBNoMgCMKrbFpagNUl2nJOKo07hoaBJ+ZlW5e1jFFWl+jx9IIcxfXO+hdeUicp5PN850wjNi0twKClos53/3jaGq0gv39HhKtLC5dvdqOstAj5TnJ2CjY9ONMcMSGIVUrHr2V0OTTMHZ6/vHHSl62bD9Zi09IC67Ir40PB9m99wfxy/dqtHuv+q0v0ePmfZwIABoaGXW43WCDxiCD8hEGv0+R59PZn0nCkPUev4oEZ9vmSZmXbe544So6stL2Y9Xsqse2QETe7bULIkumO8zR93mATQaIVKotFRYTh+UXKXlPChNiR4KXGzGztk+PPLULN2oV6u+pxcWOcJ70+Xd+K3gH7sLUBq+EcQypxa939yvmcXv7HBcX1SkxNH4vYqHA8cscE67qMhGg8XDTBwV7AG8fsRQh/U3/L7O0mfpMSTKLEuASzeHjE2KLo8aOGWiW6+24b77INal5M8muXGjcGbT0D+OWBGqutwm9o7e6T+M4fP3V7IEJ5l4hAEmoDZ4IgCGdoFY7UKJyQiB8vt3nDax2jbFjsOCJAjYdnZQEATF0DdoLMk/MmYnWJHnfpzR5MpSJbDHodbp9gG1uvMuRI9hUv6+KiXLrff+0u874nLt9yaYwmh8Px+VM7XuHltzt9f8lSgKehrVeyf8mUNADA4DBXbDcYX7bKIfGIIPxEhdGEzQdrkZ/uOJxo2e3SCeimpQW4JguvAoCzDfYl2B2JBacutzrs9/7b0vF21XWkxNmSSDsLJ/jp/vPWv3esnGP3eeEE5clngyVXkkGv0zxBFSelfmCGLX9TnEo+qAyLMPDLD2ps50V2T9aSeyht7Bi0KsSAd/ebBaUbbX2YkZlg9zkA61saOQvz05z2CwDzcpKx/1sLsXVFsSS8LyYqXDHETszjc7NRVW//HfEn8RZxThAZg02UEH5DWckxih4/7uJKzqOq+jaUPW7vxSS/dh9Um3METEyJldhq0OvQ3juIP5285vbgSujzuddOIuf7+/D110+F5NswIjQJtMhNECOJ4J96jg7kOXNcITwM+OxaG37wF1v+IK1jlD+cvAIGYG6O47QTci42dSAlLgrLi8yiR77FGyfbMj7adsiI6hsd2LA4D28cuyp52VTX0o3oyDBER4ZhZ0Wdtc0Ni/Mky6aufpfu9+29A0iJi8KGxXluj9Fc2T8qIsy67bZDRsRHR7rd94XGDsW+j9Xd9PiYAg2JRwThJ4ScR6bOfoxPUA+Xujt/nGR5dYkez92di6RYaejUP82wT4Atji2WE6bh1371Zo9k4jvsJHlLVnKM9W/5RHPD4jxUXWvD1o/sQ+4Ejx+514WS95JAjCi59btnbKJW76C9VxAAmLr6AJjzNlkn4m6EEmcmxSgKVOEWNeveaeNQkKEsHvUO2rtVLb09AycuOw45E7jdktfIoNdh+8rZWk0GAEzWjXXoieYPkmLNQuRP95/Hf71zLqhcdCuMJvzSEuOfbQmjVPL48TVrF+pxZ640H4FBr5NcuwqjCb8+YB6ITtbFSWx1FvKmFYNeh1RL9cGSfF1QXCNi5FNhNDl8bhEE4RrBnzFldPDivmrNApL4uf2DpQXY/cw8RIYz/PboVet6LWOUbYeMeHFfNTYtLcAf1hok6531v35PJdYtykX5BRNK52XjQpM59UFjRx8emJGOF/dVY92iXGxcMtVqy7ZDRuv9e8fKOdixcg4GReEEd+qlY5tJKbFYv6dSMa+Smk1lpUWSPl0d42jZv8aS5mFuTgo2LpmKdYtyFY9Xa99qtm87ZHR6TJTziCAIK4VZiXj5vYtYtyjXYdhVjSxXTYXRhGd2ncDz93gmBEQ6iEMGgAdnZprFrQ5b2NrcySkO93l87kTVzzYumYqtK4oVb4RCCXt57pinLfmJlJghcoutvNomcpNVfs8mhKg9NX+yT0SUiHCG2zITsK/qOiov37T7PCU2Cl+aaS/w/eqJOyTH7IjT9WZvMaXqEs4GA4cuNgdNiNjgMMeW8lqP3I69TVV9G76x2BZSKff48SQPkKsJs7XY+vV7pPm6ykqL8NbpBk0hb1qoMJqsHoEHqptD8m0YEXpU1bdhWaHjSqAE4Uvio52Hr7vLHBe9P7yBUp5Jwv9sWlqAwzUtzjcEJJ7Gq0vMYfGPFmehSDRXUPNKFnO4pkUx15IzO4Sx+NCweRyRnRKHBwvNHkjzJqfgWmsvNi0tsKaZEGw5XNOCZYUZ2Lqi2BpKL06pUFXfJsmJNDY6wpIfUlpFzZFN4vA4Z8evhJb9r94025MWby5+MzQMxePV2rea7YdrWjQfUzD/ikk8Igg/UVXfho1LpjhNfv3bTy5r2k+pVLtamBgAbF1RjOxkxwmiNx+sRUyU7bZQPMnxwEfw3FDDoNdh+9fmIDslRvFzee4YR3Nu8c11eVEmplgS7unG2sLsSqbYCxN//0wUeufm3VhJeHt2wWTs21CC0nnZMJrsH4QP3J6OQxftJ+AVRpNmASVtrPlBJuSkEejtH8LL7110uC8DAh4iJhYOC8bHB5WL7tqFeruE72KPH3/mAXKmNSnl6zLodZiUGqeYuNvVwZVwbDMs52PdwtyAeGERo4+1C/VOQ3AJwpdsfdJ3CdsfmuU4N6EvcDZuI/zD6hI9dq2aq2lbpRecLz1SiL88v0CyTu6VLGfXqrmKuZac2SGMxcX/351vHqemxY+xtivu26DXYdequXjpkULJmFZ8P1fLD/kFJ/lUHe0rP35X3tWpnb/FBeaID25587d2oV7xeLW+iFazfdequZqOKdgh8Ygg/IRwMyorLUL5hWbV7e4pkIatFWYlWt8GiFldIvXSiQpnOHXFcTiUqdOWu2fqeGm1A8FltXfA5nLq7Kb81F2TnGwBsDCgq29IkrNIDUH9FyO8RBsUJaUuv2BCc7s5LK25o8+6/mORWCO0NVkX57xjBzS09ijmPBK8S7JT4lAw3r5yxN5TDXiuxD5BuiuT8gmWsEBBFBBo7Oh1Gsb20UVTwEPEWrtt5216ZkLAQsPcQZwH6N/+esalkDtXch55gtbBlTOEt2TJlnxnBRkJbolQBEEQBEEQ/sZf4y6CxCOC8DsGvQ6rDDbhx5CbKhFWDlQ3SbYXvB2cTVp/uGy6wxKZ6/dUSgSHCzekMccL83XYfLAWOou3CwDsPHxJtb0Ni/Pw6sfqnwPSuN8w0Z29vdtejAFs6r+YsDCG5UWZkuTTZaVFOFJrdsOdmWWrIifOmXTL0se0jAS7cKMrN7sVBQylkKTmzj4kREfarb9iEacKsxLR1NGHZxdIxbzVJZMxpBC/5GhS/mix+ltKg15nzXuVNnaM3fdhUmqsZHnu5JSAh4h19kmrzbnrGQMEppy8Qa9DZ98gfnPksk9D7ri349xcxFsiFEEQBGGDJrREqKKloIzLbXq9RRsBHkZ5jVA4DhKPCMLPVBhNeP2oLTRtclqs5GZRKBJDAFi9HeQT50qZl9HtWUkOc0fIvSY2LS2w/s0A7PvsBtYtykWMKDm0vOSmmPiYCGvFMTXkcb8CXSol7MXnIdKiqIUxhujIcIm9Br3Omqw7WVQdbvvKOci1eBolWfIqnbvebgs3srQfHx2B9Xsq0SUTN5RCkmZmJSEqwv5Wee+0dGw7ZMQzu06grLQIG74wxfrZ8qIJeP3oFcxQCCN0NCk/UN0sqcYnfni/sLcKvQPm893SbV+xoqNXeizHLt10y8NHSaQR1rtKdnKs3Tp3RYlAlJP3VjLqUCMUBi8EQRAEQfgGn7zUClJBikRe1yDxiCD8iDDhFYs8e45eRem8bOvykdoW3JFtE5AE4Uic8wYATl+z9954cGamat9V9W2Sya8430tWcgymZcTjFx/UWAUKADh1pVW1vRf3VeMHIkFHibUL9Xb9AkBcVIRTMSLHIgI9Pnci9p9ttMtPUzTRHNcvv+e39gxgeVGmNdTs1weMdgJWcmwUykqLcM2SIFhAzbNE6Rl6sbEDL793ERuXTEFVfRt2i8qRTh0fj3WLcrHu9VN2+6mJM4BZ4BPnthESGFcYTXi76jqY5WinjY+3q1C0bpE0RO7uKTq3QsTkIo14fSARvJae3nUcX/z5Rz6v3Cb/zfky5M6VIZovdR0aPxEEQRAEIcB8MDIItLe1GsFgVSgIWSQeEYQfETxxxAnlNi0tQHaKLS9PfvpYnBOFlFUYTXZVyQDgvmnShHOf1bfirdMNqn3Lky6/esiWgDs2KgLLCjPQPzgsCTWqMNpXaBhnqUYwJydZMTGfWr/DoodFTXOnohghvnEL5cuF0uRaQp0EQeFnjxXhC9PMIXCLC8bZBAbRTdmg1+H+28ZL9lcSdhpae9Ct4Cm160gdtq+cjdUlehRmJaLsgC15uFBZ7+EiezHvckuXnRAopkZUwrTJks9JqFjx/1m8m5RyMNU2d0mWmzv7cf9t6XjrdINLXkOCSPPsb07gK1sqJOtdxdslRw16HXoHhlF9o8MrYWSOntHy35w45O6FvVV4YW+VZPsKownvnbVPYk8QBEEQBBGKeHUcJxONfBEa5w6+EMhGMiQeEYQfUcotIogPAhcaO7FxiS0ESi3n0T/ONUqWf7r/vMO+5UmXD1TbknZ39w9i88FafOf+fPSIQtEeUvBkau7ow9ycZByvu+W0XLy4X/EzQ58WpzjxF7+NYDKhx67Cgko+IcFT69SVViwvmoB/fN5oE4REu1QYTThQ3YTlIoGnMCvRzqPnw+omRITZ3ypL59rEC4Neh18/cYf1s/V7KrFxyRS8q1AR78GZmXZCoHi/pFhbGF6RxQNN+N48PnciYiLDcPVmj11I4f6z0u/DuevtyE2Lw/6zjS57DRn0OnT3D+F4neME7P7GURiZt3MiyX+rQpW8wqxEvF11HW9XXZd8tn5PJSalupec3ZWXcN4e4iidt+ob7T7NJUUQBEEQRPASLMKOM4LBW2i0QeIRQQSYF/ZWSQSLL83MxC8+sHmxqHndyMWD794/1WnJY4Neh6/daa6QJvaKaWjtRVlpEVaX6PFPt9tC6p6WJYEGzJ5Sf1hrwKalBXhxX7XjgxP1++SdtspsCTH2CajlOJtQCx/LRSZxku6fPTYL21fOtgs3utXdbxV4yi/Y1p9tsD/PiwvGISpC+hDdsDgPfz51TdLmPQU2T7An503E6hI9/ver9iKRQa9T9ZgpKy1CWvwYxc8E+2KiIrBhcR5eP3rFbl8xU8aNxeaDtZpCu+QCglpoliDGBCJ5tbMwMk9yInFw1WMSeHrncTz56lGs31OJrSuKseUJW3ln4fsmr2AYCiiFKW4prw14mCJBEAThOuRFQXiDYA0tU8MbWlcwHHIw2OAMEo8Iws/IJ6mfX29H/+CwdTkrOQZDorL0agmGp6THY1qGbbJamJXkNB9LhdGEtz+7jg2L81B+wQR9mtlTIjM5Gga9Di/srUL5BZtH0rG6FkTJKrgJoWqrS/SSJNbO+t33mc1To80SduUo/4+A8weCdAN5km5JhS/Lph29g1axrKy0yNrCy+9dxNYVxZL2MpNi7HrcuGSqnXih5BUjhN7JEXtsRYhK7TkSecSi2MYlU3H/bdKwRfm+p+vbNId2iQWECqPJzvtKsFkQYwKRvNpRGJl4+dnfnMD3/lzlUk6khtYehIeZRaCKGhOa2nutydAFegeH8XGNCQvzzeLfSVHCemGdQEtnn0tCmitu4d4eVwjnbf1vK3Hs0k0AwJqFuQGv1EcQBEEQRGDxpRgZLAJVMDpZBbPnV0SgDSCI0YY891BqXBT6ROLRjsOXNN2qLzZ2oLG9D+Pix6Cpow9V9a3oclD9TCw+GPQ6xMdE4McWz6EbbebJsjgUBzAnxRaLG0I7wsRydYne2oaWfku3HQVgzs9TYTTZnQuxcCVw6IIJX7srx65N4Zkjv78qCW0GvQ5V9W04d70dADAxJdbqpVRV34anDJOwq+IyVpeYPa0iwxkGhswdNLT2KL4JkIsXSl4xz92da7fftkNGicdWGGMQJIFth4xotuQ5AoBrrT3YUm60Jh4XiyEPzszEG8euStoVMzMrEa8fvYI79akOhYAt5UZcbunCukW5WLXzOPoGhxEZbv8N/Mm75/Gbp+da2yorLcKa3Sex9PYMvPd5o6JQ481Bh9p1FfcphNv9/vhVbFicp1kAiRsTgc0Ha7FuUS5Wv3YCXf1DYDB72cm/33+pbEBX3xA+umj7rr5Z2YDpmQnITDRXl6u82opv3pfvxlEGBoNeh6nj43Gk1pzjrGB8gpM9CIIgRgjBO0cjiBFJsAojwSFlBT/keUQQfkaee+h43S3ERoVblweGhhER7vynubOiDmWlRRifGA0A+L/7zsHRblX1bVi3KNda/WzzwVo8NjsLADA9MwEvv3cRG+7Nk3jeREeGYdbEJLt2XEEuegBAblocqurb7M7FPpF41djeCwCYkGzv+bN+TyWaO80iy62ufk12FGYl4tcHbAKLIGqFhwF/O232xtpZUYc1u09iccE463YfVjehRyFhNmDzClPyilm3KBf/+/5FAIBYf/vp/gsSj61w0Ycv7qu2TuAB4P1ztnxF8hw8cmFEHkJ4sakT6xblOq0QJuTw+cUHNYgfY/4eCsKZmGWFmXb9d/QO4nfHr3olebU3cJQTyRGJMZEoKy3Cz/9xEd0WAbZoYpJqQvj3Pm/EoOgc3VOQhh/vq8bze8zV9YRcVVq9jwKZ8wgwn7cTl29al6tvtPugF4IgiCCEZowEQRCaIfGIIAKA2WPCHNK0ypCDb91nS5A9NAxsuDfPaRur5k+WTNhL507Ey+9dVN2+MCsRmw+ac5kIYsf9M8zVxlJio7B95WwMDZtt+2eLqLS6JBfLi7Ik7Sh5gDhCKUl4QnSktR3xZw8XZVrFFMELabLOPglxWWkRPr3SCgD47Jo2Mcug1+Hr95j7vGQyVzxbtyjXmhdo45KpWFaYIfECA8xhhN0Dw0pNOjzGwzUt1upoYYxhlkVQyEiMlogS4vMZHRkmeSPzhWnpmkUZeWW3rxRnYWhYPWeWgEGvw9YVxejtH4Kpy76Km0D5hWbVvEhqQo23q605wllOJGcY9Dp09Q9ZLf68oV1139S4KMnysyX2Hma+DuPzFsJ5E76fALC1vFbzeSMIgiAIgnCKDz2OvBEBF5z+UMEHiUcEEQAqjCZU3+iwerv87B820Sc8DJKE2QJyL4b89HhUGE3Wcu4PF03A9pWzVfsUh6wJYof4ZiusrzCa8P65JmxYnIdthy7h0EVpKJm3EyOLJ6nlF0z4p9vHW48PUH7WGPQ6PGNJ5i32EnLGo3eYhbArN7vx5LyJVnFFEGheeqQQ370/Hx9WN1n3qWnucu2ALDx3dy62Ws7TMOcwNnciOjIMN7v6Jcc8dfxY69+rS3LxdZGYpOR1pcQ37smTJP4GgBxdnPU6axH8BocdP3nFYoynQo0j3E3G7SwnkiKi75Y87G9wmOPpXccVd3u0OAtrFEISBU5dadWcb2lLuRGfiLzNAMfH6205TjhvurG2RO3P3Z3rsochEXwwxr7IGDvPGKthjH1fZZt/Zox9zhg7yxjb428bCSLg0IyRIPyKN3MdefPnS06I2iDxiCD8jDzx8ZycZHT3DyHcopJEhodhcEjq6bKl3GgXknaguhHP7DqB2EhbyJtBr8OiqWmK/WoJLZLbtnHJFLwjKzcveFQ4q9ClZcIvFyHWLcrF26evY3nRBFxo7ACg/DahwmjCnmNXMF+figPnm+w+U+vX2NyJlLgoa7WywqxEu3OyukSP9feYPb/Cw5jZG8jhUShj0Ovwi8fNYoagy+xYOQfLCjMkCanP3zAfZ0psJHZW1GHXkTrcZfFKu3arR7V98fmOi47AukVSMeOSqUuzyPfW6QZNxyOIMW4JNRoR8mAdFF1XLV48St5fjoSzLeVGa3W9tp4BO6+9wWGOXgceZ1PSbcnqxd9hwCwWOkP4/RRmJWLd67bvwwt7q6zhlOLr56v5jZKQXDA+3mUPQyK4YIyFA/gVgAcATAfwOGNsumybKQBeADCfc34bgG/63VCCGMFQ5TMiVPGHkOKN3Eck+PgfEo8Iws/IcwClJ0QjNiocmUnmN//Fk5Lt9gkPg93k9s3KBmxcMgVjo8157zlgTQA9Y4It4e2YiDCrWCIXeOT3bbltq0v0KBB5xgC2kvCXW7okk2Z5hS75hF9JyHjrdIOkatjmg7XYtLQAU8fHY2G+WQS7ZLL3/BEErucXS8P7HFX9kgtjjrxl5uSkICo8DEPDHM+V5CJuTLjdNlqYLxIzVhlyYNDr8OBMaXjZlvJaAIAu3nz9+waHMcaiFL5/rtHq6SM/f2KhRghJFPOnk/WawqYqjCa8XXXdqTexkCh97UK9y0KNKxj0Ovz44RlYudPm9aPVi8cVCrMSrR5+XX2D2Lhkit02mUnRqvuLQ/LEQhpgDlN0JsiJk8VvWGzr+2+nG6zhlOLrRwMkwkXmAqjhnNdyzvsB/A7AQ7JtVgP4Fef8FgBwzptAEKMNH95c/Rm6TRC+IEhzW9vhDTuDpfpbsEPiEUH4GfnEe1JqHF59ajZuyzTnHJkzKUWSMFtIbi2f3E7PTMBtmbbJ5Wf1rViz+yTuvy1dkr+kYPxYVbFEfp9UEgW+/8A0SVJnwCzePDgz027SLEY+4ZdPhCuMJuw/2ygRU8pKi7C6xCxOpCeYJ+5K3jdC2wa9TlLRzFF5drkw5shb5q2qBoyJtIlu7vLJpRaJp5MgwIiTkguDS86BrSuK8d3783HYco2WTB+Pt043KApiYqFGnngcMIfoaRFcqurbsKjhLNYAACAASURBVKwwAxmJ6kKJsJ2ruPvWNSs5VrLsi2TcBr3Omlvsys0eO/ENMCcJV8LZUd0xMQn7zzY6DOMTrtma107i7HVbguquviFrHi6l4756s9sn+YjEAy8aPo0IJgC4Klqut6wTkw8gnzF2mDH2CWPsi0oNMcaeY4ydYIydaG62r4hJEETwECqTfSI0GA16SjBVfwuF803iEUEEGGvYiGXKVpARLxEXBEFkdYkeJVPMk8m5Ocm4crMba3afRGevuRLYoYvmCaXcs4Vz803R3dCiqIgwSTW4NbtPSsSb+6abPYdWGXIk+4knvnKvmYbWHqvQ5aiCGACU5NuvE283Nd3mZeUoNE+rt4wgam1dUWwV3br6hhTbdIQjTyehNDpgy9k0WRcHg16H1SV6rLYkYG7vHcD+s42aPG8Meh1iIm23dKVE40qsXajHS48UIjYqwul2/qKqvlWy7E2xRBxuKRZfC8bH223raDghFsbkv6nUsWNQVlqEVz6yTzwt9iIz6HXo6BvE26IqgwCwMD9N9XrHR0eYv0c1JgwPc8VzoyVvlNI2xKgjAsAUAIsAPA5gG2MsSb4R5/wVzvlszvnstDTlsGiCCFl8OG8MRNhaKEw+ieAneOSU0Ukwn38SjwgiSLA98BkMeh0evcP8klgQRLYdMuLjiyYsL8pETXOX1Wui/lY3AODopZvYuqIYBr1OMmCZlmETV+RiiZZQpfV7KrF1RTEesdgzIMrHVGE04eTlW9bE3/J9BcQhOgBwva0XA0PDdkKXEs4GQkJupLxxcS6VZ1dDyUMp1o2wNUeeThVGE5o7+rBhcR4+umBv73fvn4ovzczEwfPNmnJVAebzHRMVgecswpNSuJ8jfPHiReyyr3UQXWE04b/frZas81YybsD2Xawwmqw5jwDlqn2OvnriY5MLa7XNnVavOLHt8rBKtWN6s/KaXQJv4ewlxUahrLQIq3YdR+6mv9vlW5Ifo1K/Stu0dPY5OFoiBLkGIFu0nGVZJ6YewN845wOc80sALsAsJvmNLeVGPPSrj/3ZJUFIWL/nlM/afvkf1c438jKVV275vU9CGU8KzLhbPMRbuKNBym3eUm7E07uOSbZ5YW8VXvlIegzuHteWciNqmjoBAE0dvdb1P5GNIR21v6XcaMux6qE9nrKl3IjqG+2SdYGyxREkHhFEkCAkVWbMfLM4cL7ZGu607ZARL793EZuWFuBnjxWhrLQImw/WYsO9eSielAIAeOquSVaRQasQ4EyYESdGPmixJzI8DG+dbpB41txpSfAsRjwxlYdVRUeGIVKeAdwNKowmbP/4EgBgyrh4r1T9UvJQcsdWNU8nYdIueCQJXkY3u/qt2x2pbcHHNSbVXFVyhGtx/23pmDPZ/H3486l61XxJ7uBpLLij3A/iAUdVfRu+98UCyefeSsYN2L6LT20/hv9+57x1vdjbTwsXGzutf8uvT1JspKSvJ7YdxT3/c0ASVilPFi9m09ICvPzeRWmIqewY+gbNIu6T8yaqHuNzr53Ed/74qWI4p7DN2t0n8e0/fIrKqzZvL3pzPSI4DmAKY2wyYywKwFcB/E22zZswex2BMaaDOYzNPn7ThxRmJeKCpWgAQQSCDov3ti8wdQ74rG01egfVCz0Q/sNRDk4tyF+6etqeu7jyYlFuc3gY8GG1LdS5s3cAb1Zew9kG2z3fk+MqzErEbz+5DAD4pPYm2nvMv7d/nGvU3H5hViJ2Ha7zij2eYi5GZHsEB9IWR5B4RBBBg3nGdqGxwy7c6eX3LmLjkilYXWL2cBAmfrXNXTjf2GEnMnjLiUTwqBDbs3VFMfafbcRbpxusE9JXPqq1ekIJrFuUi1c+ks5DIsPNlj1XkoutK4o1CT2OHlxV9W14ZsFk67I3q36J8eZkWu6RNNWSkLzN8tBzJbG3vM0HZ2biX/50GoA559FbpxvwzK4TdpX63BGUtpQbVUOdPBWnxF4wSuFx3krGLW5vYJhjcNjxhVX76l1r7cFvjtRZl9e9Ln1znSoqe28OSQUumbolXmTyinViVpfosX3lbNXvsfgaqOXkMuh16OwbxJ9OXlP1XjPodWjvHcSfT13DpNRYhVaIUIVzPghgPYD9AM4B+APn/Cxj7D8YY1+ybLYfQAtj7HMABwB8l3Pe4k87DXodtq+c488uCULCv3xxaqBNIEYgjnJwakH+0tXT9tzFlfGv3ObNB2vxg6W2l4GXTF2ICA/Dl4uzLMudHh2XQa/D05Y5QHZSDNotQnC4aOLgrH2DXoc1C80vcetMXQE7z4It37AUAqpv7QmoLY4g8YggggRhHltn6rILd9q+cjaGFF4mCflwBJHhmV0nsO2QUZL8rbG916PJvVr41aTUOOu65+7OtasGt/lgLebnpVr7fut0A6IjwyVJqD0VetYu1CPfUjJdOGRvCw3eRuyRtKXciPOWt+5CjiKhAp2WxN7yNg16HX7y5ZkAgL9U1mP/2UZsXDIFmw/acu+4+yZjZlaSXaiTgKdvRYRj/PpvT+Glv5+zC1vTysqdx+zWbTtkHxajJICpeQEp8f65Rjx1V451WRD+lJALPcKykmeaGLsQU8v/rd39ElvVBCi1ftW2udLSrWoLEZpwzv/OOc/nnOs55z+2rPtXzvnfLH9zzvlGzvl0zvntnPPfBcJOg16H+DGOc64RhK8Q577zNmJRPis5xmf9iLkj2y5tWVDhrDiHrxkT4Z9sMlpTDjjCoNdh6e0ZXmvPHxj0Oqy4cxIAs82rS/R46i7z8jA350f9yaOFWJCnw5lr7R4f17e+kI8FeToYTV2YZRmLPrNgMr4yO8tqg7P2n1+UhwV5Opxt8NweT3nu7lyUTNHh/I2OgNuiBolHBBEkCGFB/3R7hqbEzkqizsYlU/Dyexdxvc1WoexIbYvTyb2jSgNaEk0LfYsRlxtXSkItTICdCT3BEELTO2CfMNsb4WCFWYnYduiSpE15BTrANUFsXq45bK2zbwhPzDU/uMtKi7Bq53Gs2H7ULnRK6zHcmZsKg16Hf102HaXbjuJbv/9UYp8SriQLNeh1aO0ewNaPavGAZbDkKvPzbOGTFTUmbDtkxI/3VduFmCkJRQvztScCnp6RgCnpY1U/F76z8r68EVbZ0TsoEYyUzr2WfuXbFE0M7gkHMXKpMJrQ1e+70CGCcIQ49523uWwR5cMA1CtUjtVCXJT2qVpqXCROWUKQw4I04+71tl7nG/mQvkHfDypXGiZ5JQdnhdGEI7UtmlMY+AJX82FWGE3Y99l1SdqNvZXXEB0ZhujIMOysqMP2w7X4/Hq7V47rk0st+Px6O5YXZeJ0fRuWF03Azoo6vHvmhub2hTYCeZ4FjtS24GxDcNiiBolHBBEkCI8zrTdqJVFHCHd5/1yTdd1dlkm/I6HA03w2Qt93iyqjicuNO0oerQUtiQN9KTKFy66Jt+KQzdXVzO6yNU2eue8KHLt00/r3b49dsVZ36xscxqGLJkzPiJfk3AkPcy3sTAj1+kulPPeuGfG1Euc5cpbXRHx93/nsuoMt1RHCOgGg9NWj+PG+asRGhePVp2Zb16uFi7175obmfj6/3o4D1eoly4Xk0/K+vBFWmZ0SK/l+KP12tfQr30YcaifOsBSMyRqJkUOF0YRndh2HkwhSgvAZP3n3vPONPMXBuM5xVU+gq197DqOWLrMXbFQ4Q3Sk64U+1JCPgYINL6TQ9CrPluR6/LLInRQGgUZu87pFufjxvmoMDXPsWDkHO1bOweDQMF7cV411i3I9Pi6hv3WLclF+wYRNSwvw/rlGDFkeKHfqU522H0znOZhscYTPfm6MsR2MsSbG2BmVz59gjFUxxj5jjFUwxmb6yhaCCGaEybYtYTbzaMJm0OtgEHlgpCdE+yXpWoXRhDPX2q1u2uJy41q8l9RgLPCJAyNEI5P0hDFejUN+fJ65IFKtqctjF9UKownf/eNp6/Ivv2p+8Igrd31c04KlvzhkfeAK3mHOvISEuZ1Q2UKptD1gX8VLIDkuyqHd4uv7/QcKVLd1lWcXTJacU7Vwse0rZ9utu9aq/KZ4VnYS/na6QbXPtt4B1b60fO+VxNJz19tVtrZn7UK9nUAlJGsX7iuOwubO3+jAlnIjth0ySn5nL+ytwgt7qyTbkrhEeEJVfRvyx8dDnxYXaFOIUUp8tO9CJnVjzcUTkmIjFb/jDECypcCCEnfmpiDbhXC3yHAGfVocHi3OwkOznFez1crYaHUbXUXnYCyghZhI6dR1cUEa5lkKhWghwk8uWZ6+LPL0pavHuCHoy20eGjZfn4dmZVpTKzxcNAH3FKRZU3F4clxCf0PDZg/r1SV6LCvMwEOzMrF1RTGq6tucth/w8xyktjjCl1rtLgBfdPD5JQALOee3A/hPAK/40BaCCFqEyfatLrO3QvX1do+EkQqjCWevtaPIEvd+9FKLU7HDUdia1j4FMaKjdxDLiyYolht3Fy2JA31Ral5AeIYuL5qAxvY+r8Yh1zR1IiUuyisuqlX1bfifr9h0+Lv0qVi3yD4f1dmGdsSNCZd4hznjiLEFFUYTdh+pAwBMz0ywfia22aDXoezxIqzZfdLqsg8AafFjVD3IXvmoVnJ9C7O8F0L16seXNJ1TpXPwvqhih5hPr7biSzPVB+ZJMZEeCSpyAa7CaMKvD6h5Daq3ITA8zJ0KrmLx8H8/uIj3zzVa3w4KnmpvV13H21XXPc6fRRACaxfq8dfnFyA7hRK2E4GhrPQOn7W98T7zi5Al08fjg28vsvt8WkYCXn5slur+/zwnGztXaU8o/9RdOfjg24vw0iOFeOmRQpft9QcbvjDF+UYOyE2ThozvWDkXj82xrzqqxpwc7UKTp3iSg9OTl66BQm7z2oV67Fg5V/JdfOmRQuxYOdcu9YU7xyX0J+5X+O6L23TUfjCd52CyxRE+E4845x8BuOng8wrO+S3L4icAsnxlC0EEM4Iwcu66Oazn1weNbnu1iF0ef7fmTkzWxaGupVviBaSEp2FrVfVtVi+WstIi/OyxWYrlxt1BMM2g12H2pGQA/kscKBc7Dp5vwvKiCdh2SCpIaAmrU8LbLqprF+oxLzdVsm5oWNmr5urNHsn3ottJzpGq+lZU1bdhcUG6eQUXfyZ9K3KrewAdvYN2OR6URJH1eyrx3N25XrmeSmJld/8Qnv3NCbfam56RoLp+0VT1HEmVV1o9ElSEe8Izu05Y81R9/R7XBg/i8/nY1iNYv6cS99+WrmnfYQ6cqLuFh4sy8esDRnz/z1VYv6cSW1cU4+ePzULptqNYv+dU0FYCIQiCCAUoWtN1PE1R4MsXjbY+gjzOTwsj4BAI3xAsUaLPAHhH7UPG2HOMsf+fvTuPb+K+88f/GsnyfVsGfGIsA+YyCBMgAgdCUpLU5CA90prQQFMHaGh2N9ndb0Pa/e53+4V0t99tt7tOgZIEGhKnafNL0jTkToPjIMLp4AAxYBljfACWb4xtbEu/P6SRZqQZaUa35Pfz8cgjaDTHZ0YjeeY978/7c5xhmOOdneJ1JggJVzqNGnfMmgQA+E5prsc3Y9yUxxOXetA3NIq12hy8f/qKX/vMbl6hsaWNsm13N9y4XHqDEU3GQdEMHX/UPGKDHSPWgtk/XD4NNec78eTq6bwgiKfd6vydomqG6+5Jb1qzw/QGI9p7XRexfOy2QigVwAdnnGsDOT4Vqb3g/DvNgIFOo8Zvv7cA6184iiderfN58OFQo/Mo48+UF7ssbu3KWZGuYmc7+nHh2nXB9wBL8WlfjLIyNDqO2gtGPLwkH7NEAllSTvtjl3rw8JJ83Ds/2yl450rNeSPGxk3447HLtoBturXLwTv1HSE7EggJP3SfQiYiX9SbJPLQIZeIjhMREfTgEcMwt8MSPPpfYvOYzebfm83mRWazeVFmpvQRcQgJF3qDEceae/DEqiK89WW7V5kn3GLIbBbQCxsWucxo8cVTEn+lWzKM6wwdfz7gYYM5o+OWv6Ivfn7R1q+aG+SR0q1OSCikqP7y3QZs2n8C2amuh8/d/PIJ7DjQgLvmTLFMEDnueoMRf/nSuR7QtQFLcGr6pCSMm8x4+1S7T4IP3KyvfRsXO71fWWbpFuOJO2cJZ+p8Y9YUvHS4WXQ5fvFpz3C/qy8faZFV88hRUkwUXj7SAsAS4N2w9xgefv4IL+BptBb55tqyshADI5bAKZtt91Wb5ZyfnBwTsiOBkPBD9ykkEvnm+kT6SsLhe0SB4vBCnxdxFNTgEcMwJQCeB3C/2Wx2fmRMyATgj+r6UjNawuUJjKv98fc+6DRqPKIrAACsXzqV1wbHPttJMZbCm/7OyHDVTU7uH/pxMzAvJwXx0a6Lhn789TU8oM3BChfdtQDLZyVUqLNn8CYA4MQlS2/mhGilT4IPbNbX02/UC67Lm9pDOSKFSrPTYvGDpQWiy3UJBGLkcCwgXlWhFa155Lgcu7/cY5ESr7L9rgDAzTETPm80YgVndMS6ll6n9e082GT795Orp2PT/hP45XsNAIApKXEhOxIIIYSEAnfXJxHRvSnM0CEnxDtBCx4xDJMP4A0A681m8/lgtYOQYPNH16VQyGjxpWDuj95gxNun2t0WtNYbjFBFKfATHxS+dkdqNzkpKfFLpqWjrqUX3YPuAx415ztto62JPeLcvEKDqRnOo8rMmJIEvcGIZ960DMCZEBMlOfjgKgDEfl/erGvDuuePOL3vr2LORZPEu8LVtfT6JPjL0mnUojWPuNvhngPc3w+GsR+nv3JGiXuzzv5v7VTnIuUxUfZLBHYUk2VF9u9hqI4EQsIP3c8RIkb6E7KJ8D0Kk2eeEYOON3Hkt+ARwzCvAjgMYCbDMK0MwzzKMMxmhmE2W2f5FwAZAH7HMMyXDMN4VtGUkDAXzMBIKD6BkVt82p/7IDUrjDvfUz7KHnPFVTc5ucfjVk0Gnlw9HZ3Xb7qdd8vKQttoa56ob+3D9rVzAQCDI2OiwYfjl3p4r6UEgIZHTYJPef2RAca4uUQvzEzwefBXrObR46+ctP2bG4Tm/n5w2/vBGeER5NQCXe06+vh1sJ59sARbVmqs67QI56A0IYREErrRdy9cMu4JCVX+HG3t+2azOctsNqvMZnOu2Wx+wWw27zKbzbus7//IbDanmc3mBdb/nIcDIoT4VSj+EWWzaq5Yb1ybOgeDNhy41Kwwfxe+FqLTqPHIrVMBeNdNjgGDyjJpN/87DzbZR1uTqXNghBdkUCktf36Egg//+vYZ3muhfWO7qekNRmzaf0J0u2az2fMgnoffj6bOQdv56usAomM2Wf+wfZQ8tt6ZWKDVMaOJS6jmkct2yJqbENfofCLEmSXwH4JP+bzh5RM/KjIeWBF29hEfcF3kghBCAowNvGzcewwA8Kfjl/H8I4tEgyP+vI4QyqjQadRObZE6ny/pDUb8tb7D1p1uqSbDknnE+VMv9dBIDXBUVWjxyheWwstyryj0hi5s2HsUX1pr60RHeffs4q26Nrx69DJiVQpbIErIfVWfo81hJDmh4Iovgzza/FRs3n8CKXEqXB8Zc7+AG7tqDCiekuR2vj21Buw82OQUIOofGoXeYHSZIVR3ybnmEdfTb9Tj2QdL6EKSEBJR3GWSerVuL1dtlhlWpd9n90Ix456QcBL00dYIIcSRTqPGihmWwsz3zJ1Cw4E78GWR9d98fB6PvSSeucOl06hxe/EkAMD5KwOibTvS5Dz+wbjJjIPnOvHgwhwAwMiYyati1lNSLMWsh0dNGBgWD9B81daPh5fk86Zd6hp0mk8oe+nLy8IBlbqWHpeX9BmJMVApFbjcM4S7505xMacwx8+Rm3XnqsDqjgMN2LKy0On7EqNSuD0/FgrUPBJCz3wJIZFEboDG99sPzXWFEzkPESlxiRDvUPCIEBJyT6v0BiOOX+rBxmUF+PRcp8ub3on4FElqN7kvBII4QoZGpWfHGKwFs9Pio23T2M/n6TfqsWn/CUxJiRVd/o2TbQCA/uFRyV0RhT7/1p4bktvMDlPPune+82hwQjKTnOsAAcCkJPH9AyyjrfXeGAUAHKjvcHpfqK4X17rnj+D7v//C9lpq8PQBbQ7GTc7Tb46ZsGVlITbvP4HHXhIuLyhU84jr2QdLeK8n4NeO+BGdT2Qicnfe+zMrihBCPEHBI0JISD2t4mbV/O9759Bw4AJcFVnnBtOeeFXacZN6gXp/1ed46XAzAH5gZcvLJ/HEq3V4xxooGRsXP6Py0+MBAPEqpeSgiFDtqHk50mtgcbtx6Q1GwfXtXl/qNC0nNU5wfbnpwtNZdS19yE61BJh+ek+x0/tsXS+xz8ZsBg6LBP5c1XuoOd8pGJCLiVJi58Em9A+P4cOzwgWzCSFkIgpmgMbdtVeodFvzZZ2hYIfDJuIDR0J8iYJHhJCQ4mnxaUpFdvbb72nx+CsnnQpQO7pPYibOqdY+lOQ6d28aGB7F26fasVFXgN3rS/H6yVbRddS3yS8i7svRvHxVfP3Thmui72nzU5EQYykpOD/P+Xix57TU7oJSsYHW+5/7HHf850Hb9PhoJbR5vik4T98zQkgkCXa3tXDgqru0/HV5tzz9DQoM+l4QMRQ8IoQE/UkQl6usGiLP0sIM5KTFYZ++2eV875527lolRqgOkMl6jeHYPcwVpcK7Pz/9w6OS5310n72r1paVhfjrqXaneVyN2CYkz5pBJSQjMVr0PZZOo5ZcTJuboeTqIp4NSmUkRMPQaa/r1Hl9GJ80dEralpQ2EEIICQzKlCHBROcfcUTBI0JIRDxfoD9wzl481ISz7f1u5xsVKpQjgs1S6hxwHtq9qkIrOQgzMDwqOSAhVCMoOVYlaVkASIpV2v6982ATrvYPu5jbrq13SHh6zxA0mQkul3X3pFZOMKa+tQ/vfiUtwKfTqPHihsX4p9UzbNMGhsclb0sMG3Bjd2tgeNSrgueEcEXC3yASngLRbc3TbBkGDGXaOKCMmMCi8484ouARIRPUrhoDGq7wAwt6g5FuCCPIv793Dqnx7oMsMmJHNj03bnrQIrs4lRI/e/O0LYji6rxTKizdzZ5+o942TWwkNCHXBuxtvWfuZFumFJdQzaNrAgEyAOgckBZ8AoQvvPQGo6xMp80rNJimtgSrTrf1Sqo/MduhJpRS4foG6esO10HGD85c5QW8mrtu+KT7HyGEEGEUKAlPkfAsk4q1EzEUPCJkgirJTcGumiYAlj90bKHqcL0hpKcjFtyEl3++eyb6bkgfSU2Kt60ZKDMmJzm9V9/ahzUlWZLWMzQ6ju8vybMVjnZ13u082IQtKwvxVl2bbZq7zB8x1UcuY1lRhtN0oeLdCwTqFQHAgvw0l5lFYhddbPBFqNucOy3dltHlugdH8cN9wiOmcb3/1RXe63GhiBkHt5ubEDar7CVrF8iCjHjJBc8JcadeRjCYEF96+1Sb+5k8VHPe0l1Y7M9FS9cgfvPRedHld9c04Yf7jkre3ltftuHpN+qxYe9R3P/c57La6krfkPRu4u7sPNjo1fJX+vgPb55+ox6vHZPeZb5dJKPYl/79vQY8/Ua9pIexQpnVwX6Qu6vGgMZrA7xpwW4TCR0UPCJkgtJp1NiyshAAcNE4aBvhjG4II8fGZdOQnii9e5cUroprb16hwb0Si28nxapQWaZBVYUWP37lJH778QXReasqtPivjy9gaNSeInXhmutgh5iKJXkeZVpxtfUM4cLVAfczgn/T8PgrJ/GPf/4Sb9W144k7iiRvb+Peo/jj0cu215+eEy/WDQB7ag147fhll/PIVahOxMDwGN760hL4So7z7XlFJra0BPd1wgjxh0IPH0RIkZvmemTOhJgol4OBKBUMWrqlBzu6B2/infoOLCvKwPkr0v5GBdqVPuGMXqkcawW+U9+BOhnB5+auG15tX4oPzl7BO/Udkh7GsqOvskLhQW5JbgpesdawZJjQaBMJHRQ8ImQC23SbBgvyUtFkHMTDS/LDOnBENY8suFkvXzR1oXvQd08MASA/Q7xQtBzRUQrsqTXg9581offGKI5c7Bad94PTVzA44n3dHgB47/RVry+APjx7xeXFamOn/aL9z8ftI89FRynw+ok2rJ4zGTsPNkne3qfnOnGrxp4txXZhE3OosQvpDt0V75WYESbG8cnogIyC5YS4k++iAD0h/jQ3WzjD1Bc0mYku389IjMFDt+SJvn+mvR+3FqZL3l6UgsHu9aWoLNPghQ23SF4ukKLcdKF2Jysllvd69/pS/HDZNMnLB+JSMUqhwO71pZKuqdmBLlih8CBXp1Fj3ZJ8AMDptr6QaBMJHRQ8ImQCO9rcjZbuG3hiVRFePtIS1iMqUbc1Z//w2peY6uObshYXT+3Yp1NS9N24iR0HGpCTGut23j8cviS5fe5sWVno8kkvl1h6+8L8NJxo7hFdLilGhRvWp6MzJttvHq72W564fna+Ewvy5AWwjnKCa0WTXN+QLC3MwKNl/Ivpj85elbU9R//nr2d5r5uNN8L694KEFvr5JpHM1fXJT1aJZ6HOy0nGrvWLJG9ndnaK7Qbfkxv9aKVwaEUr0oXbE2sX5jgFgFhLprkPlMVHR/Fe6zRqTOf8nc0QyWJMjrUsl+MmG8wXHl4q72GsTqO2tTtUHuR+q9QS1DzbMRAybSKhgYJHhExQ7I1+VYUWT66eiaoKra3+TDihhCM+bgbWbx5agKTYKPGZPfC2i3o97PkkxajJjAV5KXj7lLRRxHxl58EmyZlHH38tHHA52dKDcRd3A8cv9dhqFJXk2i+62Seuv3loAY65CD4JmcrJ+DK5qV9UkpvilNk0POZdX73/c98c3usCdbzkIBwhhEwU83KSZc3/3mnnv4EMgFumpuF0Wz+efE3aAxkAONPeZ7uG0xuMkJPkk5cWh5vjwn9bTrfzf+u9ue56p74D10fGsHaBcxf3+lb33c8Gb/K7rekNRly4eh0AkJsWi65B/ePRlgAAIABJREFU4cE82D1r6xkCAyAvzRLAUid63mVWIXAXHa1k8IrMh7F6gxFmAD+5PXQe5HYNjiA9PjoiHi4T36LgESETVH1rHy8NlU2dpRvCyLG00LkwtLdcPRl82JrmLNWp1j4M35TWHU3po79Wcs7xO2dNFpw+POo+ECN0Cc6OeLa0MENWzSMAuNZvrxPxSYPrmkfcemas2CjvDmDp1DTe66RYFTav0Hi1TkJYl7v9X4eEECFn2sX/Hkj52XQM0MREKW3/NnRed7ls1/URbD/QgBiHDUUpGPzD6hlYVZyJTxo63TfCatxkxqb9J7Cn1oBH9x0THFlUTGvPELJFMoJGx81QKRjbAxDH1coJUt0cM2Fs3ISPBf6OsXUN1y0R78rnWDB70/4TePHQRQBAe6/4SKhs120zLNcTrT3D0OalwHjdi5FjHQ5EtJKBUsFgbNyETftPSAq4cB/kPnVXaDzItbVpXXg/XCb+QcEjQiaozSs0TmmoOo06oDeEvhhlgro7iDOb4XJUME+4qk308pEWWSOJmcyWDCQpvC1yzZJzjoul1nuK/Sh+/tZp/Pcn8kac+fHt3n0v/+8Dc71avvqoc9dBGn2F+EqPSLYAIf4mNtLkquJMvPToEpeBkby0WOSk8rtBtffZuzu39bgudj04MoZVxZn4h2/M4E2fnZOM+tY+jJuBIhkFvdMTorGmJAuHGrswY0oSEqKV7hey2lZejOFR4Yc50UoGs3OScffcKQCA/HT+Ps/Okp5tVZyVhAe0OYIjs6YnqPBMeTG2ry0RXT4hhr9Pa0qybCOjJseqsKo4U7ArIHupUZARjzk5Kbi9OBN3zc1yGahyZ1Iy/xrhW6W5eECbY9s/KQ+qQvFBbii2iYQO3/ZnIIQQGcRGmZDa9Yk4c7zO9XUh8fvmZ+O145fROeA8YspdcybjgzPe1dYRo1IwkgNNvvI6p9i1p96ss6+DLWbe4MEoOLMkXpzrDUbUt/Y5Bfka3TwBd0fpkJ9/tX8Yj+47jhc2SK/HQYiY+Xmp+PSc9AwLQnzlvvnZePWo81DvL25YDACYnByLjj7hjJZvzM7CnOxkPPXnU7Zpd86ajJesdfpum5GJ905fEd12Xno8XtywGDXn+ef+Xx5fDgC2Bx0FPz0gug6GsddUemBBDn62Zrbtva3VJ/FOvbSu4ZVlGlSWaQS3deindyAzKQbPvvs1AKC8JBs7D9ofHNw5ezJOt/dL2g67bwDw6lH+qKAnf77a7fKTkmIxMGwP+D37YAnerGvFF03dWDkzE//1PS3ae4fwP3/jP6BZU5KNPx67jOzUOFRXLuW998oRz0YnnTk5iZcJ9eyD4kEvMUIPs3QadVBrDIVim0jooMwjQkjQeDvKxK4ag9NwtJQNYXekqcvnNaHY0daONAtnIDl2lwpnPUPejyj28hf2mxKT9Qr/7++cLrvb2r5DzbzXQp8rG3xVKoCTl/i1I3bXSB/dTcr223uHcasmnS4mCSERzZ+DcbCZweYwH/Ej2M2Xs30amZcQ71DwiBASVDqNGt9dlAtA/igTJbkpeP7zi7bX7M2zt0OxhzNuN7Wf/LFOdMQwT7GjrS0ucK59dO/8bFlD0Mvhq6yjQPfZ/1n5LNu/R6xFq2+Zli67G16LQ00YoQvgrdV12LKyEIcau7BlBT+It2mFd0G9kTHn7gw/KoucQCEJrvC+dSYTlbtARLDjFL7qtm5bTbB3CK5/K6Tsb7ADXYSEOwoeEUKCSm8w4uOvr3k0ooNOo8aPV1rSa1u6B2VnLkW6geExXOl37l7mDXa0Nce6AwBwa2FGyGcePf7KSax/4UjAtscdFpgdBtlsguwAZ1Is/3gLXSSvmKHGzoNNeOy2QhQ7dHP7ziLP6zoAloKphBAy0QQiUyVsfl3DpqGEEH+h4BEhJGi4NY48HdFhy0oNlhepcbZjQHbmEpHvvvmW4XW7BQrcbth7zG+ZR1FyhnNxIS0+GrUXApd9tE/fbPs3G4D5w+FmPLrvuKz1nL/KL+oq1M3hrbp2bFlZKPgd+KNATQ852GGNubZW12FPrYG6iRKvhUBCAyEeCXY3qEBk0rjbxYAeAxf7K6X7X7A/L0LCHQWPCCFB44sRHQ43deFsR79HmUuRyN/XRWzNoy8v9zq9V3O+EytmhHbwrrlLeGQdfznKKVwdp7L8ya36WyOeXD1d1nr+7k7388+YnIj//qQReoPR6YZi5pQkWdtzdLnHuWDsPXMn49cfXpjQ3UQJIZHNF8EZc6Sk7IhcYIRTV7BwaishoYiCR4SQoNm8QuOUJSFnKHVfZC4Rz7BD43Ilx0bhrbp2v2xvzEc1jwI8YBtvyOBha82j6ZMT0Gz0Logl1G2ttXcI4yYz/nrK+TPQ5qd5tT0hr59owwsbFlG2HyGERDBf1U7yN7adQs0Nk10gJORR8IgQErZ8kbkUaQJ1gZSREOM0LTs1DtvKi/2yPV91Wwu02Vn2rBw2cDVzcpLsoYHf+8p9UO5X3y6BUsHg7VMd+MPhZlnr90RJTgoFjohPUDIACRa3Ra998KeHEUnZCc+/auGJMo4I8Q0KHhFCwpa3mUvEc12DzoW4NZmJqCzzz7FX+Dkq5q+6Pe9+1eE07Y26dqxbIq+A9dmOAbfzLCpIx+71pRgcGQtIXafz165Tlh8hJKwFM6gQbvEMsSBYIAkdMzmfoS8vJSibiUxEFDwihJAI4u/08pYuy5DxQjWPAP8FYfzNX3V7zl1xDvooIL/73Kws72oW/frDc14tL+Tn5bOomyjxCboHIxORrcCzn6JIvvpehcv3U0o7KQOJEO9Q8IgQQohkrx23dLeanOzcbQ3wXxBm3M/FivzW/YpxbrcJwDFOIW0pvmrt569WYJ7jF7uxtbpOcHnjdedMMW8tyE+d8N1ECSHEnYgpmB0CpIyoFigh1BRCAoaCR4QQQmTrun5TcLq/gjDhevHd1HnDadoyTQYaO+UVzJay92c7BlBVoRV8r+HKdVnbk4q6iRJfCM9vN4kE3ibrToSuS5G0j5G0L4QEAwWPCCEkQmzYexR7agPTbWxo1OQ0zQyz37ow+XuUNH+1++65U5ymzctNkV3zyJHQBfAPbp0qmgW0vCjDq+2JtMIP6ySEkMAJheyRcH04EgyhdKQoEEUmIgoeEUJIhFhWlIHtBxqCtv2ma9fx6L7jQdu+NzbtP+GX9b5+otVpWvGUZGxfW+LVesVueMS6Dd42I9Or7RHiT3QPRiJRpAQX2ELZ4bI/oVDYm5BIRcEjQgiJEJVlGq8zWrzRcPU6HlyYHbTte6Mkxz+1moT877fP+C3TSazbYO35Tr9sjxBCwpm7gIjcMAQ3sO8uq4kd4IKCHb5FmVyE+A8FjwghJIJ4m9HijeyUWL93L/OXky09AdtWxZJ8r4tMC10cv3T4kuj873x1xavtEULIRBSIP2nhFuwIha5+hJDgoOARIYREkEDVPBLS3jeMd+o7grZ9bwjVcPKXV45cwsVO7wpYjws0d1ZWkuj8MVH0ZJsQQvyNm8kULt283GJ4/wsqocAVxbIICZyoYDeAEEKIb+ypNWBHEGseAcDw6HhQtx8O+ofG8Npx51pIcjBwvmD+umMA7X3CwcMf3FqAPbUXvdomIb62q8aAktwUnLrcG+ymkAnqQH274HS9wYifvXkaXYMjosvWt/YiJor/HP5K37Dt34ZrlocEDR0Dgl2Vu65b1r1P3yy4fbkZqu29Q9hVY8ClLstono3XBiQvu6vGAKVISsHW6pPoHx61dYuu8zJTl9tGrqffqMfZjn7MzkoWXbZ/aJT3esPeoyieYp2fsay79oJzN+0XPm8CALT2DPGWvdztPCJquGN/V7nY84lGRyXeoswjQgiJEIcau1ARxJpHAKWzB4rQYX7vdAf+/b1zgvNT4IiEopLcFGytrkOsShnsppAJav8XLYLTt1bX4VZNOm6Oif9RUyfG4A+Hm3nTajj15V45Yll3eoJKcFCGWJUSeoMRRy9286brDUZsra5DSW6K6CAIQv7WcA1KBfBOfQfeqe+w1VSS4nL3oOjDp9oLRly4eh1t1sCLUsFfr9wATEluimCW8lt1bTh/ZcBlBvPoOP8B1bKiDOw/3Gx7rVRYroUcsdMSY+x5E8uKMmDodA5iSRUKmVhC2N9VFvd8IsRbFDwihJAIsW/jYuSlJwS1DePhWvQoAhg6B6HNSw12MwiRTKdRo6pCi87r4tkdhARDVYUW29eWIClWPLCZnRqHHy0v5E1bwRnZ8ge6AgBA1yA/W4Y1MjaOrdV1+MntRbzpW6vrUFWhhU6jFh0EQUh+Rjx2HmzC7vWl2L2+FI3XpHePrj5yGdvKiwXf+8fVM/D8I4vw4VlL7TzHGoHvnpZXU0+nUWP3+lKn6VFKBV7YcIvge6zBm/w+25VlGmxcVgAAOHdlADsPNuHv7ihyWo4NeKXGq3jLPiOyz1KE6tUO+7vK4p5PhHiLgkeEEBJBgp2SHKoXUxPFSer+Q8KMTqPGgwtz/bLuKEWo5gaQYFpamG779y0Fabh/gWWU0KyUWNt09kY7MUYFMQyAJ+6YjlsK0mzTpnDW8dQ3ZuCWqWn4qq0PG3UFyE6N5S1/tX8EDy/JR7FDN62Hl+RLvtGfl5Nsa/v5q9dty+o0ajxiDV4Vqt0/VLqlIA2VZcLXD4/oCqDTqDE325K5Miebn8HiyWilOo0ad86axJu20bodnUaNb87LAgDkpcXx5lmY7/yA5O/vtBznM+39eHhJPv7hGzORmRTNm+euOVME21FZpsGcbPFucq5qCYYynUaNb5fmAJB3PhHiDgWPCCEkguyqCV7BbBJ8pgBmfn3Z0kPnG/Ga3mDE+zIzF6Qak/h9UCdGu5+Jg0JS/pUS69+SrMeaLd3ElArgWHMPPjp7FWu12bx6RWyNousjlqwhpcCHbgZw5GIXDJ2Dtowj7jqONnfDYBzEE6uKsFffDOPATd7yD92Sh5ePtKCho9827UFtDl4+0iJYI4mLAbBWm43Tbf1YXqTGlb5hrOUsqzcY8fqJVqzVZuOicRBrtTmi65o5ORHHmntEB9x4Sd8MvcGI0+2WGkxn2vm1mL5qkz96qN5gxJGL3VBZD2y0ksFe63b0BiO+aOrCWm02r0bR8iI16lqcH5Bwj/PLR1qwp9aAgeEx2/vRSgYfnBH+jdlTa8CZ9n7B9wDAcG0Qc3OEg0uh/DugNxjxt4ZO2zFxdz4RIhUFjwghJIJQn/aJLZCZX7848DWdb8QrbC2O26YH96m48fpN9zNxxKgU0ObRue8vfZwbf38YNwHrluThp/dYuizduDmOD85c5XXb2lpdh2ferMfAsKXGzj/eNdNpPe29Q7YuQauKLVk03JpH7HtLNRkAALPDL3RJTgqqKrT4n08bbdO+sygPVRVabK2uswVShEQrGdScN6JiSR4ONVr+X3O+E1tWFmLT/hPYtP8EtqwsRM15I7aVF/Pa5ahz4CYqluSJ1jz61Yfn8aM/HLdl72jz0njv3zNXOKtHjN5gxKb9JzA2bkKsSolnyosRo1JibNyER/cd47WdW8eR3U+uPbUG23F+cvVMbFlZiO0HGjA2bjnWxVlJiFEpbV3qe2+M8pbdztlnoe+0GWZMTop1mh7K2N9V9phwzydCvEXBI0IIiSByR2chxFM/L59FqfDEK/Wtfaiq0OJq/zBv+qriTKTHi3cXkkKTmYAFEgM83y6V123unrlTkBIvL1vJUbADZo64XbYcpcbJywRakJeCjAQVohQMijLl1+GLU/n39mRZUQby0hMwbgKeKS+GJjMB0ycn8rptVVVocdjQjZgoS37JLIERwIzXR2y1ZNja1LdNz+StQ6dRo761D7vXl9q6YrHOXxuATqPG4oJ03nS2Zk19a5/o33ST2bL+vPQEbCsvRl56AqoqtBg3AWtKsrCmJAvjJss8lWUaXg0cR//9fft6hNw2Q43pkxORkxpn3TY/CJaXHi+6biH1rX1YU5KFB7Q52L2+FJVlGuxeX4oHtDmYMSWJ13ZuHUd2P7kONXbx6vmMmyy/H2xbNZmJ2L2+FLoiSwDv+sgYb1lNZgI0mQl4prwYd83NwjqH4NRdc6aga1BecDnY2N9V9phwzydCvOXfvFBCCCEBRZkgJFAW5Ke5n4kQF9gabZOSYnHnr2ts01/csBiPvHjUZbaEO588tRIAUPDTA27nXT17Ml4/0Sp53T9fMwfpCdGS1i3mv76nxcJffOTx8r52+Ok7RPdn3dKpeO5T6V1U33p8Oe+13OO0ddV0/OoD4ZEjfeGhW/Jx3/xs22uhWj86jRp/+8eV0D37Cdr7hp3eB4CS3FTbDTobT5kiUDeJPc8PNfIzP75lrfX1g1sL8NkF/nvcYtm/fM85Iyg5TiVYUFssoO8q0H+rJgPLrcHM7QLZR79bV4rEmCj8x/uW9xZOTcMRzghxcjNehWozihUH12nUtv1nPyfu8di3cbHTujev0OCvp9rxk1frbOsYN5lx8Fwncjk1lByXZb1y5LLt3zmpcUiOU+HLMKonKOf4EiIXZR4RQkgEoYsDEihfXu5xPxMhklCp/VDGhHR1l+AROipMGB4qd01m3w/HfWN/WugcJsQ3KHhECCERhPq0k0D5xTtf0/lGIgITlnfFgUOHx7/CLXRqDrcGw7neFCHEMxQ8IoSQCEJ92kmg/HzNLDrfiI8ENzoRjK2bw/EOfAITCjBOlE+Q3XWx7J2Qji0yji9DurWEhDyqeUQIIRGEah6RQNHmp0GTmRjsZhBC/IxutyM72Ocusyysd93s+NJ3O0MZi2QioswjQgiJIJQJQggJP+F8d+qZsLrxDKe2SuBJIGjinaHiHAMwoXhsHE9ZbzKOxJaM5IAiIWIoeEQIIRFEaJQNQvwhsm4nyUQWjNgI3XiGF8Hi2F6vMzx+RX0ZiAkWqnlEiG9Q8IgQQgghHtEbjNhVI30Ib0JI+Am/UEHwyIkJCgU0ghHkkJsFF86JaL4MfIVV9iAhPkLBI0IIIYTIVtfSg63VdVRni0w4E+2Wke6RhXHDPN4cI0+y0IL5mYRz0hxlIBHiHQoeEUJIBNmw92iwm0AmiH9752tUVWih06iD3RQS9oI82hoFR4iIQGeXSM2MCUYAR+xQRPrXh34fCLGj4BEhhESQZUUZwW4CmSDum59FgSPiI8HNBgjHGi6BFGnHR05ASGpWkDfBHDPv3+GXGRNOLfbkXA7nTCtCfI2CR4QQEkEqy6hgNgmMt091QG8wBrsZhHgvsmIjPkeZF8IEi2hH4LFyDLiEY4CLFc5tJyQUUPCIEEIIIbL9y5pZ2FpdRwEkQiJcBMZDZPNnyCHcjm84ZaJRsIgQ36LgESGEEEJk0+anoapCi/rWvmA3hRCvBONWOJxuaSMxm0YqV13cfPUZerKeifyZeMOTwNdErfVEiJCoYDeAEEKI7+yppWHTSWAwDAOdRk11jwghYUXO6GbsvP4IFIRbLZ1wDJZ4nSXlYvEw+/gI8QnKPCKEkAhyqLEr2E0gE8SrR1uC3QRCfELuTeBEy/oI9Ihj4ULoqIRbQCjSOXZbo25shHiHMo8IISSC7Nu4GAU/PRDsZpAJoHhKUrCbQFxgGOZuAL8FoATwvNls/qXD+xsA/ApAm3VSldlsfj6gjQSwYe9RFE/mn0t3/OdBXOq64dV6F/7iQ4yOS7tR/K+Pzsla9/3PHcLDS/M9aZZN+W8/82p5X1v4iw/9st5dNfKzYVu8/Ozd2VVjQGZSDOpb+7B5hcY2rSQ3hTePUgFcHxkDAOw7dNFpPbUXOm3Lfd7oWe23DXuPQp0Qw5u2p9aAQ41dWFqYwWsTl9kM6A1G/PVUO6ZmJNj2Q4jjvgm972r552ubEButwIlLPZZtOwRgTlzqFl3W0f3PfY7ZWclu2+yqrY70BqPTZzl0c5y3TEvXoMtl2Pkcj9Opy71o6RY+H4818/db6udBSDijzCNCCCGEeMSTG0PifwzDKAE8B+AeALMBfJ9hmNkCs75mNpsXWP8LeOAIAJYVZWD3Z028aYbOQYyZvMsQ6B4cxTDnBtKVU639stZdkBGPHQcaPGmWTef1m14t72vdg6Oi73kTzHEVtBDz1pdt7mfyQnp8NLZW1/HaVpKbgq3VdbbXSgV4n7He4BwguWgchFIBbK2uQ05qnEdtWVaUgddPttpeH6hvx44DDVhWlOHUJq7RcRM27T+Bd+o73B5jV+th33clSsFgx4EGWwZaW88Q7305XcMaOgYktVmM43J6g1Hws3zhc3uwT6kAXj162dZWoWXY5RyP0xdN3aI3y4PWwCJL6udBSDij4BEhhESIXTUGGvmKBMy2N7+ii+TQtRhAo9lsbjKbzTcB/BHA/UFuk6DKMg023VbIm/ZMeTEK1QlerfeZ8mL84dHFkuaNVsrrllXf2odt5cWeNMvGy9iYzz3jYn/ePtXu8Xo9qYnm765fX7b2oqpCy2ubTqNGVYXW9nrnwSZsKy+2ZR4pBe6Y8tPjsfNgE6oqtCjM9Ox8rSzT4NsLc22vq49cxrbyYlSWaZzaxNU/bGnX7vWlbo+xq/Ww77vy+1rLsThuzTz6+OurvPfZ6VJERykktVmM43Jbq+sEP8tHl08DAJy/OoCdB5vw/cV5AIBLXYOCy7DLOR6nnNRYtPYOC7YlMynGaZo3+0ZIOKDgESGERAh3TxcJ8aWNy6ZBp1FDbzBSBlLoyQFwmfO61TrN0bcYhqlnGOZ1hmHyhFbEMMxjDMMcZxjmeGdnpz/aik0OXTwqyzTIS4/3ap3szbcU31ssrwvad0pzUVnmXbeUBXmpyE6J9WodvuRqf+6ZOyWALQGWFfn35nt5kXChf+60h5fko7JMgwV5qQCA7y5y/no0dg7i4SX50GnUXgW8Nq+0H/t5OSm8z8LVObxRVyD5HPcmoFFhPRZ3z5kMALhz1mTe+3fNmSy0mCA5bZaCPf6OykuyAACN1yyf0eYVRQCAyz1DossAzseprXcYk5KiBee92j+CxBil7bWv942QUETBI0IIiRDuni4S4kvTJyViT60Bj+47ThlI4emvAArMZnMJgI8A/EFoJrPZ/Huz2bzIbDYvyszM9EtDXj95mdfxZU+tAZdF6oywpk9ynemxp9aSiRmrcr7UjY2yT1tTkoU/H7/sNI8rfz7R6nJky2ilAioFP5spIVqJ5UUZttdfXu7FtYERl9thWzknO9npPaFp3mD3R+nQ7gW5qTh4XjxoqJCRtLXGekPvzuEm7zJo3bXp80ajYJau3mBEWrwKj6/U4OUjLdhTa0Cz8QY23VaIN+ranM6ltdocvHykhbcuObXF2Xk/PXcVDICF+an4qq2Pd25x28SKVSkQG6XAXn2z5Gxjdj3l86QHAmNVCmy6rRCvHr2MPbUGfHbBiMduK8Qhh/pOtReMWFOSBQbArZp0l53Y5LTZnR9bPyeh9RmvjyAtXoWfrCrCy0da8MHZDt5rsTawx+n+BdlgYAk0Xhtw7mKaGBOFtdpsXB8ZR7RS/udBSLjyW/CIYZgXGYa5xjDMaZH3GYZh/pthmEbrU6+F/moLIYRMFL93qB1CiL9sP3AWOw404MGF2ahv7Qt2cwhfGwBuqkQu7IWxAQBms7nLbDaz0YvnAZQGqG08e2oN2HGgAT+9x95tavuBBjQZB10sBVy45vr97Qca8MgLRzEyanJ6bxkniPPhmSsYGjU5BXtcKclNcVPzyAxVlIIXpLo5bsKhxi7EWLvIKRi4reuUEBuFKAVwpr0fKoeudWfapddpkrJn2w80ID5aif0OXf0KMxOcHkpM5nTXUbqJlnBvpj9x6O4kRkoNHVcfl1LBuOyGtyAvFVur63htY+vgPLduIf7p7mJsWVmIHQcasOX2QqyYmYmxcRPvXIpTKfDx11exZWUhtlbX4aKb81UMe/5vKy/GGz9ehm3lxdhxoMEW/GTbtGy6PaPlqdUz8OLGWwBY6uy4C1iw6/nx7RroDc4jsootf/ecKXj6m7Psx2JlIbZ9cxZ+89AC3nxbVhZCb+hCxZI8fGHoxooZ/OybGO73YMwkqc2u9oX1z3cXo6pC6/KzfGr1TFv7f3y7Bk+tnim4DHe5H9+uQe0FIyqW5OFQoxG5qZYMwUUFabZ5x00mvH/6CuKjlYhRKfDUXTMASPs8CAln/sw82gfgbhfv3wNguvW/xwDs9GNbCCEk4u2qMUBm6Q5CPNY1OIo52cl4+1QHLnV5duNE/OYYgOkMw0xjGCYawPcAvM2dgWEYbhrIfQC+DmD7bA41dmFbeTF+aK1RAgCazAREuQnmfHuhUC88u/QEFWKjlXj6HucgAvdGPyMxBquKM2V1XWvuuuGy5tGs7GTcvyAbzz44zzZtxqRE3F6cicRYFQAgMzEaGQkqJEYrxVaD3etL8Z1FechOicXs7GTEcTJf5udKzzxKjBXfBis9QYXnH1nk1O3m6sCw07SRMXsQpUDtunshN7A8Q2CExlXFztlsjznUwGJxz4hnymeJbnNutqXrl2PAjdU9eBNVFVpe2+pb+3h1cMZNwLbyYoybLO89oM3hfeYvbLgFa0qyMG4Cqiq0TkWkpTCb7ec/21WtskyDbeXFONTYxWsTt63jJkum8e71pVhTkuU2eM+uZ9wE/Oa7CwTfF8J2HeUeCwBYWpjBm489BnnpCZb5ODHRVcWZeJDzXS3OSpLUZlf7wsVmXEv9LMWW4S7nuD/s2Z4Wb+++9oA2BzOmJOH5RxZh9/pSjJsg+fMgJJxF+WvFZrP5M4ZhClzMcj+Al8xmsxnAFwzDpDIMk2U2mzv81SZCCIlkSgXwtwb/1CQhRMjp9n7EqRS4d352sJtCOMxm8xjDMFsBfABACeBFs9l8hmGYfwNw3Gw2vw3gCYZh7gMwBqAbwIZgtHXfRkumy+i4PSDxyVMr8ciLR1HjorvUnbOn4PWT4qNynfz5agCAyWTGjvf4WUJmeDa5AAAgAElEQVSfPLUS055+FwBw+Ok7AFiGI5fqL48vQ2p8NLaLZB/95fHlAIBmTpBq7w8XY1JSLEp/8REA4J0nbkNmUgyefO1LvFEnvB86Db82z9Idn2Bo1FK890+bdZj5s/cltXdSUiwGhl0HeNnj5WjR1HSnabOyknG4yZLB8stvzce3dupF17t5hQa/tB7/vzy+HAU/PcB7/8UNi52mzRQIMgGASqnATet5MjdbvKssW+8mKyVOcJj1LSuLnI6t49DqYkOts5+54/IXrl7HRxIzq7jY85+rskzjVIOKexzZtjm2QQx3/iGBEQjdDSsv9X22LeuWTMXsf/kAcSolXtxg2T92tDP2u+Ep7nFgefJZCh07x/1g/9/SNYT9X1xC2XQ1Pjpr+YyffbDEaX3c/xMSqfwWPJJArJijU/CIYZjHYMlOQn6+vKKGhBAyUbBP18RuaAjxNQbAuHOvIBICzGbzuwDedZj2L5x/Pw3g6UC3K9Dk1KAJhGC0x7+DlwVn2DgpWzWLzGX243Bu/h4pLpBC7KsTdJH02RLiqbAomB2IYo2EEBLuNq9wflpJiD+ZAZRNz6A0feK1QN6oMgIRHKFpPt2mj/fQ1+sLJd7epPv7sxTeZsA3SQKEPltC7IIZPHJbzJEQQog8z7xZH+wmkAlEpWTweWMXjbZGAs7XN3T+zEbxFbFMGsLHfpaRHGAjhJBgCGbw6G0AP7COurYUQB/VOyKEEM/pDUa8dqw12M0gE8j/+/Z8REcp8NdT7cFuCplgJnpYIJKzIUT3TeY+U7DNWSSfN4QQ//Nb8IhhmFcBHAYwk2GYVoZhHmUYZjPDMJuts7wLoAlAI4A9AH7sr7YQQshEUN/ah5cedS6+SYi/vHa8BU/cUYSOvuFgN4WEuWB0NfI3bujC17sXeUfLd4JxLoVB4hrxEH22hNj5c7S177t53wzgcX9tnxBCJhp3I6IQ4mtHL/bgq7Z+7F5fGuymEBJWbFkxFAXym2Dc9MuJW4V8TCICA7qEEO+ERcFsQgghhISeGJUSa0qyaHhiEnDhkKkk3EKHqR5GEGTtv4+jFP4+9FKCPpQNQgIlDH5qCAkYCh4RQgghxCPfKc3Fsw+WBLsZhEwIEz1gwr2Hd1XPiJ1voh8vf3EXTKHjTkjk8lu3NUIIIYREtrfq2pCTFotxE3WbJN6R+3A/3JIB7O31zZ11oPY/GHEAbzM93LXZn1lrFDghhEQyyjwihBBCiEfm5iRj+4EGKOlqghAnvo4jeLw+H8dKuAESfwRL/B2AMfthA5HYtcnTXYrEYwH457whJNzQ5R4hhBBCPPJ5YxfuKM7EocauYDeFTDBSb1BDrzaSb9oja7e8uOcNtaPHYly0LFTb7CjU2xlyX50gocNAiB0FjwghhBDikZlTElF3uQ+P3VYY7KaQCSYcbmy5TfRFECsYiQ+hlGvBPYSuah4RQgjxDwoeEUIIIcQj569cx5aVhTTaGvFaOASDfMrL2EfoZVQFGMWOCCEk4Ch4RAghhBCPLCpIw86DTdAbjMFuCplgXHVb8kQklzPx566FYgwrFNskJFxPOV9/98JFuH5ehPgSBY8IIYQQ4pHjzT24Z+5k1Lf2BbsphASM1JtnlzebYXz/zes+FsJ31IEscBzKx8FTEzVIRAgRR8EjQgghhHhkeVEG/nS8FSW5KcFuCiFekZOt4km9HafVBzDYECkhAG4wI1RjNaGe9RTq7QtldOgIAaKC3QBCCCGEhKdjl3rwz3fPRH1rH9U9Ih7bVWPgBSD1BiN6btx0uczXHf0+3f7l7huS53+8+iTy0+NF39cbjPj39xuQl2afZ5++GYmxSgzdHLNNe/qNenx67prLdpXkptgy+26OjUtuI5e7Y8m2BQCmZiTwpp+41G17j9XeO+R2fXtqDXj1yGX837VznbbB9cN9R52m/cf7DYLrHJGx/7tqDC7n1xuMqG/tw+YVGsnrcwySS12H0LIA8MbJVizIS5W0fX/TG4z4/WdNgtPlHqeZkxN5r5UOqQreHDfqIk1IcFHmESGERIhdNQa6sCIBdfecKags00i+sSBESEluCrZW19leb9p/AmfaXXeF/O0nFzzenuPvZEluCt6sa5O8/NJpGXinvkP0/U37T+D8lQFeYEipYLDjgD0ocry5G+/Ud6BvaFR0PUoFsLW6DiW5KSjJTcHAsD3wJOe3PiZK6Xaet+ra8E59h9PN+qSkWLzlcGyau+yBttNtzp/TnloDdhxowK2adN7n6rgeAPhbQ6fTtDaR4JSJk27k7vwoyU1Bzw3hY3vh6oDtuErleI7qDUbBdQh1X2OXbe3h79f0SYnOMwcBuy/LijJ40y933/DoOD35p1O210oFeOe92HETW5fQMQ+WUM12IySQKHhECCER4lLXIDbtPxHsZpAJZGpGPHbVGILdDBLmdBo1qiq0vGnu6q2MjJk83p7jDahOo8aD2hzJy6+/dSp2ry91Oc8LG27BL+63Z93sPXQR28qLMTRqyYbZ9uZp7F5fimUuMvZ2HmxCVYUWOo0aOo0aSbH2DgNybqLjot0Hj6KUCuxeX+qUQdjeO4Qoh9QRBeej+c+Pzjuta8eBBmwrL8b2tSW8z9VxPQAQp3KeFqUU/uyjOcv/z98ahXcElu5FOo0aqfEqwff36i/ZjqtUjufo1uo63jpcdQdjl/3gzBXe9Hm5oZF1xO5LZRn/IcCBrzo8Ok6//u58AMC4yYSdB5uwrbzYaVtS1il2zANtwo9sSAgHBY8IISRC3Ds/O9hNIBPM87UXqd4R8QnuzeRGXQEeuXWqy/lvmZqGggzxrmNCHlxoCRA9vCTf6b1nymcjKYZfzSE/PY73OpoT1NBp1FheZG/zVE43to26Aug0amjz02zTvlOai8oyDSoWW7a9/tap0GnUyEyKEW3vw0vyecdFxQmecPfBFxksbJu55uUk48jFbmzUFeDOWZNt07NT7cdl9Rz79FlZSQCAWwrSbIEInUaN22dm2rbBXQ8AVJYVYmlhOgBg5hTL8o8un4b5Ar8rSk7U6u65U0T3hc0QiVbyg2a3TLV8Hhusx14unUaN26ZblnP8bKQsu0FXAABIFwlqBYvYvmzQCR8nd7GU24snY3FBOm6Om/HwknxUlmm8Om6riid5tCwhxPcoeEQIIRFCp1G7fRpOiC/RE1niK9xuWHv1zfjTiVY8sapIcN7k2Cgcu9SDjr5hWds4eK4TT6wqwstHWpzeq7vcw+uWsrxIjZbuIURZAxbRSgY3x+1z6A1GnGrtRaxKAZWSwSVOzaS9+mboDUbUtfTYpv35RCv21Brw7ukreGJVEV450gK9wYjOgREAsNWFieJcmb9snYc1Om7ivcdq77Vs29NvY7SSsbWZq613GE+sKsJefTMONXYi2tq49t4hsHG0j85ctc3f0DGAxQVpONbcgz21loxEy3Hq461HxQnC7altwpn2fqzVZuP8lQGs1ebgpcOXcP7qAG8+ABi39lv70fJp+OA0P4tHCFvzqDDTUsepvs3SjlePXfaoi7feYMTp9n7bOSRnHXqDEX86bjmn+6zdDwM5GhxLKKNPaF+eWFWE1463enycGjuv247TnlqDV8fty8u9Hi1LCPE9Ch4RQgghxCP//q15tmK+hHhKrJZJUpzwuC7qxBjERytld12rqtDiydUznbq+sNsvn2fJZslOicXnjUaoFAzGTWasW5KHGJU9i2X/4Uu2LsJPrZ5hCzBxPbrvGH7+l9O21z9cNg07DjRgy8pCWxs27T+BQ9abYZVSgXVL8sCJD2HLykJsra6D3mCE3mDk1Tzi7wNjPS7Rgvs9dNN1oekYlRJj4yZs2n+Cd3NeVaHFUk0GxsZNGB414dullswtk9le2+fJb8ywzb+tvBh/2qzDtvJi7DjQgGferLd1NWLXMzRq4mVQDY2aMDI6jo+/voZt5cX4+OuruDlm2d6d1owT1k3rwdm8UoN/vGum6P4wsHymvdZ6UukJ9uOyVJOBqgqt7bhKxZ4j3HNI6jocl2Uz5r4SqBcVDOy+sAE/ALL3keW4r1tWFjqd954eN3bZYAlCrI+QkEPBI0IIiRB6g5FqHpGAOnd1gIplE6/Vt/bxgiG715di9/pSHGrsEpy/58ZNPP/IIpROTRN8Xwzb5cWx6wu7fbYgc6xKiVXFmZidk4xt5cXIS0/A7vWlUFmDREcudmFNSRZ2ry/FuAl4QJuDZzh1XXavL8WMKUm4faY9+DFuMmNbebEtOKTTqLGmJAspcZYuTBt105CXnsCrDzNustzY17f2ob61j1fziLsPP18zG4B4QV93o5TtXl+KB7Q5WFOSxQsG6zRq1Lf24QFtDraVF8NkbXtBRrwtIDM3x969jO2qVlmmwbbyYhw2dNvq27DrWVWcifsX2LtYryrOxKzsZKwpyUJlmQZrSrJQnJVk2Z5DO7klk+bluO4uW9/ah1Trse23BpEeuiXPNjIke1ylYs8R7jkkdR2OyybGWtrVeO265O37E7svjt83XxyncROczntPj5tQbTRCSGAJP9IhhBASdv56qj3YTfALBcMfZYeEjt9/1oSk2CiMm0BBJOIxx3OHe7NY8NMDTvPPy02FTqNG/JooPPDcIZ9tf2TUhD+faEV+Rjxe3LDYab64aCVGh8dQVbEQqfHRvLYCwHbrqFI6jRp/eXw5LhoHbaOybVw2DSkOtW6efbAEw6MmvFnXhqJJifhWaS5vPWy72G0IDaUOAPOtw71nJMTAeP2m0/spcSrRkcfY9XP345fv2UfH4n42hxqN+OOxy8hOjcPImAnGwZuiAavKMg2vALPjZ/zq0csA4HScn32wxPbvONUlfMDpFhcTpcQNN1lU3O3tO9QMYBQFGQk4f/U6pqkTsHHZNMF9lrI+R9x1uMpKcVrWOvNaGUXa/Y3dF8fvm7fHyd1xk7MudllCSPBQ5hEhhESIqRkJEVnzaFKyeEFZqUrcPKUmnqksK8Sv3j+PS12DwW4KmUDYTmK+rhljdjMYtz9qfPlyjYEsQRbMamfuts1+To6fZyDaLOczoJpx4YU+LkIoeEQIISTEXesf8Xod6QmhNbpNpNhVY0CMSkEj/ZGA8nciol/uEf144znRarGYIe8cCORN/0T4LCZqDGUifLaEuEPBI0IIiRAluSlBLSbpLwofXPnXitROId7bvb6UuhIQIkDKL5ec+9FgjM4VSuT+JQjk4QqXrJRwaSchJDRR8IgQQiKETqPGXXMmB7sZPicwkJFsdL3sH6YJfjNLgovOPgt33e38s83wITQ8PSGEEPkoeEQIIRHk3vnZUCk9u1AO1ctrX9SF+Ic7p/ugJcSRSqlwGt6bEH8L1m+V3MwfKXPL2Rd36/N3DZ1QiRUzkNaWEGmuqImeSUYICT8UPCKEkAgT5WGqjtIXKT5+4JML7NDctbD3H9+yjIwUqSP9kdAU7FtuTzJZxOI6gdgXX2+DYYL7k+o+iCY2PUT+EIRKOwghRCYKHhFCSITQG4zYWl2H7y7K82j5xNgoH7fIN3RF3tfT+eD0VfczeeF7iz075uHObLbUPJqakRDsppAJyF+34MEITlE8wZnjMeEGf6QcL0rsIb5EpxMhFDwihJCIUd/ah6oKLQ552IWo98aoj1vkGz9cNs3rddS39fmgJeJKclL8uv5Q9fWVfug0amxeoQl2U8gE5POMGh+Ho2R1SfNiZyZikGQi7nOoCJkMrgCZYLtLiEsUPCKEkAjB3sBfMt7waPlQvT46UN/h9TqmT070QUvEfXa+06/rD5as5FiX78+akhyglhBiF+zfKk8KVAeizYE6LoEK3IhtR/aoa9bPi4IAwf/uEELCGwWPCCEkgtS39qFiab5Hy6qUofknQeGDZl24et37lbjw2YXILBgdF610+f7XV/oD1BJCgk9uxoWcGEs4dMMSamOw2xQq6DhELvpsCbELzTsFQgghHtm8QoP75ud4trCfH0l6uvp3fJB59JCHdaCkkrtv8W6CMqFiaHTc5fsVi6cGqCWERDZfdFuLtMwa1/sj/YD5ujsiIYRMVBQ8IoQQYuHnx2tpCSqPlnv89iKvt/11h38zZB5dPg13zZkiaV5tXipu3HQdlAkV10dCsw4WIf7grjuaNyM/+qJOTLCHdhfafCgGrOxNCtzx8uQ4BOPTpCQa+ULxHCckWCh4RAghEeYrD4tDz872b/2am2Mmj5ableV9u/xdMHuvvhl6gxHLNBlu573UfQMblxX4tT1rtR5mnzkYGXX9mXlS+4WQUOfuXlFqJouce85wukENVlvD6BC5FCn7QQiZeCh4RAghEeZzD+vvFPu5+PH1Ec+ybX794Tkft8T32EyilcWZbuetqtBi023+HZ3sNw8t8Ml6bo5TcIgQT/n62+NufcEI6oRyPRjHtoVK0CaYh8wXxyBUjiMhJPAoeEQIIREmK9X1CFliykuyfNwS3zjT7n2XszkeZFWtmSetGxoAfO+WXOxeX4pxCclVOo1adlvk0ht8U8Db3UVCKN84ksjnr/MvKN2JvKl5FIQWU6aU94LRLPrJ9lywu40SEgqigt0AQgghvrVWm4P9hy/JXm7foYteb3t2VjLOitQXWlqYji+aumWv88lvzMB/fOBd9pEnAagVMyfhna+uSJp3+9oSAJbR7jISVOgadF0r6MQl+cdBjq3VdZLnjVIwGDMJXxTHqJRui2YT4q1dNQaU5KbwpukNRvz+sybB+Vu6b+CH+47iUGOX6Dr1BiPqW/uweQU/y2/D3qNYVsTvXrqn1oBDjV145NYCn7b/4LlO22vHQIHQMk+/Ue+0bvY4PHZbIW8d3ABxfWsvAKDZeEOwfVf7ht3ug9jxEtJ7Y9SvRf+Fjg0AjFi7Pv9B34y+IfHf2F01Tfj03DX0W+dh7/lrLxihmZQoeT/ltPeK9Rhf4Rxrdj+83Z7Y+SV3vRv2HsWthc5dqzfsPYqsFOeHTp5sw598dRzkbo8KrhNiR5lHhBBCAACfNHS6n8mNn62ZJfresiLPMm6Ks5Jcvi/3qbLUmx7ujZ9UJbkp6HYTONIbjHjmzdOy1y1HVYVW8rxzcixZWQqB46h0c5VAz2GJL5TkpvACnnqDEVur65yCPKzhm+P4W0OnLZggZGt1nWAAYllRBnYcaLC93lNrwI4DDaLbEuKY6SPW/uIp4r9d7DKdAyMAgMbO63invoM3uiT3OGytrsMoJ7WRu73/eN8SXE+MEX4mrBT6cnOw2xE6XkLOXRmwBWZO+6GeHHtsmjqvO7xjOe5KBYOXXDwgudI3hE8bOm1/Gy4aLetRMOLnhbft/dPxywCAzy7Y/24oFb7Zntj5JXe9y4oy8Mv3Gpym56TGip57vj5W3vDVcZC7vY7eIQBAs3HQL9shJJxQ8IgQQiKMp8/IUuO8T0Z11SVrd41wFoE7X7vLGpIQweDeOz3/yCJJ241yc8Plqa3Vddixdq7s5ZJjpX8+crrGzbLWuhK6APfFSHeEuKPTqHkBz63Vdaiq0KKyTDiboKN/GPHRSjyiKxBdZ1WFVvB7UFmmwbbyYtvrHQcasK28WHRbXGIjpom1f2F+mui62GWONluyEF863Izd60uxe32p03oqyzSoqtBimFPAnru9f7prJgDAJNKtxl32ILsdqb8bM6ckwdBpuZH+zcfnJS0jB3tsXj16mTd91FqDba++GT+4daro8lFKBbaVF9vmP3fVEjz67IJR1n7Kae93F+UBALJT42zTdx5s8sn2xM4vueutLNPgp/cUO01/7/RV0XMvEN2spfLVcZC7vU/PXQMAvFHX5pftEBJOKHhECCERZFeNwaPR1hYXpKF3aMwn2xezaUWh7d/ZMuoyVX0qvk7AdexIZQ0AcW8ydRq1pNHIOq+PSGkeAEv2AmDptjZVHS84z4I8S3CmeEoSFhWkS143ADxTXoybUgoqWcmpecR2sxBKzX/u00aXy756RH73SEKEcG8AH16S7/aG8EfLp6GybJqk9TniBopuKUiTFDhyR6dRQ2n9CrHt5/42CcWddBq1bYTG8nlZ0GnU0GnUyEiI5q2HnVelZHjLsu6YNRkAcG1A+DdrXo7rzAwpx5srNV6F0qmWwNiDPhrZ0ZFOo8aSafzfyZnWTK7v3ZKHH5UVCi0GAFhTkoXKMg1mWbNWp2ZYfpPvmTvFb4GGu+daauRd5GSnyD2urug0aqTGq7xer9BxY9cn9zsYDIFuo06jxoqZkwAAd82RXgeRkEhFwSNCCIkgJbkp+H8e1AdaNWsy7pAwUpg7rro5tXRb6nEkxCihzUuVvM4pKTEet8cEQKfJwJsn7U8M9QYjas53Il7l+k9gfav0IBxbe2XzCo1ofYRTl/sQrWTwVVsfjsuseVRZpsH6peJP2h3JqXm0xFoDo3/YubvdLW6CXDNddMshRA69wYjUeBW2rCjEy0daRAOgbELg859fxHunOxAbZfkeO36bXQVQ99QawABYNDUNx5p7bMFfd1wVzNUbjEiOU+Gxsmku2++4zKnWPmzQTcXHX1+D3mCE3mCEyWzGj5bz16M3GG2ZNI7719w1iNQ4FbZyMgVncbr7tnTfwMZlBbbXm26zBxA2uzneQnpvjOLC1evYoCvAXzldnXxJbzDiyEX+7+TV/mH8cFkB/nS8Fe+f6RDMxlQpGXx49ir21BrQ0TeMe+ZMRkvXDdw1ZzI+Pdfps8EEHJnMZqTGqbBWmw0GwL3zs9weVzndfvUGIxjA6byQ68VDTWAAaNQJAIBbC9Nt62O/gz9eqXG5jWAWINcbjEiLV2HTbfLPW0+3d6y5Gxt0U1Fz3vuu/YSEOwoeEUJIBNFp1LYuDHJc7h7E33xQ8+i/PxHPVPlLXTsAQKVUYEpKnOh8k5NjkJ6gsr2+KFIEVopxkxm3F2diO6eb2NbqOmxZWYhhgXop3Cf7s9zUWuJaag3A6A1GW5DMUaxKgW+VWkZl2yaj5hHDWNb7/50UTplXClzIy6l5xNZvEbofcFfgfIGLbjmESMXWLvnduoX4X/fMQlWFFlur6wSDOgkxUVi3JA83bo5j+4EGDI+ZsG5JHhJio3jB663VdYI3lmyNo23lxXh9iw7byoux40CD5AAS4Jylx7b/uXULsa18tq39dS09bve5qkKLf71vLqoqtNi0/wQ27T+B59YtxM/WzOYdh63VdYjhBLy5AeKt1XX43cMLkRJvD6b8fM1s27+/OW8KvjF7su21Oina9u/bZmTatiP1RvzclQFUrdPiX++bI+u3Rir22Hx/cR5v+uYVGvzLvXOwZWUhdhxoEMwEiVUpMTI6jh0HGvDNeVNwpLkH28qLcay5B1tWFsraT7ntfXyVBjXnjdhWXoxDjV2i25Mbe+GeX9zzQu5+sOd+xZI89AyNYt2SPHzR1I175k62nXu/W7cQ/3x3scfb8CfucXj6m7P83kah7yghEx0FjwghJIK46jbmyhsn23l1QDzlqnvEA9psAMDouMnlsNSW9xhEW6MirorOci2Zxg9kRCsVeKa8GDsP8mstVVVoMW4C1InOGU0LORlRx5rFb/wcsfWCfv9ZEyYJrBcAKssK8eyDJdBp1Hh27TzJ6zabgUf3HceWlcLdNIQGStNp1HhggbTuJOxn0SRQDHSt9TNjfWdRrqR1EiJHfWsfr3YJW2tEaDS13etLkZeegFXFmYhTKbCqOBN56QnYvb4UWzgjLlVVaAWzBw81dvFqHLE1kFyN3OZp+xuuDMhaZk1JFtaUZAkeh6oKLaIU9st27o0sux7uPnD3va13mPfacT52O1KzLWdOSeK10dfYY1OYmcibPm79sRs3AdvKi9HeN+S07O71pZiVnYzbizPR1jvMqxk1bhI/L3zRXnb9vt6e2Pkld73suZ+XnoCqCi22ry3BtvJitPUOi557vj5W3vDVcfBme4RMdN5XRyWEEBIySnJT8KN9x9zOV5ARj+Yue4bMwnzp3chccXWz9K512PsbI+P4qq1XdL7OgRFsKy/G7z414E5Nhm05IXEqBYasRWTPtvO3fc+8Kags02BOdgrvZomt7VB9pMW5/Vfto/vkpsWhtcf55kQIe1H52G2FWP/CUcF5Xj7SgqWaDOg0atk1j55cPd0pCMaKUylwY5SfRaU3GPHBGfHjJmRBXhpOOmRKvH/mKu/1x2f5r10FAQmRSmiYbfZ7WvDTA4LThZbp5NT8cazhwtq3cbHTtMoyDSrLNPhbw1Wn96QQa392Shx+/5nle+uYrSS0zLMPlgiuR2g/uNPYf+/buNh2vDav0NhG1mL3mfuaO5+r7Qhha+/4C9umiw4B7ces3e3Y979TmocF//YRbx6dRo2/PL7caZ3c/fN1EIB7DP2xPVffDzlcnftCPNmGP/nqOHizPUImOso8IoSQSCOhIMHVfvtNVkK0EvVtffjVB86j5rD1RHird7Fetii0kPJ5li4GMVEKl1k9CTFK7DzYhOfWLURJbirmZSeLzsuAwd1zLN0x8jP4hao/PHMVeoMROo1acr2gh6yZNekJKvTecK4B5I5Oo0ZBhnDB7C0rC/HovuMepdizT7KFKBXOn9HW6jo8uXq6rG3MmJzoNO3hJfm81795aAHv9ZcuuuUQMtFRbNU7YvXjCCGEBAcFjwghJILUt/bhdxUL3Q7rzgZyAMswzk/cUYQYgUCRymHaM+XF2OBieOzJyeKjqL13+gpmZyVheMyE3FTxmkdjJrMtVVypAL5q7xedN0rJ4GhzD9Zqs3HGaT4zNu0/IStY88fjlqGhFQyDR5eLj+QEAInRSuHpscJP5XcebMKTq6d7lGK/q8YAnUZtGzWIS2h4blfDnDu61GV5ut974ybSE+y1UL45bwpedsjOOt3Ob/u5qwPQG4wed5ckJJRQsIIQQggRR8EjQgiJIJtXaBCtUiBKqcBcFxk745yAQ2ZSDMZNlloR7ND2rL+/k5+9Mic7BW294l25hLpdsKZPSqiPO44AACAASURBVMRF4w2s1Wa7XEdyrMqWhj5usgzLLGZgeAxVFVrMnJKMVQ6jxf3dnTOwpiQL9a19kkeHeWiRZVuj4ya3I4llcgJl3ADVdeuoZStm8FPp2YCOJ6nwl7oG8cyb9WgT6EYnVvNIqreshcwPnuvk1VX6oqnbqc5SlMP5oWQYPLrvuK3mEyETgdmDnCJfjFDlarS3iYAJ5jBfxIY+B0ImLgoeEUJIBOGODpKZZCnczF7mTU23Z61wR6mJUyltAY2kOBWeWGUf7rmpk19zor61DwoPrxu/vjKAJ1dPt41GIyZWZc/o2bxCg6kZ8dDmCddkSoqNstU/eXEDv57D7KxkPPtgiWiwRuj695UvLgGwBLDc4Wb8sNlEeoPRVkvq7+6cgdWcEY68qcugYIBXjlwWrDUyNu48apws1uMwNSMehw322lBVFVqnz393Db/u0u7PLNlUoVQXgxDiH54EzYi4CR4LJISEIQoeEUJIBOGODnKrxjJ8vBnAnOxkdF631zn6+OtrvOX21Brw6L7jqKrQ4snVM23Tr3FqIwGAUgF80sBfVqp/vruYNxoNy7Er1sjYOO91lJLBl5eFC2xHC3S1Y3lyXT5uXSghxv14EkIX/vWtfbaaR6fb+nD8kr0mEDc76aXDzS7XrclM4L1+42Q71i3Jw8DwmNO8o0KpRw7bc+WBBZYR1cZNZnza0Ml7j81KYv3X9/g1j+6YNUly9zhCIkWwureFSqwhVLr3hUo7CCFkoqDgESGERJDNKzS2LJBm60g1S6al4d752Vgx3Z4d8vYpe1BgaHQcv/7wgmAGyWcX+MGE7QcasEAkCwgAnn6j3vbv/HR+UGheTgqvfSzHgMjImD2TRm8wYufBJheZSr69eWC7hXQP3sQ5FyPHOWK7bW1eobHVPNp+4Gtet6+t1XXYU2vArhoDZmeJdykEgLbeIV4GWGXZNGxfW4LKMuc6TGJHwFVtJZWSsWWQrSqehDnZyWjsHMQD2mxee10V3Y6NUuDEpR6PCoATQoi3wj0Tinp/EULCDQWPCCEkQrX3DQMA5ueloSQ3BTUX7Df5SwvtQ8V3DozgydXTIdT76RucblcAsLxIja9cBCWu9g/b/t03JG20MsdRxKKV9j9NbCZVZZkG83LEAy5CRZsZkX8LSYyJwmO3FSKKs+0XPr/ochnjgH1fdRq1Uxu+vzgfOw/au3ltWVmIX394ASW5KVhUkA5XFuanofP6CBJjohAbpcBefTP21BpQfeSy07xit0+uaivNykqG0ho9+sc/n0JL9w08saoINeeNKJ+XBcAy0ppjVtGm/Sds/x4zmfHj2zXYWl1HASRCiN9RrIUQQoKLgkeEEBKhlhZauq0xsAQ32GLQAPDZefvNfpxKiZ0HmwSLHlcsyYdKab9kP9Xai1EXNXaONdu7aTkO8/7/t3f/cVJX973H3wd2YUHM8mPQICuszIJozAqy/HCUQLD+hGo0SROpKMQSpZfQ1LSNkt6kt/cRTXrbpLfXhFWqpjVFY1ObGLUxPypb6ii4BF1AQZkFZQGF4ccq8nv33D/m+539zq+dmd35uft6Ph772J0z35nv+X7Pzsz5fuacz9m8J3nQKX6EzKkzndFAjDtSKRgKa8+RE1o2Z0LC4908T+6qYS6r5EGlZB6+fZpW3nCR/nL+RdE6/FGa1daOnY4dIXXnj5rliT3pxinnxYw8WrW2VY8sbkiaHyg+EfWruw7p2ZZ9evj2aXp0yXRNrx2h+5/bprs9x+8+ZNRZ6fMzxRsxdFA0Z5OR0YL6Mbrnmgu1bO4EPb95n26eOlY/Xv9uQlDoTEdXqKrunGFatbZVy+ZO6NEKcgCyUyo5csp9xA8AoGcIHgFAHxV/oVF37rDo35M9K4l9dKpDy+ZOSBrU2Lq3Xac9AYOOTpvxWPv4Zd5D+48m3S4+aDWoYoCWr9kULfcmAf/KVbHTqHzDBmn5mk269hPnakJcnqBfbtmX8Upg7rHX1wyP1uH2QG23j7k5yRSvVWtbo6utbdnTrlVrW3X9JZHk5LfNHJf0HF81+RxVVcZ+HNedM0wPLZqmgN+ngN+nGReM0sr5k9Ua7gqQnTW4QuNHDtXBjyL7y2YKxO5Dx6Ijzf7fwql64Jb6mCmCF378bD24cKqWr9kUfcwAo5h6fmxIpR5cOFUdnd2PcgLKTakEaUpNqeUYKrX6AEBfR/AIAPqo6LfDTv/au3LWNk8+n7MGDdCqta3RfDxe3/vV2/qGJ99Qp7WaNi4259HNU8dGR8Es8QRcbppyXsx2n5k6Nmk94wMq4aMno0m/pdgk4PEXC9ve+1C3zRyn37/0vJgpYpL05IbdSfM4NTaFEkbUxI9Q8g0brG3vfZC0vq7PN5yvcSOHSJLmTBqtpbP9enDh1Ohqa3/7wnYtmztB63ce0op5dTEjeZp3HYo+z998rl4PL2qIee65F54TU++75/i1dLY/Jrn4iqvq9OHJM9GE18MGd61Sl+wYvXYd/Cg6SsodoeadIuiO+PKOnBoxdJDmfzK2TQN+n+prqjMa3QUAucQIKAAoLIJHANAHNTaF9I4TxJAiiaz/ObgrevuqyaOjfx8/3anrLzk3mo/H65HFDfrEeV1lRtLv3o1d+ew3b76vqsqBqqocoNXrWnV2VYWu8I/SExtiRx5t2duu+55uSRtoGDa4IiFwkmzEjpGiQRkpMXfSZ6aOTboSWH1NtZav2aQTp7tWdXNHOrkjDo6dOhMz6iaZux7fqMPHTuvmqWP1yy3vKRgKK+D36eqLInmiPjXRp1VrW6Mr2LkjeYKhsN7YFxeYivsCfe/h40n36R3h465c92fXuqvjdT2Je4yp1IwYqoEmtguQ7Dx7ZygaYzS/fkzM/e6osExGdwHlIt0oPoIWxUGCaQAoLoJHANAH1ddU65nXulZUe/+DEzFLum/YdTiay2hI5UCtWR8ZpdPS1p4wYsWbJHnSx89OmCjQ0Wk18dxh+to1k+TuYu7k0dGEzK7vPv+mnm3ZlzTQMHxIV96eoyfPZJSAeejggTFBGUmaeUFXIuqmtw50PY+nKgG/Tw8unKrw0VPRMu9IJymyAl18MCqZhxZN0/e/MEWPLG6Irqa2YVdkpNFvtx2ImQ7o7relrV23X14bfY7mdw4nBHpeeOO9tOfADfa4Aa9hgysSjjGVfe3HozmP/nFda8rtvMEqY7pGKUlS2+Fj0emEyYJ7AHKr2EGrYu+/r2F6JIByQ/AIAPqggN+nGy+NTDFq3nlYr+1u1/WXxK6ctuq2aZIiOY8+M3WsOjoTR6z84vW9MY/5+nWTE4JCAwcYff26yerolB5bMl0PLZqmjk7p7z5/aWyljInm8fEKhsJqP9G1Mptv2OCkK3g1NoW0fufB6O2KAQMUDIWj063+cV2rNuzsmg62bO6ElCuBBfw+3T5rfMxtqeviaPTZg9MGRBbUj4kJDLmrqbkjjR5Z3KBVa1tj9h/w+xLyA23d054Q6Ln24o9nlIS6sSmk370TSVJu4sq7q/9FYz6mM06k75KxmY0aMor95n/vkRMp8zgBfVl/z7VTKsdfKvXIFiOoAJQrgkcA0Ectn1enyoFGG989rNtmjtOVE7umqi0J1GrooIEaOXSQvjKvTk1vHVB9TXV0xIrbt31h6/t6aNG06OMCfp++evWkmP24ASF3JIz79/z686IBLEma7wm2eLW0tau6qmvkUVXlwOgIHa/6mmr96U9ei94+09EZM2Xq5dAhrfTkZ+puJbBgKKyfewJjboDHRtNEpe/dxyfo7uhUzGpq3pFG3bkjUJtwXmpGDskoCXV9TbW+9czWpOXdjVw6cqwrWOcdTRTPO8Uw/oInPo8TAOQSQRYAKC0EjwCgj9p9+JjOrqqMXuT/99tdF/mPBXfprsc36sE/nKqvxeXjCfh9+tKVtZKSrxB29xy//qChJno71ciTYCisF7fvj14APNeyL5qY2htwSBYkSTZCJ+D36ftfmBK9fexUR3TKVEtbux5Z3BCT4yjVSmDe1dtc8SOUDhw9mTYoEj+qKFnOoGTH0Rsm7moq4PfpgVs+KUkaODD2vmQ5j84aFEmq/e6hYxqQwYWZd4rhAGP06H/vjN6Oz+MEIL/62zSn+OONf/9jGh0AFBbBIwDog7wBknuuuVDL5k7Qf2x5L3r/grjEx95RMsFQWP++aW/KkSXBUFi/eXN/zO1k+3dzJU3wOSN0rNVdj2/UwAGxgY1gKKwjx08nfY745NreVeIGVQxQwO+L7j/TwI139TaXe+zupcjQyoFpE2ZnMqqoEK7/5BjdNOU87T7UlWTbPcZ4gysHaPiQSn1lXp06M7ju8p6jD0+c1t//5i1JigaeMh1dBQAAgPJG8AgA+qD4AElHp2JyHj1wS70eWjQt5qLfXXbdG3TyJqOWIgGdO3/UHLOEu5so2hvoaWlr14L6MXpo0TQNc6akff2Gi7Sgfkx0lTDv490RMd79JFvFa6DnU+vUmU6tXhfKaLUv7zS07kYIWeer7rOrKpIGX74w/fyEx+RDNrk8gqGw1r0d1op5ddGyVCvUfXD8jH5422X62jUXqsKJAL3SejBhu2SOnuzQl668IKE8n+cB/Vf8CEVveSnUJVlw22vxYxv0VPPumLLV60Ja/NiGHtehwxPxTVcf7/2NTSGtXtd1n/fvTOvV2BTSG3tjV4m87+kW3fd0S7f16KnGppBaw0dTPndjU0iv7jyc9HG5qkOu9eT/qNQ0NoWStnkmK6mWs77QdkAuEDwCgD4oPnhw9xx/TM4jKflFf3zQKX7Vrpa2dt1zzUStWtu1QpebKNobwLl7jl8P3FIfU4f6sdV64Jb6aN3cldFumzlOw6q6Vgo7cuxU0lW8gqFwzH4HDpDuf25bzIpmOWNMzHO6oZzbZnYl2c7VVK3eTLxwg2zXfuJczfKPiilPVr9a31BJkY6wOwNky57MRg2tmFenNevf7UVtgczFJ+/3ludb/HQpty5nOjolSRt2HkobtL6ibpQa13ZdWD720k7d/9w2XVGXOsdYd+prqqNJ7qXYVTCTBdu95859r3R5/169LpRRveprqvXgizti9vlsyz4927Kv23r0VH1NtZ6Ie7/xPnd9TbX+4t9eT3icO7K1EP8n2XL/jz50FojYvKe9ZOuaSn1NdUybS5H/xVQrqfYV8e9HufxfB8pJXoNHxpjrjDHbjTE7jDH3Jrl/nDHmRWPMJmNMizHmhnzWBwD6q8amkFoPfBRTluxbs1Sjcrz3L53tjwkorVrbGpMoOhPBUFhv7z8anRp36kzkosw/+ix9cOJM0lxL8VOxznQqukpcrnivGb3Bl2SJWwsxVSvdN5vuOfn9S89LWCUv1bS7+A7v0k9NSLqdu3/XPddcqH+4NXL++1vuFRRefODaW543KQb8uXU5fjryZvMX//Z6QnA73tLZft09tys4/ze/3K6V8yfH5GXLRsDvU2WKRGXJgu3x79HexQSqKru6//c/ty2jegX8Pi3/dGR04zsHP9LyNZv00KJpMQsqJKtHTwX8Pt06c1xMWfwXG3/z2UsTHrdqbWvO6pBr7v/RrvAxSdL3frW9ZOuaSsDvi2lzV7KVVPuS+PejXP6vA+Ukb8EjY8xAST+QdL2kiyXdaoy5OG6zv5T0lLV2qqQvSvphvuoDAP1ZfU21ntjQ9S1ub781C/h9ahg/QlLypNrdic/H9ODCqdGcRwePnkqZayk+sLXCs0pcrrhBkaMnTscEX8aNjIzY+dmmtpj69GTYemNTSM27DsWUpcobla6NvCvcuR3bUWcN0gtb30964b0rfCyrDm98gOzyblZmA3It4Pfp5qlji10NSZG6fN5ZKOC2WeMzeg392TUXqqoi0tVuqB3R48CRy3iCR0sCtbppSmQ1y2TvwQG/T7Mn+qL3L53t19Tzh0uSls6eoLMHR6YKT8+iXkuuuEATzx2m3YePR/cZ8Pt01eRzUtajN+65epLqzula1TL+uX/v4nP0ifM+FlOW6zrkWsDv0xdnRKY/L7o8s/+jUhPw+1Q/tutzaUmSFUP7ooDfp2k97PcAfUU+Rx7NkLTDWttqrT0l6UlJN8VtYyW57/rVkvYKAJBzAb9Pt87oytfT22/NgqGwWsMf9Wi59mRT49ycR9+++ZJuV/GKHwmTyWpfjU0hbXwnMVjTXdDn+OmOmODLWYMj0+p+vL73Abj6mmrd9++bo7ebdx1KGCW0de8HWbdRwO/TZ6eN1cGPTiV0bAc7F7DjRg6NlqfLq9TYFEo4tped/EgDMlmqDeilYCistdv3p98wV7oZUecuFLBiXp3+JcP3vEdfatXJM52aUTtCzbsOJ+QaylanZ9raY8Fd+s9t+7td2GDr3g+i969eF9I7h45pxbw6rV7XqqMnOzSjdoRezaJeze8cSgjwB0Nhbdp9pEefBem8tvuIDn3UtZhC/HO/3HpQ+9pP6Oap50XLcl2HXAuGwnp+83taMa9OT2zYXdJ1TSUYCmvnwY9UVTlAVZUD9FhwV1keR7aCobB29rDfA/QV+QwejZXkzRTY5pR5/ZWk24wxbZKel/SVZE9kjPmyMabZGNN84MCBfNQVAPq8CaOHRf/uzbdmyUYOZbNce6pkzpI0bXwkD1KqVbzib2ey2ld9TbXufborWJMq6NPYFNLmPUckSed+rCppHb/1+10DaHsagAv4ffrOLZ+M3r736c0xgaobLz1PL27f36MRXS9uO9Btx3bPkePRC76ONMutJcvxsOKJ7legQ+lIlzrAs91njTHWGNNQyPql475Ov/+FKQnl+RY/TbUn73luLqGV8yfrqbsDWjl/su5/bluPA0jBUDgm55Frln9U0vrEr7bp5oc7e0iFTpzuVFXlAH316kkZ1yvZObjr8Y266/GNPf4syHR/3mNyn9u9f9ncCWp6q2t/y+ZOyFkdcq23n52lwLuS6qOLp+vRxdMlRfIeldNxZKsvtB2QC8VOmH2rpB9Za2sk3SDpcWNMQp2stQ9baxustQ2jR49OeBIAQHqtB7pWrunNt2apkmr3JgfQ0EEVCWXxCb2TjYQJhsJqaWvvdrWv+GBNqqBPfU21vveryFL0RskvUj85drjOdkYh9TQA19gU0seGVEZvf/aympj7/3tHOOtvNlN1bON9auLo6AVfsjxOXslyPLg5j1DaMkwdIGPM2ZL+RNL6wtYwPfd9ZlbcVMlC5BpLVZds3vNe2nEwJpfQ0tl+rZw/WS/tyGx1w2R1qPCM+HPzDbW0tSetT/xqmyvnT1ZHZ1e9Hlk8XS1t7RnXK9k5WFA/Rgvqx+T0syDV/txjcp/bvT9+9U73djH+T9LJx2dnoXlXUnWnLT60aJoW1I8pq+PIVl9oOyAXEnvrubNH0vme2zVOmdedkq6TJGvty8aYKkk+SQUcowwAfV8wFNYTG7oGg7rBhZ6MnEkWqHE7kUnlKLtyqtVOkuX2iddQOzL6d6qgT8Dv09euvVDf+vlWvf/ByaTBl5a2I6qsGKAVV0SCO7P8o7I+f/U11TErJT356rsxS3q7bTLLPyrjNkrVsV24OhITuPPKC/TDtSEdOX5KkrSgfox+urEt5fO5An6fPn3hOdGRUPEX8ihZ0dQBkmSMcVMHvBG33f+W9F1Jf17Y6qXnvs+cON2RtLwYdfHq9j1P0o+WzEgoWzrb3+O8R3fP8evvfrU9Zv/xvwN+n77zH9tiyuPr7/3b3SaTeiU7Bw/cUp9Qlu68ZCrdOXfvj99XqvJS0JP/o1LTF46hJ/rrcQPx8jny6FVJE40xFxhjBimSEPuZuG3elXSVJBljLpJUJYl5aQCQYy1t7TE5j0rtW7N0o2Ck3q124iaorqoY0O2InkUzx2t67QjtORJJCBvvu7/clrNh6+4ggvjcQz35ZjPdKnl/ds2FmnvhaL2667CWBGqTXvQlEwyF9XpbVz6TV1p7NmoCBZc2dYAx5jJJ51trn+vuiUgdAAAApDwGj6y1ZyQtl/SCpDcVWVVtqzHmr40xNzqbfU3SUmPM65KekLTYWhYABoBcu3uOPybnkZQ4LawcBPw+XVnXtYJQJoGjYCgczXk0ZNDAboM+r+w8qNCBroSYLveT6d7rJ/d62HpLW7seWjRNd1xeK0n60hW1SZc+dveRizZ6ZedBtbS1x0yHS5cwO9lUOHIe9Q1OioDvKdIP61Z/TB1ATxQAgER5zXlkrX3eWjvJWuu31n7bKfumtfYZ5+83rLVXWGsvtdZOsdb+Kp/1AQAUj8lkeFEawVBYb+z7IKucQC1t7TE5j1IFfbzBkqGDK7Rs7oSE53rr/aMxt3sS3HG3//nrexOCVPmSLB9SZ5or5PipcMlyS6VbtQ5Fky51wNmSLpG01hizS9IsSc+UWtLsgmMRQQAAUip2wmwAQB+Xqy/xe7rayd1z/Jru5DxyA1jJgj7eYEl9TbVWrW2N3nfs1BlJ0qRzY0dv5fI48ilZPqR0oyvip8LV11RHA0VuQvFkq9ahJHSbOsBa226t9Vlra621tZJekXSjtba5ONUFAACljuARAKAs5Hu1E2+wJD6/0u7DxyVJ9TXDe72fVMeRT8nyIQ3IsgcQ8Puiq611dNoeJ1xH/mWYOgBlhul0AIBiyudqawCAEtHYFNKBD0/ElGWyzH0u5GomSKFXOwn4ffrC9PP1k1d3a8TQSoWPnsrJ86Y6jnJwZZ1PY6qrtK/9RMY5p1Ac1trnJT0fV/bNFNvOLUSdeiIHs10BAEAOMPIIAPqB+ppqPbGha/GlQk45Ktcvy4OhsH79xvtaMa9Oh4+dLnZ1stLYFEqYzhefnyhdwuxkXm49qJNnOrPKOQUgN8r1vRQA0DcQPAKAfqClrV1zJnWNElm+ZpOWzZ2QdspXJkGIUpHLusbnJaoZPkSS1NJ2JCd1zbf6muqYPEq5CBb2NOcUUDaIzgAAkBLBIwDoB+prqvVK66Ho7TmTfFq1tjVtMKE3QYhCB566q2u214TxeYnOGhyZ5R2/2lqp8uZRGjm0Mif5ifKdcwooFUyVAwAgEcEjAOgHAn6f/vjTXbl2frZpr5bNnZA2mBCfzDmbIIQbzDl6IrJS2ea2IwmBp1wGmLKpa7r9xq805vrctJqs65VOpucg23MV8Pt064zzdejY6ZzkJ0p2TpKtWgfkUk+mV/ZVlozZAIAiIngEAP1AMBTWqrWtmjNptCTpM1PHatXa1oymHAX8Po08q1KSsgpCuMGcNmelsr/91VsJwRw3wHTqTKckaeM7h3o1vSrg9+m6Sz6eUNf4y898TOvqqUzrkm2dg6GwXtj6PvmJgCwRowEAIBHBIwDoB1ra2rVs7gRt3tOuFfPq1PTWgYxyHkmRIIS10rI5E7IOQgT8Pn3pylpJ0h2Xj086cuXBhVP14cnI6KRv/GxLr6ZXBUNhbdh5KCFgEn8tmO2IqnxeS2Zal2zqTH4ioAcY5AQAQEoEjwCgH6ivqdaqta0xwYRMch65QYgf/OFl+vr1F2UdhAiGwnqqua3b0S8Bv093XnGBJOn2WYkBpkxlEjDxXhsG/D59amJkX5mOqMpXLpSA36fPXjY2bV285d1tR34iAAAA5BLBIwDoB3oaTOhNECLT0S/BUFj/urH7AFM+jjEYCmvL3g9KYlpXMBTWi9sPpK1LMBTWiKGVWv5pf7fbdZefqJxW0APQhdl0AIBiqih2BQAA+ZcsqXHA70s72qanj5O6D+a4Zd4AU8Dv0yz/qB6vDNZdXcNHT8aU53K/vZVpXbyjwAJ+nwJ1vh7V2c2d1Okkdnml9aD+9Cevx0yJA0oFK58BAFAaGHkEAMiLTFbnKtb0qlKa1pVpXXJVZ/dxHU7w6KtPvlaUoBkAAADKByOPAABF05uRTeW4397UJZd1Dvh9un3WeP3Ty+9oUS/yTAH59vB/tSaUBUNhtbS1x7wmFj+2QcOHVMZst3pdSC/tOJjRfhqbQtHReN3tpxgam0J65+BHMavABUNh/eL1vRo/6qyc1a+xKZSQBy/X5yCbfRSiPgCAzDHyCADQ57H0dqxgKKxftOwriXxPQHeSBQ+Wr9mUUH5F3Sj97LW90dur14V0/3PbdEXdqIz388MXu/J+pdpPMdTXVOvZln0xZXc9vlHPtuzLaf3cKa2ufJyDbPZRiPoAADJH8AgAgDRsH4o+ZZrIHCgFl0+IDf6kyvO1dLZfiwO10dv3P7dNK+dP1tLZmY1QCfh9+p8LLpIkHT/VUbQcaMkE/D49tGiaBsb12h9aNC2n9XOntLrycQ7cfQwcYNLuw922IoNtAQD5R/AIANDn5SrprlH5Z+8tpXxPQLZumzkuZfDgr278hOrOGSZJml47IuPAkesL08dp4cxx2rDrULf7KYaA36cve45nSaA2L/UL+H36oysvkNT9ue7tPpbOzmwfAb9Pf5ThtgCA/CJ4BADo83I1cMj2gcWyM0lkDpSidNMsV68LKbT/qGbUjtCruw5r9bpQ0u1SCYbC+uWW90pyOmcwFNaP17+jqsoBqqocoMeCu/JSv2AorKc37cnrOQiGwnqquS2jfWSzLQAgvwgeAQD6jWxHIDU2hRIuVoKhsBqbsrsoBdAzL7d2Jbzubpqlm+No5fzJeurugFbOn6z7n9uWcQCplKdzBkNh3fX4RknSo4un69HF0yVF8h7lsn6FOAfZ7KOU2wQA+iOCRwAApOAmbP3o5BlJUsvudhK2AgW0eU/sdMpU0yxf2nEwJsfR0tl+rZw/OePV1kp5OmdLW7sW1I+J5jhycyAtqB+T0/oV4hxks49SbhMA6I9MuSUBbWhosM3NzcWuBgCgjBz48KSmf/s38g0bpOa/vDqrxwZDYd3x6Aad7rCqHlKpVbddRt6NPDPGbLTWNhS7HohVjD5YR6eVf+XzkqRd35nfo+eovfe5Xj2+nPSnY/Xqr8cNALnWXR+MkUcAAHQj4PdpSSCSsPWOy8cTOAIAAEC/Q/AIAIBu3zwUVAAAC/pJREFUBENh/fR3JGwFAABA/0XwCADQ5/V0lTQStgLFlWWOewAAkCcEjwAA/Uh2l6IkbAUAAACkimJXAACAwsluBNLdc/wJZe5qRwAAAEB/wcgjAAAAAAAApETwCADQj5BBBSgnhpcsAAAlgeARAAAAAAAAUiJ4BAAAAAAAgJQIHgEAAAAAACAlgkcAAAAoSYakRwAAlASCRwCAvs8WuwIAAABA+SJ4BADoNxjEAAAAAGSP4BEAoN+wjEACAAAAskbwCADQ9zHiCAAAAOgxgkcAgD6tsSmk5p2HYsqCobAam0JFqhGATDQ2hRQMhWPKsnnt9vbx5aQ/HatXfz1uACgGgkcAgD6tvqZaK/99i6RIzqNgKKzlazapvqa6yDUD0J36mmotX7Mpejvb125vH19O+tOxevXX4waAYjC2zBJANDQ02Obm5mJXAwBQRp5v2as/XrNJQwcNVFXlQD24cKoCfl+xq4UUjDEbrbUNxa4HYhWjDxYMhbVw9XpJ0sizBmX92u3t48tJfzpWr/563ACQD931wRh5BADo8669ZIxqRgzRsVMdum3mOC4sgDIR8Ps0ftRQSerRazfg92nC6LN6/PhyEvD7NPOCkZL6/rF6Bfw+zZ4YOdb+dNwAUGgEjwAAfd76nQd17FSHVsyr04/Xv5uQIwNAaQqGwvrwxJkev3aDobCOHDvdL177wVBYb+8/2i+O1SsYCmvr3g/63XEDQKERPAIA9GluDowHF07VPddcqAcXTtXyNZu4wABKXG9fu/3ptd+fjtWrvx43ABQDwSMAQJ/W0tYekwMj4PfpwYVT1dLWXuSaAehOb1+7/em135+O1au/HjcAFAMJswEAQEkhYXZpog8GAEDfRsJsAAAAAAAA9AjBIwAAAAAAAKRE8AgAAAAAAAApETwCAAAAAABASgSPAAAAAAAAkBLBIwAAAAAAAKRE8AgAAAAAAAApETwCAAAAAABASgSPAAAAAAAAkBLBIwAAAAAAAKRE8AgAAAAAAAApETwCAAAAAABASgSPAAAAAAAAkBLBIwAAAAAAAKRE8AgAAAAAAAAp5TV4ZIy5zhiz3Rizwxhzb4pt/sAY84YxZqsxZk0+6wMAAAAAAIDsVOTriY0xAyX9QNLVktokvWqMecZa+4Znm4mS7pN0hbX2sDHmnHzVBwAAAAAAANnL58ijGZJ2WGtbrbWnJD0p6aa4bZZK+oG19rAkWWv357E+AAAAAAAAyFI+g0djJe323G5zyrwmSZpkjHnJGPOKMea6ZE9kjPmyMabZGNN84MCBPFUXAAAAAAAA8YqdMLtC0kRJcyXdKmm1MWZ4/EbW2oettQ3W2obRo0cXuIoAAAAAAAD9V95yHknaI+l8z+0ap8yrTdJ6a+1pSTuNMW8pEkx6NdWTbty4MWyMeSfXlXX4JIXz9NzIDm1RGmiH0kA7lA7aojDGF7sCSEQfrF+gHUoHbVEaaIfSQDsUTso+mLHW5mWPxpgKSW9JukqRoNGrkhZaa7d6trlO0q3W2juMMT5JmyRNsdYezEul0te52VrbUIx9IxZtURpoh9JAO5QO2gLID15bpYF2KB20RWmgHUoD7VAa8jZtzVp7RtJySS9IelPSU9barcaYvzbG3Ohs9oKkg8aYNyS9KOnPixU4AgAAAAAAQKJ8TluTtfZ5Sc/HlX3T87eVdI/zAwAAAAAAgBJT7ITZpebhYlcAUbRFaaAdSgPtUDpoCyA/eG2VBtqhdNAWpYF2KA20QwnIW84jAAAAAAAAlD9GHgEAAAAAACAlgkcAAAAAAABIieCRwxhznTFmuzFmhzHm3mLXp68xxjxqjNlvjNniKRtpjPm1MeZt5/cIp9wYY/7BaYsWY8xlnsfc4Wz/tjHmjmIcSzkzxpxvjHnRGPOGMWarMeZPnHLaosCMMVXGmA3GmNedtvhfTvkFxpj1zjn/iTFmkFM+2Lm9w7m/1vNc9znl240x1xbniMqbMWagMWaTMeZZ5zbtABQA/a/8ow9WGuiDlQb6X6WF/ld5IXikyD+tpB9Iul7SxZJuNcZcXNxa9Tk/knRdXNm9kn5rrZ0o6bfObSnSDhOdny9LWiVFPlwlfUvSTEkzJH3L/YBFxs5I+pq19mJJsyT9D+d/nbYovJOS5llrL5U0RdJ1xphZkr4r6fvW2jpJhyXd6Wx/p6TDTvn3ne3ktN8XJX1CkdfYD533NGTnTyS96blNOwB5Rv+rYH4k+mClgD5YaaD/VVrof5URgkcRMyTtsNa2WmtPSXpS0k1FrlOfYq39L0mH4opvkvRPzt//JOkznvJ/thGvSBpujBkj6VpJv7bWHrLWHpb0ayV2htANa+0+a+3vnL8/VOTNeqxoi4JzzulR52al82MlzZP0U6c8vi3cNvqppKuMMcYpf9Jae9Jau1PSDkXe05AhY0yNpPmS/tG5bUQ7AIVA/6sA6IOVBvpgpYH+V+mg/1V+CB5FjJW023O7zSlDfp1rrd3n/P2epHOdv1O1B+2UQ85wz6mS1ou2KApnqO5rkvYr0vkLSTpirT3jbOI9r9Fz7tzfLmmUaItc+HtJfyGp07k9SrQDUAi8boqHz/0iog9WXPS/Sgb9rzJD8AglwVprFYn6owCMMcMk/Zukr1prP/DeR1sUjrW2w1o7RVKNIt+STC5ylfodY8wCSfuttRuLXRcAKAY+9wuLPljx0f8qPvpf5YngUcQeSed7btc4Zciv953ht3J+73fKU7UH7ZQDxphKRTot/2Ktfdoppi2KyFp7RNKLki5XZFh6hXOX97xGz7lzf7Wkg6IteusKSTcaY3YpMmVmnqT/K9oBKAReN8XD534R0AcrLfS/ior+VxkieBTxqqSJTnb3QYok3XqmyHXqD56R5K4QcYekn3vKb3dWmZglqd0ZzvuCpGuMMSOcxIDXOGXIkDM3+BFJb1prv+e5i7YoMGPMaGPMcOfvIZKuViT/wYuSPudsFt8Wbht9TtJ/Ot9QPiPpi84qFBcoklhzQ2GOovxZa++z1tZYa2sVee//T2vtH4p2AAqB/lfx8LlfYPTBSgP9r9JA/6tMWWv5sVaSbpD0liJzXr9R7Pr0tR9JT0jaJ+m0InNR71RknupvJb0t6TeSRjrbGkVWXwlJ2iypwfM8X1IkEdoOSUuKfVzl9iPpSkWGQ7dIes35uYG2KEpb1Eva5LTFFknfdMonKPKht0PSv0oa7JRXObd3OPdP8DzXN5w22i7p+mIfW7n+SJor6VnagR9+CvdD/6sg55g+WAn80AcrjR/6X6X3Q/+rfH6Mc8IBAAAAAACABExbAwAAAAAAQEoEjwAAAAAAAJASwSMAAAAAAACkRPAIAAAAAAAAKRE8AgAAAAAAQEoEjwAUjDGmwxjzmjHmdWPM74wxgTTbDzfG/HEGz7vWGNOQu5oCAAD0HfTBAPQWwSMAhXTcWjvFWnuppPskPZBm++GS0nZcAAAA0C36YAB6heARgGL5mKTDkmSMGWaM+a3zTdhmY8xNzjbfkeR3vin7P862X3e2ed0Y8x3P833eGLPBGPOWMWZ2YQ8FAACgbNAHA5C1imJXAEC/MsQY85qkKkljJM1zyk9Iutla+4ExxifpFWPMM5LulXSJtXaKJBljrpd0k6SZ1tpjxpiRnueusNbOMMbcIOlbkn6vQMcEAABQ6uiDAegVgkcACum4pxNyuaR/NsZcIslIut8Y8ylJnZLGSjo3yeN/T9Jj1tpjkmStPeS572nn90ZJtfmpPgAAQFmiDwagVwgeASgKa+3LzjdcoyXd4PyeZq09bYzZpcg3Y9k46fzuEO9tAAAASdEHA9AT5DwCUBTGmMmSBko6KKla0n6n0/JpSeOdzT6UdLbnYb+WtMQYM9R5Du+QaQAAAKRBHwxATxAZBlBI7nx7KTJM+g5rbYcx5l8k/cIYs1lSs6RtkmStPWiMeckYs0XSf1hr/9wYM0VSszHmlKTnJa0swnEAAACUE/pgAHrFWGuLXQcAAAAAAACUKKatAQAAAAAAICWCRwAAAAAAAEiJ4BEAAAAAAABSIngEAAAAAACAlAgeAQAAAAAAICWCRwAAAAAAAEiJ4BEAAAAAAABS+v8fOGAEKL4ENQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0sa5HSPCG3z"
      },
      "source": [
        "## Testing with individual images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANlapyGZCG31"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='data/', \n",
        "                     train=False,\n",
        "                     transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVPu1uhDCG4b"
      },
      "source": [
        "Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH3U8NbDCG4c"
      },
      "source": [
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    return preds[0].item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_pCs15kCG4m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "588439cc-495a-4206-980c-0928a7573a21"
      },
      "source": [
        "model.to(\"cpu\")\n",
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 7 , Predicted: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ELf9CpmCG4r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d2b1ad77-5eb3-44db-a5d9-f3e8e4838872"
      },
      "source": [
        "img, label = test_dataset[10]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0 , Predicted: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfnQIhbCG5W"
      },
      "source": [
        "## Extras: Saving and loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSgECsRCG5Y"
      },
      "source": [
        "It would be a good idea to save our model since we've trained it for a long time and achieved reasonable accuracy. \n",
        "\n",
        "Don't ever just save to colab as the files will get lost after runtime disconnects - always save to your drive  \n",
        "\n",
        "**When saving the model, we are actually saving the parameters (weights and bias matrices) to a file.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5JSKtvVCG5Z"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-logistic.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHyOJu56CG5e"
      },
      "source": [
        "The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU9_2rjrCG5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "f2fa5bab-65f2-4f55-8e09-786caaed3fd7"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[ 0.0340, -0.0343, -0.0326,  ..., -0.0117,  0.0193, -0.0325],\n",
              "                      [ 0.0190,  0.0165, -0.0352,  ..., -0.0333,  0.0079,  0.0047],\n",
              "                      [-0.0232, -0.0181, -0.0190,  ..., -0.0034, -0.0288,  0.0015],\n",
              "                      ...,\n",
              "                      [-0.0289, -0.0074, -0.0120,  ...,  0.0272,  0.0035,  0.0215],\n",
              "                      [ 0.0334, -0.0243,  0.0121,  ...,  0.0342, -0.0335, -0.0303],\n",
              "                      [ 0.0277, -0.0229, -0.0316,  ..., -0.0086,  0.0267,  0.0145]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0379,  0.1232, -0.0117, -0.0179, -0.0009,  0.0137, -0.0091,  0.0544,\n",
              "                      -0.1006,  0.0053]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-H0VOC0CG5l"
      },
      "source": [
        "To load the model weights, we can instante a new object of the class `MnistModel`, and use the `.load_state_dict` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IGfkXDPCG5m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "28964a6a-fae6-4229-d662-1a5f7b751c2b"
      },
      "source": [
        "model2 = MnistModel()\n",
        "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[ 0.0340, -0.0343, -0.0326,  ..., -0.0117,  0.0193, -0.0325],\n",
              "                      [ 0.0190,  0.0165, -0.0352,  ..., -0.0333,  0.0079,  0.0047],\n",
              "                      [-0.0232, -0.0181, -0.0190,  ..., -0.0034, -0.0288,  0.0015],\n",
              "                      ...,\n",
              "                      [-0.0289, -0.0074, -0.0120,  ...,  0.0272,  0.0035,  0.0215],\n",
              "                      [ 0.0334, -0.0243,  0.0121,  ...,  0.0342, -0.0335, -0.0303],\n",
              "                      [ 0.0277, -0.0229, -0.0316,  ..., -0.0086,  0.0267,  0.0145]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0379,  0.1232, -0.0117, -0.0179, -0.0009,  0.0137, -0.0091,  0.0544,\n",
              "                      -0.1006,  0.0053]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}